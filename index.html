<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>SStar1314</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SStar1314">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="SStar1314">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SStar1314">
  
    <link rel="alternative" href="/atom.xml" title="SStar1314" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/main.css">
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/favicon.ico" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Xia, MingXing</a></h1>
		</hgroup>

		
		<p class="header-subtitle">西安</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">所有文章</a></li>
	        
				<li><a href="/categories">目录</a></li>
	        
				<li><a href="/tags">标签</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="#" title="rss">rss</a>
		        
					<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Xia, MingXing</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/favicon.ico" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Xia, MingXing</h1>
			</hgroup>
			
			<p class="header-subtitle">西安</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories">目录</a></li>
		        
					<li><a href="/tags">标签</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-Linux系统启动源代码分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Linux系统启动源代码分析/">Linux 系统启动源代码分析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Linux-系统启动源代码"><a href="#Linux-系统启动源代码" class="headerlink" title="Linux 系统启动源代码"></a>Linux 系统启动源代码</h2><p>以x86_64 arch机器为例：</p>
<p>—————————————————— 内核load进内存阶段 （arch/*/boot/）—————————————————-</p>
<p>入口文件是：  <strong>arch/x86/boot/compressed/head_64.S</strong>  注意：  kernel 代码不是以 main 函数为入口的。<br>head_64.S 是 汇编 代码文件， 该文件会 call  verify_cpu + make_boot_params(初始化boot_params) + efi_main(处理并返回boot_params)，最终里面  call  extract_kernel 会调用 入口函数 extract_kernel, 该 函数 位于 arch/x86/boot/compressed/misc.c 会：</p>
<pre><code>1. 拿到 boot_params , 由汇编代码传入该参数      该 参数的结构体 位于 arch/x86/include/uapi/asm/bootparam.h .
2. 通过 sanitize_boot_params 函数  初始化 boot_params 参数部分内容， 代码位于 arch/x86/include/asm/bootparam_utils.h .
3. 通过 boot_params-&gt;screen_info 内容，  调用 arch/x86/boot/compressed/console.c  文件中的 console_init 函数, 初始化 tty 的console .
4. (arch/x86/boot/compressed/kaslr.c) choose_random_location 函数 会随机挑选一段内存地址， 用于解压内核 压缩文件。
5. 调用 (lib/decompress_bunzip2.c) __decompress 函数解压缩内核压缩文件, 根据不同的压缩文件类型，调用不同的解压缩函数， 压缩文件区分应该是发生在 编译内核时。
6. 内核文件解压之后会成为 elf 文件， 位于内存中， 通过调用 parse_elf 函数 load 进 内核内容,  parse_elf 位于 arch/x86/boot/compressed/misc.c 中。
7. 判断是否 需要重新 分配内存地址， 调用 handle_relocations 函数。
</code></pre><p>arch/x86/boot/compressed/head_64.S 在解压缩内核之后会执行解压缩之后的内核代码！！！</p>
<p>而 与arch 无关， 较为 common(legacy) 的 文件  arch/x86/boot/header.S  会 最终调用 (arch/x86/boot/main.c) main 函数 ， 该函数会：</p>
<pre><code>1. copy_boot_params   拷贝启动参数
2. console_init  初始化console
3. init_heap
4. validate_cpu
5. set_bios_mode
6. detect_memory
7. keyboard_init
8. query_ist
9. query_apm_bios (if  config_apm)
10. query_edd  (if config_edd)
11. set_video
12. go_to_protected_mode
</code></pre><p>—————————————————— 内核启动阶段 （init/main.c） —————————————————-</p>
<p>入口文件是 ： init/main.c        启动函数是：  start_kernel   该函数会：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line">1. 调用 (kernel/fork.c)  set_task_stack_end_magic(&amp;init_task) 函数， 注册系统内核启动后的 idle(PID=0)  进程。 该 init_task 在 init_task.h 文件中定义，在 fork.c 文件中设置栈边界。</div><div class="line">2. smp_setup_processor_id 函数, 检查cpu是否为多处理器，获取当前处理器逻辑号。</div><div class="line">3. 调用(debugobjects.c)debug_objects_early_init 函数,  初始化debug对象的锁，并将debug对象链接成链表。</div><div class="line">4. 调用(stackprotector.h)boot_init_stack_canary函数, 尽可能早地进行stack protect，防止 栈越界 canary 攻击，关于canary attack 可以参照： https://hardenedlinux.github.io/2016/11/27/canary.html</div><div class="line">5.  调用(group.c)group_init_early 函数,  初始化 group_root 结构体， 并且将每一个 cgroup_subsys 加入到 group_root 对象中，并初始化每一个 cgroup_subsys 对象。 支持的 cgroup_subsys 位于 include/linux/cgroup_subsys.h 文件中。</div><div class="line">6. local_irq_disable() 函数 将本地中断暂时disabled。</div><div class="line">7. 调用(kernel/cpu.c)boot_cpu_init函数，  设置 获得 cpu 第一个处理器标志对象， 标志 该处理器对象为 online+active+present+possible.</div><div class="line">8. 调用 (mm/highmem.c)page_address_init 函数,  初始化 高端内存页表 page_address_htable 对象 .</div><div class="line">9. 调用 pr_notice 函数 打印 Linux 版本信息(linux_banner) .</div><div class="line">10. 调用 setup_arch(command_line) 函数， 设置与 硬件架构相关的 配置,  command_line 为内核启动参数。 相关结构体有：  hwrpb_struct， notifier_block(中断处理结构体,注册形成notifier_chain)，alpha_using_srm/alpha_using_qemu(使用 srm或者qemu), callback_init(初始化 kernel_page + kernel_PCB + third_level_PTE)  </div><div class="line">11. 调用 mm_types.h(mm_init_cpumask)函数， 初始化 init_mm 对象，init_mm 是 mm_struct 结构体对象，位于 init-mm.c 文件中.</div><div class="line">12. 调用 setup_command_line 函数，保存 原始的 command_line 对象， 保存给以后分析。    </div><div class="line">13. cpu 相关的4个函数(不太明白)： setup_nr_cpu_ids + setup_per_cpu_areas + boot_cpu_state_init + smp_prepare_boot_cpu</div><div class="line">14. 调用 (page_alloc.c)build_all_zonelists 函数 初始化 页表区， 建立系统内存页表链表 .</div><div class="line">15. 调用 (page_alloc.c)page_alloc_init 函数 初始化页表， 初始化内存页表。</div><div class="line">16. 调用 parse_early_param 函数, 解析 command_line 对象，拿到 早期 参数, 参数以 kernel_param 结构体保存 。</div><div class="line">17. 调用 pase_args 函数， 解析不能被 pase_early_param 函数解析的参数。</div><div class="line">18. 调用 (pid.c)pidhash_init 函数,  call  (page_alloc.c)alloc_large_system_hash 函数, allocate 一个大的系统 <span class="built_in">hash</span> table，名字为 PID .</div><div class="line">19. 调用 (dcache.c)bfs_caches_init_early 函数, call 两次 alloc_large_system_hash 函数, allocate 两个大的系统 <span class="built_in">hash</span> table，名字为 Dentry Cache + Inode-Cache .</div><div class="line">20. 调用 (eatable.c)sort_main_extable 函数, 对异常处理函数table进行排序。</div><div class="line">21. 调用 (arch/x86/kernel/traps.c)trap_init 函数,  初始化 硬件中断.</div><div class="line">22. 调用 (main.c)mm_init 函数, 建立内存分配器.   该函数会调用  page_ext_init_flatmem + mem_init + kmem_cache_init + percpu_init_late + pgtable_init + vmalloc_init + ioremap_huge_init  .</div><div class="line">23. 调用 (core.c)sched_init 函数， 初始化 调度器 .  该调度器会处理各种中断.  很复杂.</div><div class="line">24. preempt_disable 函数， 禁止调度抢占.</div><div class="line">25. 调用 idr_init_cache 函数， call  kmem_cache_create 函数 创建 kmem_cache , 该cache 名为 idr_layer_cache .</div><div class="line">26. 调用 (workqueue.c)workqueue_init_early 函数,  该函数 建立 各种数据结构／系统workqueue，  调用 多次 alloc_workqueque 函数 构建 各种事件队列.</div><div class="line">27. rcu_init 函数， 初始化互斥访问机制.</div><div class="line">28. (tiny.c/tree.c)trace_init 函数,   trace printk 调用.</div><div class="line">29. (context_tracking.c)context_tracking_init 函数,   tracking context 在哪个cpu上运行.</div><div class="line">30. (radix-tree.c)radix_tree_init函数,  call  kmem_cache_create 创建 kmem_cache， 该cache 名为 radix_tree_node .  radix tree 为 基数树.</div><div class="line">31. (irq.c)early_irq_init 函数, 调用 arch_early_irq_init 函数, 构建 irq_domain 结构体.</div><div class="line">32. (irqinit.c)init_IRQ 函数,  调用  arch/x86/kernel/irqinit.c init_IRQ 函数， 初始化中断向量. 关键结构体 有 x86_init_ops .</div><div class="line">33. (tick-common.c)tick_init 函数, 初始化 时钟控制.</div><div class="line">34. rcu_init_nohz 函数.</div><div class="line">35. (timer.c)init_timers 函数, 初始化 定时器. 原理是： 通过 调用 open_softirq 软中断， 注册中断处理函数为 run_timer_softirq.</div><div class="line">36. hrtimers_init 函数， 初始化 高精度定时器.</div><div class="line">37. (softer_init.c)softirq_init 函数，  软中断 初始化.  调用 open_softirq 注册2个级别 TASKLET_SOFTIRQ + HI_SOFTIRQ  中断 向量， 关键对象 softirq_vec .   可以学习:  soft_irq 与 tasklet 区别。</div><div class="line">38. timekeeping_init : 初始化资源和普通计时器,  初始化 clocksource 和 common timekeeping values.</div><div class="line">39. time_init :    call  (arch/x86/kernel/time.c) x86_late_time_init 函数, 注册 结构体 x86_init_ops 对象的  timer 属性.   以及 时钟中断, 在后面的 late_time_init 函数中调用.           </div><div class="line">40. sched_clock_postinit :  初始化 clock_data 结构体, 更新 调度 计时器.   </div><div class="line">41. printk_safe_init: 调用 init_irq_work 函数 初始化中断栈, 将 pending 的 所有 message 全都<span class="built_in">print</span> 出去.</div><div class="line">42. (core.c)perf_event_init:  初始化 idr 结构体, call perf_pmu_function 注册 performance monitoring unit(pmu) 各类事件，   pmu 事件类型有：perf_swevent +   perf_tracepoint + perf_cpu_clock + perf_task_clock + perf_breakpoint .</div><div class="line">43. (profile.c)profile_init : kernel profiling 工具 初始化.  初始化 内核调优 的代码, 与内核启动的传入参数有关.  CPU_PROFILING + SCHED_PROFILING + SLEEP_PROFILING + KVM_PROFILING .</div><div class="line">44.  call_function_init: 不知道做啥.</div><div class="line">45.  (slab.c)kmem_cache_init_late : 调.整 cpu cache大小, 并且 注册 一段 memory 用于 hotplug 回调.</div><div class="line">46. (tty_io.h)console_init : 初始化 console  device. 可以显示 printk 的内容.</div><div class="line">47. (locked.c)lockdep_info :  打印 一些 依赖 信息.</div><div class="line">48. (locking-selftest.c)locking_selftest : self-test  <span class="keyword">for</span>  hard/soft-irqs.</div><div class="line">49. (ifdef  CONFIG_BLK_DEV_INITRD)initrd_start :  kernel启动时是否传入 initrd 参数， 传入的话 会进入 raw disk.</div><div class="line">50. (page_ext.c)page_ext_init : 防止高位内存出界, 出界的内存可能没有被初始化，重新初始化, 并且 设置 回调 函数.</div><div class="line">51. (debugobjects.c) debug_objects_mem_init :  debug 内存 是否 越界.</div><div class="line">52. (kmemleak.c)kmemleak_init : 内核内存 分配 泄漏 检测 逻辑 初始化.</div><div class="line">53. (page_alloc.c)setup_per_cpu_pageset :  为每一个 cpu 分配内存 页表 及 页表区.  在此函数调用之前, 可被使用的内存 仅为 boot memory.     </div><div class="line">54. (mempolicy.c)numa_policy_init : 设置内存 numa(Non-uniform memory access, 非统一内存访问架构) 规则. 关键结构体: mempolicy</div><div class="line">55. (arch/x86/kernel/time.c)later_time_init : 执行 time_init 函数初始化的内容.</div><div class="line">56. calibrate_delay : 校准 时延.</div><div class="line">57. (pid.c)pidmap_init :  初始化  pid_max 值, 以及 初始化 pid_namespace 结构体.  保留 pid 为0 的位置.</div><div class="line">58. (rmap.c)anon_vma_init : (Anonymous Virtual Memory Access) 匿名虚拟内存区域初始化.  </div><div class="line">59. (drivers/acpi/bus.c)acpi_early_init:     ACPI ？？？  不知道 干什么?</div><div class="line">60. (efi.c)efi_enter_virtual_mode :  也不清楚 EFI 做啥？</div><div class="line">61.  (esprit_64.c)init_espfix_bsp : 调整 进程 esp 寄存器位置, 在 non-init 进程创建之前调用.</div><div class="line">62.  (fork.c)thread_stack_cache_init : 调用 kmem_cache_create 在内核内存区 创建 thread_stack cache.</div><div class="line">63. (cred.c)cred_init : 调用 kmem_cache_create 在内核内存区 创建 cred_jar  cache, 用于 存储 credentials.</div><div class="line">64. (fork.c)fork_init : 进程创建机制 初始化, 调用 kmem_cache_create 在内核内存区 创建 task_struct cache,  set_max_threads, 关键结构体 task_struct</div><div class="line">65. (fork.c)proc_caches_init : 进程创建所需的其它结构体 初始化, 调用 kmem_cache_create 在内核内存区 创建 sighand_cache/signal_cache/files_cache/fs_cache/mm_struct  cache .</div><div class="line">66. (fs/buffer.c)buffer_init : 调用 kmem_cache_create 在内核内存区 创建 buffer_head  cache. 将一定数量的内存区 设置为 buffer.</div><div class="line">67. (security/keys/key.c)key_init : 内核密钥 管理,  调用 kmem_cache_create 在内核内存区 创建 key_jar  cache .    </div><div class="line">68. (security.c)security_init : 内核安全框架. 比较复杂, 没看懂 ?</div><div class="line">69. (debug_core.c)dbg_late_init : kernel debug stuff.</div><div class="line">70. (fs/dcache.c)vfs_caches_init : 创建虚拟文件系统.  主要调用函数有:  dcache_init, inode_init, files_init, files_maxfiles_init, mnt_init(该函数还会调用 kernfs_init/sysfs_init/kobject_create_and_add/init_rootfs/init_mount_tree), bdev_cache_init, chrdev_init.  调用 kmem_cache_create 在内核内存区 创建 names_cache + dentry + inode_cache + filp + mnt_cache + bdev_cache   cache.  调用 alloc_large_system_hash   分配 Dentry cache + Inode-cache + Mount-cache + Mountpoint-cache     HashMap.</div><div class="line">    (mount.c)kernfs_init : 该函数 在内核内存区 创建 kernfs_node_cache  cache.  </div><div class="line">    (kobject.c)kobject_create_and_add : 创建 kobject  fs 对象.</div><div class="line">    (mount.c)sysfs_init : 该函数 调用  register_filesystem 函数, 初始化 file_system_type 对象 sysfs .  register_filesystem 当加载fs对应module时调用该函数.   </div><div class="line">    (do_mounts.c)init_rootfs : 该函数调用  register_filesystem 函数, 初始化 file_system_type 对象 rootfs + tmpfs/ramfs  </div><div class="line">    (namespace.c)init_mount_tree :  设置 rootfs mount tree.</div><div class="line">    (block_dev.c)bdev_cache_init : 注册 block 设备,  注册对象为 file_system_type  bdev .   bd_mount 扫描磁盘, 获得 磁盘 文件系统.      </div><div class="line">    (char_dev.c)chrdev_init :  注册 字符设备.</div><div class="line">71. (mm/filemap.c)pagecache_init :  该函数 主要是初始化 页写回 机制, 主要函数为 (page_writeback.c)page_writeback_init .</div><div class="line">72. (kernel/signal.c)signals_init :  调用 kmem_cache_create 在内核内存区 创建  sigqueue  cache 对象.  </div><div class="line">73. (fs/proc/root.c)proc_root_init :  proc 文件系统初始化, 主要调用  proc_init_inodecache + set_proc_pid_nlink + register_filesystem + proc_self_init + proc_thread_self_init + proc_symlink + proc_net_init + proc_mkdir + proc_create_mount_point + proc_tty_init + proc_sys_init .        </div><div class="line">        (fs/proc/inode.c)proc_init_inodecache :  调用 kmem_cache_create 在内核内存区 创建  proc_inode_cache 对象.</div><div class="line">        (fs/proc/base.c)set_proc_pid_nlink :  主要 调用 pid_entry_nlink 函数,  关键结构体 为 pid_entry . 初始化 2个对象:  tid_base_stuff  + tgid_base_stuff .         </div><div class="line">        (filesystem.c)register_filesystem : 注册 proc  filesystem .  关键 函数为  proc_mount, 该函数会注册 /proc/self 文件夹.  </div><div class="line">        (fs/proc/thread_self.c)proc_self_init + proc_thread_self_init :  初始化 /proc/self 文件夹.  </div><div class="line">        (fs/proc/thread_self.c)proc_symlink :  创建 链接 文件 /proc/mounts  链接  /proc/self/mounts .  </div><div class="line">        (fs/proc/proc_net.c)proc_net_init :   创建  链接 文件 /proc/net  链接  /proc/self/net .  初始化  /proc/self/net  文件夹.</div><div class="line">        proc_mkdir : 创建 sysvipc, fs, driver, bus  文件夹 .  </div><div class="line">        proc_create_mount_point : 创建  /proc/fs/nfsd 文件夹 .</div><div class="line">        proc_tty_init : 创建 /proc/tty 文件夹.</div><div class="line">        (fs/proc/proc_sysctl.c)proc_sys_init :  注册和初始化  /proc/sys (sysctl)  文件系统.   关键函数 ：  __register_sysctl_paths</div><div class="line">74. nsfs_init : 不知道干啥 ???</div><div class="line">75. (kernel/cpuset.c)cpuset_init :  初始化 cpuset 系统, 并且 创建 /sys/fs/cgroup/cpuset 文件夹.  </div><div class="line">76. (kernel/cgroup.c)cgroup_init :  control group 初始化.  </div><div class="line">77. (kernel/taskstats.c)taskstats_init_early : 早期初始化，初始化 taskstats 结构体.</div><div class="line">78. delayacct_init() + check_bugs() + acpi_subsystem_init() + arch_post_acpi_subsys_init() + sfi_init_late()</div><div class="line">79. (main.c)rest_init :  完成 剩下 non-init 的 任务.    包括 :    —&gt; https://danielmaker.github.io/blog/linux/images/start_kernel_call_graph.svg</div><div class="line">            rcu_scheduler_starting :     启动 rcu_scheduler</div><div class="line">            kernel_thread(kernel_init) :  do_fork 启动一个 进程 执行 kernel_init 函数.  PID 为 1 的进程.   —&gt;  kernel_init_freeable函数   +   run_init_process</div><div class="line">            numa_default_policy : 内存分配  numa 策略.</div><div class="line">            kernel_thread(kthreadd) :   执行 kthreadd 函数.  PID 为 2 的 进程.  为spawn所有其它的 thread 进程 .</div><div class="line">            find_task_by_pid_ns</div><div class="line">            init_idle_bootup_task</div><div class="line">            schedule_preempt_disabled</div><div class="line">            cpu_startup_entry            ——&gt;   do_idle  cpu 调度        idle 为 PID 为 0 的进程.</div></pre></td></tr></table></figure></p>
<p><img src="/images/start_kernel_call_graph.svg" alt=""></p>
<p>内核代码中 调用 alloc_large_system_hash 函数的位置有不少，会构建一些 大的系统 hash table， 名称分别有：</p>
<ol>
<li>Dentry cache</li>
<li>Inode-cache</li>
<li>Mount-cache</li>
<li>Mountpoint-cache</li>
<li>PV qspinlock</li>
<li>futex</li>
<li>PID</li>
<li>TCP established</li>
<li>TCP bind</li>
<li>UDP  or  UDP-Lite</li>
</ol>
<p>—————————————中断 宏： SAVE_ALL &amp; RESTORE_ALL    用户态和内核态 上下文切换 ————————————<br>中断向量表：    <strong>linux/arch/x86/entry/systemcalls/syscall_32.tbl</strong><br>中断结构体：    <strong>irq_desc</strong></p>
<p>linux/arch/x86/entry/entry_32.S  : 系统调用的代码段,   入口应该是：<br><a href="http://lxr.linux.no/linux+v3.19/arch/x86/kernel/entry_32.S" target="_blank" rel="external">http://lxr.linux.no/linux+v3.19/arch/x86/kernel/entry_32.S</a>      —&gt;      ENTRY(system_call)</p>
<p>Linux 进程 fork 关键代码：<br><a href="http://lxr.linux.no/linux+v3.19/kernel/fork.c#L1185" target="_blank" rel="external">http://lxr.linux.no/linux+v3.19/kernel/fork.c#L1185</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Linux系统启动源代码分析/" class="archive-article-date">
  	<time datetime="2017-07-19T14:45:54.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/">Linux</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Linux/">Linux</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Linux编译+文件IO" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Linux编译+文件IO/">Linux编译 + 文件I/O</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="编译相关命令"><a href="#编译相关命令" class="headerlink" title="编译相关命令"></a>编译相关命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1. gcc -g -Wall hello_world.c -o hello_world          生成可执行程序</div><div class="line">2. gcc -E hello_world.c &gt; hello_world.i               预处理过程的输出文件, gcc -E 预处理结束之后，编译会自动停止</div><div class="line">3. gcc -S hello_world.c -o hello_world.s               对源代码进行语法分析，产生汇编文件, gcc -S 汇编结束之后，编译会自动停止</div><div class="line">4. gcc -g -Wall -v  hello_world.c -o hello_world    -v查看中间结果输出</div><div class="line">5. readelf -a hello_world                     Linux下可执行程序格式一般为ELF格式， readelf可以读取ELF格式的文件</div><div class="line">6. strace ./hello_world                    strace可以跟踪系统调用，从而可以了解应用程序加载／运行／退出 的过程。</div></pre></td></tr></table></figure>
<p>用户空间的程序默认是通过栈来传递参数的，但对于系统调用来说，内核态和用户态使用的是不同的栈，这使得系统调用的参数只能通过寄存器的方式进行传递。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">1. objdump -S thread_safe        可以对运行代码进行反汇编</div></pre></td></tr></table></figure></p>
<p>阻塞的系统调用：当进行系统调用时，除非出错或被信号打断，进程将会一直陷入内核态直到调用完成。<br>非阻塞的系统调用：是指无论I/O操作成功与否，调用都会立刻返回。</p>
<h3 id="文件I-O"><a href="#文件I-O" class="headerlink" title="文件I/O"></a>文件I/O</h3><ol>
<li>内核中 进程对应的数据结构是task_struct， 位于内核代码的 include/linux/sched.h 文件中。</li>
<li>内核中 文件表对应的数据结构是files_struct，位于内核代码的 include/linux/fdtable.h 文件中。</li>
<li>数据结构 file，位于内核代码 include/linux/fs.h 文件中。</li>
<li>Linux中的第一个进程init，位于内核代码的 include/linux/init_task.h 以及 fs/file.c 文件中。</li>
<li>所有进程都是由init进程fork出来的，代码位于 kernel/fork.c 文件中，该文件会 调用 include/linux/fdtable.h 以及 fs/file.c 文件中的 dup_fd 函数。</li>
<li>glibc中的open函数，调用内核中的 fs/open.c 文件的do_sys_open 函数，do_sys_open函数会使用get_unused_fd_flags函数，get_unused_fd_flags函数在include/linux/file.h是宏定义，实际调用 fs/file.c 中的alloc_fd函数。</li>
<li>内核会通过fs/open.c文件中的fd_install函数将文件管理结构file与fd结合起来，当用户使用fd与内核交互时，内核可以通过fd得到内部管理文件的结构struct file。</li>
<li>内核在文件close的时候，会关闭该fd，如果当前next_fd小于该fd，则将next_fd的直设置为该fd值。这是内核的文件描述符使用策略，尽可能的使用刚释放的较小的fd。</li>
<li>内核文件include/linux/fs.h中，file_operations 结构体定义了文件操作函数, inode_operations 结构体定义了linux存储inode操作函数。</li>
<li>文件打开但忘记close的时候，可能会出现两种情况：a. 文件描述符始终没有被释放；b. 用于文件管理的某些内存结构没有被释放。<br>a. 内核会扩展文件表，当文件表达到上限时，会报EMFILE错误。<br>b. 未超过上限时，可以申请空的内存；达到上限时，会报VFS: file-max limit reached的错误。</li>
</ol>
<h3 id="ELF文件"><a href="#ELF文件" class="headerlink" title="ELF文件"></a>ELF文件</h3><p>ELF = Executable and Linkable Format<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">readelf  命令 可以读 Linux 可执行文件.</div></pre></td></tr></table></figure></p>
<h3 id="glibc调用-system-call"><a href="#glibc调用-system-call" class="headerlink" title="glibc调用 system_call"></a>glibc调用 system_call</h3><p>glibc的源码中的 <strong>sysdep.h</strong>  里面有  <strong>INLINE_SYSCALL</strong> 定义</p>
<h3 id="Linux-创建-thread"><a href="#Linux-创建-thread" class="headerlink" title="Linux 创建 thread"></a>Linux 创建 thread</h3><p>Linux中实际上 thread 也是被当成 process 来创建, process和thread 是一样的, 区别在于系统调用时的传参.<br><img src="/images/Linux_pthread_create_1.png" alt=""></p>
<h3 id="Linux-boot-process"><a href="#Linux-boot-process" class="headerlink" title="Linux boot process"></a>Linux boot process</h3><p>refer： <a href="http://duartes.org/gustavo/blog/post/kernel-boot-process/" target="_blank" rel="external">http://duartes.org/gustavo/blog/post/kernel-boot-process/</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Linux编译+文件IO/" class="archive-article-date">
  	<time datetime="2017-07-19T14:24:27.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/">Linux</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Linux/">Linux</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Linux Kickstart Install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Linux Kickstart Install/">Linux Kickstart Install(无人值守安装)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Linux-Install"><a href="#Linux-Install" class="headerlink" title="Linux Install"></a>Linux Install</h3><ul>
<li>Linux dd：  driver disk 安装。</li>
<li>Linux ks：  kickstart 安装，无人值守安装。</li>
</ul>
<p>boot:  linux  ks=nfs:192.168.0.254:/var/ftp/pub/ks.cfg</p>
<p><strong>anaconda</strong> 是python写的系统安装工具。</p>
<p>Linux 中有 4个主分区 ＋ 逻辑分区(需要将其中一个主分区变为扩展分区)。</p>
<p>Installation过程中可以切换窗口，Ctrl ＋ Alt ＋ F1,2,3,4,5,6,每个窗口分别有不同的作用。</p>
<h3 id="Linux-Kickstart-Install"><a href="#Linux-Kickstart-Install" class="headerlink" title="Linux Kickstart Install"></a>Linux Kickstart Install</h3><p><strong>anaconda-ks.cfg</strong>       kickstart安装配置文件<br>python —&gt; anaconda —&gt; install Linux<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install pykickstart</div><div class="line">yum install system-config-kickstart          无人值守需要先安装rpm包</div></pre></td></tr></table></figure></p>
<p><strong>/tftpboot/n1.cfg配置文件：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">delay=0</div><div class="line">image=/tftpboot/osimage/rhels7.1-x86_64-install-gss1/vmlinuz</div><div class="line">   label=<span class="string">"Kickstart"</span></div><div class="line">   initrd=/tftpboot/osimage/rhels7.1-x86_64-install-gss1/initrd.img</div><div class="line">   append=<span class="string">"quiet inst.repo=http://%N:80/install/rhels7.1/x86_64 inst.ks=http://%N:80/install/autoinst/cn1 ip=eth0:dhcp  net.ifnames=0 net.ifnames=0  BOOTIF=%B"</span></div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Linux Kickstart Install/" class="archive-article-date">
  	<time datetime="2017-07-19T13:51:12.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/">Linux</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Linux/">Linux</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Linux奇形怪状的命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Linux奇形怪状的命令/">Linux一些命令总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="通过Sighup信号让Linux-Daemon重新加载配置文件"><a href="#通过Sighup信号让Linux-Daemon重新加载配置文件" class="headerlink" title="通过Sighup信号让Linux Daemon重新加载配置文件"></a>通过Sighup信号让Linux Daemon重新加载配置文件</h3><p>对于守护进程而言，一般情况下守护进程都是detach from shell，所以接受不到sighup信号，这个信号经常被用作 reload configuration左右。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">kill</span>  -s   SIGHUP  <span class="variable">$pid</span></div></pre></td></tr></table></figure></p>
<h3 id="Linux-syslog"><a href="#Linux-syslog" class="headerlink" title="Linux syslog"></a>Linux syslog</h3><p>Linux 系统中的主要log都在/var/log目录下：</p>
<ul>
<li>/var/log/lastlog:  用户登录系统记录日志。</li>
<li>/var/log/secure:  与Linux中身份验证相关的日志文件，比如：ssh。</li>
<li>/var/log/messages:  是Linux中许多应用的首选日志文件。</li>
<li>/var/log/maillog:  邮件相关。</li>
<li>/var/log/cron:  计划任务日志， /etc/cron.d/sysstat。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">klogd  —dmesg               —&gt; /var/<span class="built_in">log</span>/dmesg</div><div class="line">syslogd                     —&gt; /ect/syslog.conf</div><div class="line">两个共同的配置文件 —&gt; /etc/sysconfig/syslog</div></pre></td></tr></table></figure>
<p>redhat 中使用了   <strong>rsyslog</strong> 替代 syslog<br>/etc/rsyslog.d/*</p>
<h3 id="Linux-tty-ttyS-pts-X-Window"><a href="#Linux-tty-ttyS-pts-X-Window" class="headerlink" title="Linux tty/ttyS/pts + X-Window"></a>Linux tty/ttyS/pts + X-Window</h3><ul>
<li>tty：是串口的终端模拟器，模拟鼠标／键盘／显示器等设备。</li>
<li>ttyS：Linux下的真正的串口。</li>
<li>pts：虚拟的设备终端，由application（如：X-window，SSH）动态创建。 (X-window中的终端模拟器Teminal 也会创建相应的pts/#)</li>
<li>X-window： 是Linux的图形界面application。</li>
</ul>
<p>w: 显示当前所有的登录用户</p>
<p>多登陆端口通信：  （pts/1向pts/0发送消息）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">__pts/1:__  <span class="built_in">echo</span>  aaaaaaaaa  &gt;  /dev/pts/0</div><div class="line">__pts/0:__  aaaaaaaaa</div></pre></td></tr></table></figure></p>
<p>Alt＋F7: 打开第一个X-window窗口，Alt+F8 打开第二个</p>
<p>Linux启动的时候，会首先打开6个tty串口模拟器(Alt + F1-6) +++++ 1个X-window application(Alt + F7)。</p>
<p>startx: 启动X-window命令。   startx –:1 (打开第二个X-window)     startx –:2 (打开第三个X-window)         这个命令在pts虚拟终端中无法运行。</p>
<p>skill -9 pts/2  让另一个控制台的人掉线。</p>
<p>vim /etc/inittab     可以减少默认启动的tty个数！！！！</p>
<h3 id="Linux-打包及压缩工具"><a href="#Linux-打包及压缩工具" class="headerlink" title="Linux 打包及压缩工具"></a>Linux 打包及压缩工具</h3><p><strong>compress/uncompress</strong>     最古老的Unix压缩工具<br><strong>gzip/gunzip</strong>          最广泛的压缩工具，Linux系统中标准压缩工具，对于文本文件能够达到很高压缩率<br><strong>bzip2/bunzip2</strong>     新版Linux压缩工具，比gzip拥有更高的压缩率<br>tar 打包(备份)作用，参数：<br>     -c:  将文件备份create<br>     -v:  将过程输出verbose<br>     -x:  从一个文件中解出备份extract<br>     -r:  将文件添加入已经存在的文件中<br>     -C: 解出备份指向位置<br>     -z:  gzip压缩<br>     -j:  bzip2压缩<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar xvf/xvf/rvf</div></pre></td></tr></table></figure></p>
<h3 id="Linux-文件类型-7种-查询inode的命令"><a href="#Linux-文件类型-7种-查询inode的命令" class="headerlink" title="Linux 文件类型(7种) + 查询inode的命令"></a>Linux 文件类型(7种) + 查询inode的命令</h3><p>Linux 下文件类型共有7种：</p>
<ul>
<li>文件<br>d     文件夹<br>l     链接<br>b     block<br>c     char自负设备<br>s     socket文件<br>p     protocol网络文件</li>
</ul>
<p>文件分为三个部分进行存储：(目录文件中存储文件名)<br>1.存储文件名(指向inode号)   —&gt;   2.inode(指向块存储单元)   —&gt;  3.block (4k为单位)</p>
<p>inode 存储文件的属性，  可以用 <strong>stat</strong> 命令查看文件inode的内容。</p>
<h3 id="Linux-文件命令"><a href="#Linux-文件命令" class="headerlink" title="Linux 文件命令"></a>Linux 文件命令</h3><p>文本文件的操作命令：<br>cat： 查看文件内容<br>more： 逐屏查看文件内容<br>less： 逐行查看文件内容<br>head：显示文件开头部分内容<br>tail： 显示文件结尾部分内容<br>diff： 报告文件差异<br>uniq： 去除文件中的相邻的重复行<br>cut： 只显示文件中的某一行                         cut -d: -f1  /etc/passwd<br>sort：按序重排文本                                   sort  -t:  +2  -n  /etc/passwd<br>wc： 统计文件的行，词，字数</p>
<p>which  ls     :   查找命令,  查找 $PATH 路径中的文件<br>whereis  ls  :   可以查找命令及相应的man文件, 查找 $PATH 和 $MANPATH 路径<br>locate  ls :      查找所有匹配ls字母的文件，  注意!!!  locate 命令从数据库中查找文件，而不是直接查找系统文件，数据库位置为/var/lib/slocate/slocate.db.        当数据库没有更新的时候，可能会查找不到。<br>          yum install mlocate     !!!<br>          updatedb 更新locate 的数据库<br>          /etc/cron.daily/mlocate 每天有定时更新数据库的任务<br>find<br>grep<br>find  .  -iname  “xxx”  -ok   file  {}  \;        交互执行file命令<br>find  .  -iname  “xxx”  -exec   file  {}  \;      直接执行file命令</p>
<h3 id="Linux-系统启动"><a href="#Linux-系统启动" class="headerlink" title="Linux 系统启动"></a>Linux 系统启动</h3><p>系统启动顺序：<br>| /etc/inittab                    总的配置文件<br>|     /etc/rc.d/rc.sysinit                    系统初始化文件, 加载许多内容<br>          fsck.ext3   mount  -o  rw.remount   /dev/sda2  /<br>          mount  -a  /etc/fstab<br>|     /etc/rc.d/rcX.d/*                    X为系统运行级别<br>|          /etc/rc.d/rcX.d/ServiceXXX1  start               用户定义自启动的service<br>|          /etc/rc.d/rcX.d/ServiceXXX2  start<br>|     /etc/rc.d/rc.local          最后访问的文件<br>|<br>| minigetty   启动   /dev/tty1-6                                   启动tty<br>|           login    —&gt;  bash     —&gt;  /etc/profile     ~/.bash_profile<br>| gdm               监控进程是否死掉，重启进程</p>
<p>respawn          监控tty进程是否死掉，如果死掉会重启tty进程    respawn与init进程共同死，共同活<br>chkconfig  service  on/off –level X    将service 启动/关闭 脚本 加入/删除 到/etc/rc.d/rcX.d 目录下<br>加入到rcX.d 目录之后，可以用        service  Sname   start/stop/restart      来控制service</p>
<h3 id="Linux-一些命令"><a href="#Linux-一些命令" class="headerlink" title="Linux 一些命令"></a>Linux 一些命令</h3><p>shutdown -h now／init 0 ／ halt  -p -f ／ poweroff： 关机<br>users： 显示当前系统登录的用户<br>who： 当前登录在本机的用户及来源<br>w： 当前登录本机的用户及运行的程序<br>write： 给当前联机的用户发消息<br>wall： 给所有登录在本机的用户广播消息<br>last： 查看用户的登录日志<br>lastlog： 查看每个用户最后登录的情况<br>finger： 查看用户信息</p>
<h3 id="Linux-网络-网络内核参数"><a href="#Linux-网络-网络内核参数" class="headerlink" title="Linux 网络 + 网络内核参数"></a>Linux 网络 + 网络内核参数</h3><p><strong>ping -s 1024  www.baidu.com</strong>          -s 可以指定测试包的大小，用于测试不同包大小的带宽。<br><strong>ab -n 1000 -c 1000  www.baidu.com</strong>         ab为 linux压力测试的命令，模拟1000个端口，进行总共1000次请求          (Apache HTTP server benchmarking tool)<br><strong>traceroute  www.baidu.com</strong>                    查询整个转发路径上的访问结点的掉包率<br><strong>mtr  www.baidu.com</strong>                    查看通路的掉包率</p>
<p><strong>top</strong><br><strong>vmstat</strong>          (Report virtual memory statistics)<br><strong>netstat</strong>          (Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships)</p>
<p><strong>arping</strong>  查询哪个机器网卡ip地址是多少多少</p>
<p>ESTABLISHED特别多              <strong>CC(Challenge Collapsar)</strong> 攻击, 建立链接攻击<br>ESTABLISHED很少,LISTEN很多     <strong>DDOS</strong> 攻击  sync-flood(泛滥攻击)</p>
<ul>
<li>抓包工具：<br>iptraf<br>tcpdump<br>wireshark</li>
</ul>
<p>Linux 内核内核参数：<br>位于 /proc/sys 目录下, 修改内核参数：<br>echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all<br>sysctl  -w  net.ipv4.icmp_echo_ignore_all=1<br>sysctl  -p<br>输出内核参数：<br>sysctl  -a  &gt; /tmp/sysctl.output<br>从文件中读入内核参数：<br>sysctl  -f   /tmp/sysctl.output  -p</p>
<h3 id="rpm包管理"><a href="#rpm包管理" class="headerlink" title="rpm包管理"></a>rpm包管理</h3><ul>
<li><p>解压rpm包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm2cpio *.rpm | cpio -div</div></pre></td></tr></table></figure>
</li>
<li><p>查询rpm包里面内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@vmenable core]<span class="comment"># rpm -qpl *.rpm</span></div><div class="line">/etc/*-release</div><div class="line">/opt/*/swidtag</div><div class="line">/opt/*/swidtag/*.swidtag</div></pre></td></tr></table></figure>
</li>
<li><p>查询rpm包里面的install/uninstall script</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@vmenable core]<span class="comment"># rpm -qpl   *.x86_64.rpm  --scripts</span></div><div class="line">preinstall scriptlet (using /bin/sh):</div><div class="line">postinstall scriptlet (using /bin/sh):</div><div class="line">preuninstall program: /bin/sh</div><div class="line">postuninstall scriptlet (using /bin/sh):</div></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Linux奇形怪状的命令/" class="archive-article-date">
  	<time datetime="2017-07-19T13:28:39.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/">Linux</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Linux/">Linux</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-categories" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/categories/">目录</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a><a href="https://sstar1314.github.io/categories/Kafka/" target="_blank" rel="external">Kafka</a></h4><h4 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a><a href="https://sstar1314.github.io/categories/NoSQL/" target="_blank" rel="external">NoSQL</a></h4><h4 id="bigdata"><a href="#bigdata" class="headerlink" title="bigdata"></a><a href="https://sstar1314.github.io/categories/bigdata/" target="_blank" rel="external">bigdata</a></h4><h4 id="分布式协同"><a href="#分布式协同" class="headerlink" title="分布式协同"></a><a href="https://sstar1314.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E5%90%8C/" target="_blank" rel="external">分布式协同</a></h4>
      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/categories/" class="archive-article-date">
  	<time datetime="2017-07-19T09:45:04.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-tags" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/tags/">标签</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a><a href="https://sstar1314.github.io/tags/Cassandra/" target="_blank" rel="external">Cassandra</a></h4><h4 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a><a href="https://sstar1314.github.io/tags/Hadoop/" target="_blank" rel="external">Hadoop</a></h4><h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a><a href="https://sstar1314.github.io/tags/Kafka/" target="_blank" rel="external">Kafka</a></h4><h4 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a><a href="https://sstar1314.github.io/tags/NoSQL/" target="_blank" rel="external">NoSQL</a></h4><h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a><a href="https://sstar1314.github.io/tags/Spark/" target="_blank" rel="external">Spark</a></h4><h4 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a><a href="https://sstar1314.github.io/tags/ZooKeeper/" target="_blank" rel="external">ZooKeeper</a></h4><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a><a href="https://sstar1314.github.io/tags/etcd/" target="_blank" rel="external">etcd</a></h4>
      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/tags/" class="archive-article-date">
  	<time datetime="2017-07-19T09:45:04.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark源码阅读" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark源码阅读/">Spark源码阅读</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Spark-源代码-import-进-Eclipse"><a href="#Spark-源代码-import-进-Eclipse" class="headerlink" title="Spark 源代码 import 进 Eclipse"></a>Spark 源代码 import 进 Eclipse</h3><p>参考： <a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></p>
<p>到Spark源代码目录下，运行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mvn -DskipTests clean package</div><div class="line">mvn eclipse:eclipse</div></pre></td></tr></table></figure></p>
<p>Spark 是由 Scala语言编写，而Scala语言的语法糖太多，对于Scala新手的我来说确实困难，花费时间会较多，先暂时放一放。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark源码阅读/" class="archive-article-date">
  	<time datetime="2017-07-19T08:00:30.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Storm环境安装及配置" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Storm环境安装及配置/">Storm(流计算)环境安装及配置</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Storm-集群安装和配置"><a href="#Storm-集群安装和配置" class="headerlink" title="Storm 集群安装和配置"></a>Storm 集群安装和配置</h3><p><strong>Summary of the steps for setting up a Storm cluster:</strong></p>
<ul>
<li>Set up a Zookeeper cluster</li>
<li>Install dependencies on Nimbus and worker machines</li>
<li>Download and extract a Storm release to Nimbus and worker machines</li>
<li>Fill in mandatory configurations into storm.yaml</li>
<li>Launch daemons under supervision using “storm” script and a supervisor of your choice</li>
</ul>
<p><strong>配置：</strong><br><img src="/images/Storm_Config.png" alt=""></p>
<p><strong>源代码入口：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/storm nimbus                                   org.apache.storm.daemon.nimbus</div><div class="line">bin/storm supervisor                              org.apache.storm.daemon.supervisor</div><div class="line">bin/storm ui                                             org.apache.storm.ui.core</div></pre></td></tr></table></figure></p>
<p><strong>运行测试案例：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/storm   jar   examples/storm-starter/storm-starter-topologies-1.0.2.jar    org.apache.storm.starter.WordCountTopology</div></pre></td></tr></table></figure></p>
<p><strong>nimbus 与 supervisor之间通过zookeeper 进行通信</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[zk: localhost:2181(CONNECTED) 9] ls /storm</div><div class="line">[backpressure, workerbeats, nimbuses, supervisors, errors, logconfigs, storms, assignments, leader-lock, blobstore]</div></pre></td></tr></table></figure></p>
<h3 id="Integration-With-External-Systems-and-Other-Libraries"><a href="#Integration-With-External-Systems-and-Other-Libraries" class="headerlink" title="Integration With External Systems, and Other Libraries"></a>Integration With External Systems, and Other Libraries</h3><ul>
<li>Flux Data Driven Topology Builder</li>
<li>Apache Kafka Integration</li>
<li>Apache HBase Integration</li>
<li>Apache HDFS Integration</li>
<li>Apache Hive Integration</li>
<li>JDBC Integration</li>
<li>Redis Integration</li>
<li>Event Hubs Intergration</li>
<li>Kestrel Intergration</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Storm环境安装及配置/" class="archive-article-date">
  	<time datetime="2017-07-19T07:56:53.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS fsimage viewer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS fsimage viewer/">HDFS Offline Image Viewer (解析fsimage file)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Offline-Image-Viewer"><a href="#HDFS-Offline-Image-Viewer" class="headerlink" title="HDFS Offline Image Viewer"></a>HDFS Offline Image Viewer</h2><p>The Offiline Image Viewer is a tool to dump the contents of hdfs fsimage files.</p>
<p>command :</p>
<ul>
<li>Web Processor<br>Web processer launches a HTTP server :     <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs oiv -i fsimage</div></pre></td></tr></table></figure>
</li>
</ul>
<p>Users can access the viewer and get information of the fsimage:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -ls webhdfs://127.0.0.1:5978/</div></pre></td></tr></table></figure></p>
<ul>
<li>XML Processor<br>XML processer  is used to dump all the contents in the fsimage.<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs oiv -p XML -i fsimage -o fsimage.xml</div></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS fsimage viewer/" class="archive-article-date">
  	<time datetime="2017-07-19T07:54:16.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS edits log viewer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS edits log viewer/">HDFS Offline Edits Viewer (解析edits log 文件)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Offline-Edits-Viewer"><a href="#HDFS-Offline-Edits-Viewer" class="headerlink" title="HDFS Offline Edits Viewer"></a>HDFS Offline Edits Viewer</h2><p>Offline Edits Viewer is a tool to parse the Edits log file.</p>
<p>command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs oev -i edits -o edits.xml</div></pre></td></tr></table></figure></p>
<p><strong>Hadoop cluster recovery:</strong> This can be done by converting the binary edits to XML, edit it manually and then convert it back to binary.</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS edits log viewer/" class="archive-article-date">
  	<time datetime="2017-07-19T07:52:12.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HBase RESTful" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HBase RESTful/">HBase RESTful API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="启动-HBase-RESTful-服务"><a href="#启动-HBase-RESTful-服务" class="headerlink" title="启动 HBase RESTful 服务"></a>启动 HBase RESTful 服务</h2><p>hbase rest api 服务启动：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase-daemon.sh start rest</div></pre></td></tr></table></figure></p>
<p>Follow these instructions for each HBase host fulfilling the REST server role.</p>
<ul>
<li><p>To start the REST server as a foreground process, use the following command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/hbase rest start</div></pre></td></tr></table></figure>
</li>
<li><p>To start the REST server as a background process, use the following command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/hbase-daemon.sh start rest</div></pre></td></tr></table></figure>
</li>
<li><p>To use a different port than the default of 8080, use the -p option.</p>
</li>
<li>To stop a running HBase REST server, use the following command:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/hbase-daemon.sh stop rest</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Cluster-Wide-Endpoints"><a href="#Cluster-Wide-Endpoints" class="headerlink" title="Cluster-Wide Endpoints"></a>Cluster-Wide Endpoints</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/version/cluster</strong>    GET    Version of HBase running on this cluster<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:8080/version/cluster"</span></div><div class="line">```         </div><div class="line">__/status/cluster__	GET	Cluster status</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:8080/status/cluster"</span></div><div class="line">```         </div><div class="line">__/__	GET	List of all nonsystem tables</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:8080/"</span></div></pre></td></tr></table></figure></p>
<h2 id="Table-Endpoints"><a href="#Table-Endpoints" class="headerlink" title="Table Endpoints"></a>Table Endpoints</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/table/schema</strong>    GET    Describe the schema of the specified table.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/schema"</span></div><div class="line">```         </div><div class="line">__/table/schema__	POST	Create a new table, or replace an existing table<span class="string">'s schema with the provided schema</span></div><div class="line">```bash</div><div class="line">curl -vi -X POST \</div><div class="line">         -H "Accept: text/xml" \</div><div class="line">         -H "Content-Type: text/xml" \</div><div class="line">         -d '&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;&lt;TableSchema name=<span class="string">"users"</span>&gt;&lt;ColumnSchema name=<span class="string">"cf"</span> /&gt;&lt;/TableSchema&gt;<span class="string">' \</span></div><div class="line">         "http://example.com:20550/users/schema"</div><div class="line">```         </div><div class="line">__/table/schema__	UPDATE	Update an existing table with the provided schema fragment</div><div class="line">```bash</div><div class="line">curl -vi -X PUT \</div><div class="line">         -H "Accept: text/xml" \</div><div class="line">         -H "Content-Type: text/xml" \</div><div class="line">         -d '&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;&lt;TableSchema name=<span class="string">"users"</span>&gt;&lt;ColumnSchema name=<span class="string">"cf"</span> KEEP_DELETED_CELLS=<span class="string">"true"</span> /&gt;&lt;/TableSchema&gt;<span class="string">' \</span></div><div class="line">         "http://example.com:20550/users/schema"</div><div class="line">```         </div><div class="line">__/table/schema__	DELETE	Delete the table. You must use thetable/schemaendpoint, not just table/.</div><div class="line">```bash</div><div class="line">curl -vi -X DELETE \</div><div class="line">         -H "Accept: text/xml" \</div><div class="line">         "http://example.com:20550/users/schema"</div><div class="line">```         </div><div class="line">__/table/regions__	GET	List the table regions.</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H "Accept: text/xml" \</div><div class="line">         "http://example.com:20550/users/regions"</div></pre></td></tr></table></figure></p>
<h2 id="Endpoints-for-Get-Operations"><a href="#Endpoints-for-Get-Operations" class="headerlink" title="Endpoints for Get Operations"></a>Endpoints for Get Operations</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/table/row/column:qualifier/timestamp</strong>    GET    Get the value of a single row. Values are Base-64 encoded.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/row1"</span></div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/row1/cf:a/1458586888395"</span></div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/row1/cf:a"</span></div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">          <span class="string">"http://example.com:20550/users/row1/cf:a/"</span></div><div class="line">```          </div><div class="line">__/table/row/column:qualifier?v=number_of_versions__	 	Multi-Get a specified number of versions of a given cell. Values are Base-64 encoded.</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/row1/cf:a?v=2"</span></div></pre></td></tr></table></figure></p>
<h2 id="Endpoints-for-Scan-Operations"><a href="#Endpoints-for-Scan-Operations" class="headerlink" title="Endpoints for Scan Operations"></a>Endpoints for Scan Operations</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/table/scanner/</strong>    PUT    Get a Scanner object. Required by all other Scan operations. Adjust the batch parameter to the number of rows the scan should return in a batch. See the next example for adding filters to your Scanner. The scanner endpoint URL is returned as the Location in the HTTP response. The other examples in this table assume that the Scanner endpoint is<a href="http://example.com:20550/users/scanner/145869072824375522207" target="_blank" rel="external">http://example.com:20550/users/scanner/145869072824375522207</a>.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">curl -vi -X PUT \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         -H <span class="string">"Content-Type: text/xml"</span> \</div><div class="line">         -d <span class="string">'&lt;Scanner batch="1"/&gt;'</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/scanner/"</span></div><div class="line">```         </div><div class="line">__/table/scanner/__	PUT	To supply filters to the Scanner object or configure the Scanner <span class="keyword">in</span> any other way, you can create a text file and add your filter to the file. For example, to <span class="built_in">return</span> only rows <span class="keyword">for</span> <span class="built_in">which</span> keys start with u123and use a batch size of 100:</div></pre></td></tr></table></figure></p>
<p><scanner batch="100"><br>  <filter><br>    {<br>      “type”: “PrefixFilter”,<br>      “value”: “u123”<br>    }<br>  </filter><br></scanner><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Pass the file to the -d argument of the curl request.</div><div class="line">```bash</div><div class="line">curl -vi -X PUT \</div><div class="line">         -H &quot;Accept: text/xml&quot; \</div><div class="line">         -H &quot;Content-Type:text/xml&quot; \</div><div class="line">         -d @filter.txt \</div><div class="line">         &quot;http://example.com:20550/users/scanner/&quot;</div><div class="line">```         </div><div class="line">__/table/scanner/scanner_id__	GET	Get the next batch from the scanner. Cell values are byte-encoded. If the scanner is exhausted, HTTP status 204 is returned.</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H &quot;Accept: text/xml&quot; \</div><div class="line">         &quot;http://example.com:20550/users/scanner/145869072824375522207&quot;</div><div class="line">```         </div><div class="line">__/table/scanner/scanner_id__	DELETE	Deletes the scanner and frees the resources it was using.</div><div class="line">```bash</div><div class="line">curl -vi -X DELETE \</div><div class="line">         -H &quot;Accept: text/xml&quot; \</div><div class="line">         &quot;http://example.com:20550/users/scanner/145869072824375522207&quot;</div></pre></td></tr></table></figure></p>
<h2 id="Endpoints-for-Put-Operations"><a href="#Endpoints-for-Put-Operations" class="headerlink" title="Endpoints for Put Operations"></a>Endpoints for Put Operations</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/table/row_key/</strong>    PUT    Write a row to a table. The row, column qualifier, and value must each be Base-64 encoded. To encode a string, you can use the base64command-line utility. To decode the string, usebase64 -d. The payload is in the –data argument, so the/users/fakerowvalue is a placeholder. Insert multiple rows by adding them to the<cellset>element. You can also save the data to be inserted to a file and pass it to the -dparameter with the syntax -d @filename.txt.    XML:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">curl -vi -X PUT \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         -H <span class="string">"Content-Type: text/xml"</span> \</div><div class="line">         -d <span class="string">'&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;&lt;CellSet&gt;&lt;Row key="cm93NQo="&gt;&lt;Cell column="Y2Y6ZQo="&gt;dmFsdWU1Cg==&lt;/Cell&gt;&lt;/Row&gt;&lt;/CellSet&gt;'</span> \</div><div class="line">         <span class="string">"http://example.com:20550/users/fakerow"</span></div><div class="line">curl -vi -X PUT \</div><div class="line">         -H <span class="string">"Accept: text/json"</span> \</div><div class="line">         -H <span class="string">"Content-Type: text/json"</span> \</div><div class="line">         -d <span class="string">'&#123;"Row":[&#123;"key":"cm93NQo=", "Cell": [&#123;"column":"Y2Y6ZQo=", "$":"dmFsdWU1Cg=="&#125;]&#125;]&#125;'</span> \</div><div class="line">         <span class="string">"example.com:20550/users/fakerow"</span></div></pre></td></tr></table></figure></cellset></p>
<h2 id="Namespace-Endpoints"><a href="#Namespace-Endpoints" class="headerlink" title="Namespace Endpoints"></a>Namespace Endpoints</h2><p>Endpoint    HTTP Verb    Description    Example<br><strong>/namespaces</strong>    GET    List all namespaces.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/namespaces/"</span></div><div class="line">```         </div><div class="line">__/namespaces/namespace__	GET	Describe a specific namespace.</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/namespaces/special_ns"</span></div><div class="line">```         </div><div class="line">__/namespaces/namespace__	POST	Create a new namespace.</div><div class="line">```bash</div><div class="line">curl -vi -X POST \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"example.com:20550/namespaces/special_ns"</span></div><div class="line">```         </div><div class="line">__/namespaces/namespace/tables__	GET	List all tables <span class="keyword">in</span> a specific namespace.</div><div class="line">```bash</div><div class="line">curl -vi -X GET \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/namespaces/special_ns/tables"</span></div><div class="line">```         </div><div class="line">__/namespaces/namespace__	PUT	Alter an existing namespace. Currently not used.</div><div class="line">```bash</div><div class="line">curl -vi -X PUT \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"http://example.com:20550/namespaces/special_ns"</span></div><div class="line">```         </div><div class="line">__/namespaces/namespace__	DELETE	Delete a namespace. The namespace must be empty.</div><div class="line">```bash</div><div class="line">curl -vi -X DELETE \</div><div class="line">         -H <span class="string">"Accept: text/xml"</span> \</div><div class="line">         <span class="string">"example.com:20550/namespaces/special_ns"</span></div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HBase RESTful/" class="archive-article-date">
  	<time datetime="2017-07-19T07:37:57.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop Other Knowledge" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop Other Knowledge/">Hadoop Other Knowledge</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="HA-enable-QJM"><a href="#HA-enable-QJM" class="headerlink" title="HA enable QJM"></a>HA enable QJM</h3><p>Hadoop Cluster High Availability (HA) enable  QJM (Quorum Journal Manager): <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></p>
<h3 id="HA-enable-shared-storage"><a href="#HA-enable-shared-storage" class="headerlink" title="HA enable shared storage"></a>HA enable shared storage</h3><p>Hadoop Cluster High Availability (HA) enable  using shared storage: <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html</a></p>
<h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><p>Hadoop Cluster Service Level Authorization (ACL)<br>Hadoop Cluster enable HTTP/Web  ACL</p>
<h3 id="Hadoop-in-Secure-mode"><a href="#Hadoop-in-Secure-mode" class="headerlink" title="Hadoop in Secure mode"></a>Hadoop in Secure mode</h3><p>Integrate with <strong>Kerberos</strong> .</p>
<h3 id="滚动升级"><a href="#滚动升级" class="headerlink" title="滚动升级"></a>滚动升级</h3><p>Rolling Upgrade url link: <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html</a></p>
<p>HDFS Rolling Upgrade url link:  <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html</a></p>
<h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><p>Storage Policy url link: <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html</a></p>
<p>Memory Storage Support url link:  <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html</a></p>
<h3 id="KMS"><a href="#KMS" class="headerlink" title="KMS"></a>KMS</h3><p>Hadoop KMS(Key Management Server) url link:  <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-kms/index.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-kms/index.html</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop Other Knowledge/" class="archive-article-date">
  	<time datetime="2017-07-19T07:11:31.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Yarn RESTful" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Yarn RESTful/">Hadoop Yarn RESTful API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="ResourceManager-API"><a href="#ResourceManager-API" class="headerlink" title="ResourceManager API:"></a>ResourceManager API:</h4><p><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html</a></p>
<h4 id="NodeManager-API"><a href="#NodeManager-API" class="headerlink" title="NodeManager API:"></a>NodeManager API:</h4><p><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html</a></p>
<h4 id="MapReduce-Application-Master-API"><a href="#MapReduce-Application-Master-API" class="headerlink" title="MapReduce Application Master API:"></a>MapReduce Application Master API:</h4><p><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html</a></p>
<h4 id="History-Server-API"><a href="#History-Server-API" class="headerlink" title="History Server API:"></a>History Server API:</h4><p><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Yarn RESTful/" class="archive-article-date">
  	<time datetime="2017-07-19T07:02:21.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS ViewFS" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS ViewFS/">HDFS ViewFS (Based on Federation)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-ViewFs"><a href="#HDFS-ViewFs" class="headerlink" title="HDFS ViewFs"></a>HDFS ViewFs</h2><p>The View File System (ViewFs) provides a way to manage multiple Hadoop file system namespaces or namespace volumes.</p>
<h3 id="How-The-Clusters-Look"><a href="#How-The-Clusters-Look" class="headerlink" title="How The Clusters Look"></a>How The Clusters Look</h3><p>Suppose there are multiple cluster, each cluster has one or more namenodes. Each namenode has its own namespace, and a namenode belongs to one and only one cluster.</p>
<h3 id="A-Global-Namespace-Per-Cluster-Using-ViewFs"><a href="#A-Global-Namespace-Per-Cluster-Using-ViewFs" class="headerlink" title="A Global Namespace Per Cluster Using ViewFs"></a>A Global Namespace Per Cluster Using ViewFs</h3><p>ViewFs implements the Hadoop file system interface just like HDFS and the local file system. 代码位于Hadoop HDFS src 工程的 ViewFS继承类。<br><img src="/images/Hadoop_ViewFS_1.png" alt=""></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span>   </div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>viewfs://clusterX<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS ViewFS/" class="archive-article-date">
  	<time datetime="2017-07-19T06:52:26.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Yarn" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Yarn/">Hadoop ResourceManager -- Yarn</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-Yarn-architect"><a href="#Hadoop-Yarn-architect" class="headerlink" title="Hadoop Yarn architect"></a>Hadoop Yarn architect</h2><p><img src="/images/Yarn_1.png" alt=""></p>
<h3 id="Hadoop-V1-JobTracker-TaskTracker"><a href="#Hadoop-V1-JobTracker-TaskTracker" class="headerlink" title="Hadoop V1 JobTracker + TaskTracker"></a>Hadoop V1 JobTracker + TaskTracker</h3><p><img src="/images/Hadoop_V1_JobTracker.png" alt=""></p>
<h3 id="Hadoop-V2-ResourceManager-–-Yarn"><a href="#Hadoop-V2-ResourceManager-–-Yarn" class="headerlink" title="Hadoop V2 ResourceManager – Yarn"></a>Hadoop V2 ResourceManager – Yarn</h3><p><img src="/images/Hadoop_V2_ResourceManager.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Yarn/" class="archive-article-date">
  	<time datetime="2017-07-19T06:46:16.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS RESTful" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS RESTful/">HDFS RESTful API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="HDFS-RESTful-API"><a href="#HDFS-RESTful-API" class="headerlink" title="HDFS RESTful API"></a>HDFS RESTful API</h3><p>Port: 50070  也是 hdfs web portal的端口号.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">webhdfs://&lt;HOST&gt;:&lt;HTTP_PORT&gt;/&lt;PATH&gt;</div><div class="line">hdfs://&lt;HOST&gt;:&lt;RPC_PORT&gt;/&lt;PATH&gt;</div><div class="line">http://&lt;HOST&gt;:&lt;HTTP_PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=...</div></pre></td></tr></table></figure></p>
<h4 id="Authenticaiton"><a href="#Authenticaiton" class="headerlink" title="Authenticaiton"></a>Authenticaiton</h4><ol>
<li><p>Authentication when security is off:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?[user.name=&lt;USER&gt;&amp;]op=…"</span></div></pre></td></tr></table></figure>
</li>
<li><p>Authentication using Kerberos SPNEGO when security is on:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i --negotiate -u : <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=..."</span></div></pre></td></tr></table></figure>
</li>
<li><p>Authentication using Hadoop delegation token when security is on:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?delegation=&lt;TOKEN&gt;&amp;op=..."</span></div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="FileSystem-operations"><a href="#FileSystem-operations" class="headerlink" title="FileSystem operations:"></a>FileSystem operations:</h4><p>List  a  Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i  <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=LISTSTATUS"</span></div></pre></td></tr></table></figure></p>
<p>Status of  a  File/Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">curl -i  <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=GETFILESTATUS"</span></div><div class="line">```   </div><div class="line">Make  a  Directory :  </div><div class="line">```bash</div><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=MKDIRS[&amp;permission=&lt;OCTAL&gt;]"</span></div></pre></td></tr></table></figure></p>
<p>Delete  a  File/Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X DELETE <span class="string">"http://&lt;host&gt;:&lt;port&gt;/webhdfs/v1/&lt;path&gt;?op=DELETE[&amp;recursive=&lt;true|false&gt;]"</span></div></pre></td></tr></table></figure></p>
<p>Rename  a  File/Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=RENAME&amp;destination=&lt;PATH&gt;"</span></div></pre></td></tr></table></figure></p>
<p>Open and Read  a  File :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -L <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=OPEN[&amp;offset=&lt;LONG&gt;][&amp;length=&lt;LONG&gt;][&amp;buffersize=&lt;INT&gt;]"</span></div></pre></td></tr></table></figure></p>
<p>Create and Write to  a  File :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CREATE</span></div><div class="line">                    [&amp;overwrite=&lt;true|false&gt;][&amp;blocksize=&lt;LONG&gt;][&amp;replication=&lt;SHORT&gt;]</div><div class="line">                    [&amp;permission=&lt;OCTAL&gt;][&amp;buffersize=&lt;INT&gt;]"</div><div class="line">curl -i -X PUT -T &lt;LOCAL_FILE&gt; <span class="string">"http://&lt;DATANODE&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CREATE..."</span></div></pre></td></tr></table></figure></p>
<p>Append to  a  File :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl -i -X POST <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=APPEND[&amp;buffersize=&lt;INT&gt;]"</span></div><div class="line">curl -i -X POST -T &lt;LOCAL_FILE&gt; <span class="string">"http://&lt;DATANODE&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=APPEND…"</span></div></pre></td></tr></table></figure></p>
<p>Concatenate Files :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X POST <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CONCAT&amp;sources=&lt;PATHS&gt;"</span></div></pre></td></tr></table></figure></p>
<h4 id="Other-File-System-Operations"><a href="#Other-File-System-Operations" class="headerlink" title="Other File System Operations :"></a>Other File System Operations :</h4><p>Get Content Summary of  a Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=GETCONTENTSUMMARY"</span></div></pre></td></tr></table></figure></p>
<p>Get File Checksum :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=GETFILECHECKSUM"</span></div></pre></td></tr></table></figure></p>
<p>Get Home Directory :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/?op=GETHOMEDIRECTORY"</span></div></pre></td></tr></table></figure></p>
<p>Set Permission :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=SETPERMISSION[&amp;permission=&lt;OCTAL&gt;]"</span></div></pre></td></tr></table></figure></p>
<p>Set Owner :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=SETOWNER</span></div><div class="line">                              [&amp;owner=&lt;USER&gt;][&amp;group=&lt;GROUP&gt;]"</div></pre></td></tr></table></figure></p>
<p>Set Replication Factor :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=SETREPLICATION</span></div><div class="line">                              [&amp;replication=&lt;SHORT&gt;]"</div></pre></td></tr></table></figure></p>
<p>Set Access or Modification Time :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=SETTIMES</span></div><div class="line">                              [&amp;modificationtime=&lt;TIME&gt;][&amp;accesstime=&lt;TIME&gt;]"</div></pre></td></tr></table></figure></p>
<h4 id="Delegation-Token-Operations"><a href="#Delegation-Token-Operations" class="headerlink" title="Delegation Token Operations :"></a>Delegation Token Operations :</h4><p>Get Delegation Token :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/?op=GETDELEGATIONTOKEN&amp;renewer=&lt;USER&gt;"</span></div></pre></td></tr></table></figure></p>
<p>Renew Delegation Token :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&amp;token=&lt;TOKEN&gt;"</span></div></pre></td></tr></table></figure></p>
<p>Cancel Delegation Token :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -i -X PUT <span class="string">"http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/?op=CANCELDELEGATIONTOKEN&amp;token=&lt;TOKEN&gt;"</span></div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS RESTful/" class="archive-article-date">
  	<time datetime="2017-07-19T06:30:59.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS Federation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS Federation/">HDFS Federation(联邦)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Federation-联邦"><a href="#HDFS-Federation-联邦" class="headerlink" title="HDFS Federation(联邦)"></a>HDFS Federation(联邦)</h2><p>在前面的文章介绍过，Hadoop的Federation是将整个文件系统划分为子集，每一个Federation中的NameNode负责管理其中一个子集，整个文件系统由这些子集通过挂载mount的方式构建。 Federation与HA结合使用。</p>
<p>官方doc: <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html</a></p>
<h3 id="HDFS-has-two-main-layers"><a href="#HDFS-has-two-main-layers" class="headerlink" title="HDFS has two main layers:"></a>HDFS has two main layers:</h3><p><strong>Namespace :</strong></p>
<pre><code>1. Consists of directories, files and blocks.
2. It supports all the namespace related file system operations such as create, delete, modify and list files and directories.            
</code></pre><p><strong>Block Storage Services ,  this has two parts :</strong></p>
<pre><code>1. Block Management (performed in the Namenode)
  Provides Datanode cluster membership by handling registrations, and periodic heart beats.
  Processes block reports and maintains location of blocks.
  Supports block related operations such as create, delete, modify and get block location.
  Manages replica placement, block replication for under replicated blocks, and deletes blocks that are over replicated.   
2. Storage   -   is provided by Datanodes by storing blocks on the local file system and allowing read/write access.
</code></pre><h3 id="Multiple-Namenodes-Namespaces"><a href="#Multiple-Namenodes-Namespaces" class="headerlink" title="Multiple Namenodes/Namespaces"></a>Multiple Namenodes/Namespaces</h3><p>Federation uses multiple independent Namenodes/namespaces to scale the name service horizontally. <strong>The Namenodes are federated; the Namenodes are independent and do not require coordination with each other.</strong> The Datanodes are used as common storage for blocks by all the Namespaces. Each Datanode registers with all the Namenodes in the cluster. Datanodes send periodic heartbeats and block reports.<br><img src="/images/HDFS_Fedaration_1.png" alt=""></p>
<h3 id="Federation-Configuration"><a href="#Federation-Configuration" class="headerlink" title="Federation Configuration"></a>Federation Configuration</h3><p>Federation configuration is <strong>backward compatible</strong> and allows existing single Namenode configuration to work without any change.<br><strong>Step 1:</strong> Add the dfs.nameservices parameter to your configuration and configure it with a list of comma separated NameServiceIDs. This will be used by the Datanodes to determine the Namenodes in the cluster.<br><strong>Step 2:</strong> For each Namenode and Secondary Namenode/BackupNode/Checkpointer add the following configuration parameters suffixed with the corresponding NameServiceID into the common configuration file:<br><strong>Namenode</strong></p>
<ul>
<li>dfs.namenode.rpc-address</li>
<li>dfs.namenode.servicerpc-address</li>
<li>dfs.namenode.http-address</li>
<li>dfs.namenode.https-address</li>
<li>dfs.namenode.keytab.file</li>
<li>dfs.namenode.name.dir</li>
<li>dfs.namenode.edits.dir</li>
<li>dfs.namenode.checkpoint.dir</li>
<li>dfs.namenode.checkpoint.edits.dir<br><strong>Secondary Namenode</strong></li>
<li>dfs.namenode.secondary.http-address</li>
<li>dfs.secondary.namenode.keytab.file<br><strong>BackupNode</strong></li>
<li>dfs.namenode.backup.address</li>
<li>dfs.secondary.namenode.keytab.file</li>
</ul>
<h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml:"></a>hdfs-site.xml:</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1,ns2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host1:rpc-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host1:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondaryhttp-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>snn-host1:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host2:rpc-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host2:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondaryhttp-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>snn-host2:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="Formatting-Namenodes"><a href="#Formatting-Namenodes" class="headerlink" title="Formatting Namenodes"></a>Formatting Namenodes</h4><p>Step 1: Format a Namenode:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format [-clusterId &lt;cluster_id&gt;]</div></pre></td></tr></table></figure></p>
<p>Step 2: Format additional Namenodes<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format -clusterId &lt;cluster_id&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Upgrading-from-an-older-release-and-configuring-federation"><a href="#Upgrading-from-an-older-release-and-configuring-federation" class="headerlink" title="Upgrading from an older release and configuring federation"></a>Upgrading from an older release and configuring federation</h4><p>Older releases only support a single Namenode, after Upgrade the cluster to newer release in order to enable federation.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs start namenode --config <span class="variable">$HADOOP_CONF_DIR</span>  -upgrade -clusterId &lt;cluster_ID&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Adding-a-new-Namenode-to-an-existing-HDFS-cluster"><a href="#Adding-a-new-Namenode-to-an-existing-HDFS-cluster" class="headerlink" title="Adding a new Namenode to an existing HDFS cluster"></a>Adding a new Namenode to an existing HDFS cluster</h4><p>Perform the following steps:</p>
<ul>
<li>Add dfs.nameservices to the configuration.</li>
<li>Update the configuration with the NameServiceID suffix. Configuration key names changed post release 0.20. You must use the new configuration parameter names in order to use federation.</li>
<li>Add the new Namenode related config to the configuration file.</li>
<li>Propagate the configuration file to the all the nodes in the cluster.</li>
<li>Start the new Namenode and Secondary/Backup.</li>
<li>Refresh the Datanodes to pickup the newly added Namenode by running the following command against all the Datanodes in the cluster:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs dfsadmin -refreshNameNodes &lt;datanode_host_name&gt;:&lt;datanode_rpc_port&gt;</div></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS Federation/" class="archive-article-date">
  	<time datetime="2017-07-19T06:04:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS Snapshots" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS Snapshots/">HDFS Snapshots</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Snapshots"><a href="#HDFS-Snapshots" class="headerlink" title="HDFS Snapshots"></a>HDFS Snapshots</h2><p>Snapshots功能非常重要，可以保证HDFS在出现异常情况时可以进行恢复。 Snapshots可以使用在整个HDFS系统上，也可以只对其中的部分文件目录。</p>
<p>HDFS Snapshots are read-only point-in-time copies of file system. Snapshots can be taken on a subtree of the file system or the entire file system.</p>
<p>The HDFS path should be Snapshottable.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Snapshots Paths “.snapshot” is used <span class="keyword">for</span> accessing its snapshots.                hadoop  fs -ls /foo/.snapshot</div><div class="line">Allow Snapshots <span class="built_in">command</span>:                   hdfs  dfsadmin -allowSnapshot &lt;path&gt;</div><div class="line">Disallow Snapshots <span class="built_in">command</span>:             hdfs  dfsadmin  -disallowSnapshot &lt;path&gt;   </div><div class="line">Create Snapshots <span class="built_in">command</span>:                  hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</div><div class="line">Delete Snapshots <span class="built_in">command</span>:                  hdfs dfs -deleteSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</div><div class="line">Rename Snapshots <span class="built_in">command</span>:                  hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;</div><div class="line">Get Snapshottable Directory Listing <span class="built_in">command</span>:           hdfs lsSnapshottableDir</div><div class="line">Get Snapshots Difference Report <span class="built_in">command</span>:                 hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS Snapshots/" class="archive-article-date">
  	<time datetime="2017-07-19T06:00:59.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS安装(三种模式)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS安装(三种模式)/">HDFS安装(三种模式)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS 分布式文件系统"></a>HDFS 分布式文件系统</h2><ul>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
</ul>
<h2 id="Hadoop-HDFS-2-x-安装"><a href="#Hadoop-HDFS-2-x-安装" class="headerlink" title="Hadoop HDFS 2.x 安装"></a>Hadoop HDFS 2.x 安装</h2><p>Hadoop HDFS 2.x 包含了3种安装模式：</p>
<ol>
<li>Standalone. 独立模式</li>
<li>Pseudo-Distributed Operation. 伪分布式</li>
<li>Cluster. 集群</li>
</ol>
<h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>默认情况下，Hadoop被配置成以非分布式模式运行的一个独立Java进程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/java/openjdk1.8/</div><div class="line"><span class="built_in">cd</span>  hadoop_home/</div><div class="line">mkdir input</div><div class="line">cp etc/hadoop/*.xml input</div><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div><div class="line">cat output/*</div></pre></td></tr></table></figure></p>
<h3 id="Pseudo-Distributed-Operation"><a href="#Pseudo-Distributed-Operation" class="headerlink" title="Pseudo-Distributed Operation"></a>Pseudo-Distributed Operation</h3><p>Hadoop可以在单节点上以所谓的伪分布式模式运行，此时每一个Hadoop守护进程都作为一个独立的Java进程运行。</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>不配置Yarn</strong><br><strong>etc/hadoop/core-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/hdfs-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>Setup passphraseless ssh login:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ssh localhost</div><div class="line">ssh-keygen -t dsa -P <span class="string">''</span> -f ~/.ssh/id_dsa</div><div class="line">cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line">chmod 0600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>
<hr>
<h4 id="伪分布式HDFS初始化及使用命令"><a href="#伪分布式HDFS初始化及使用命令" class="headerlink" title="伪分布式HDFS初始化及使用命令"></a>伪分布式HDFS初始化及使用命令</h4><p>The following instructions are to run a MapReduce job locally.</p>
<ol>
<li><p>Format the filesystem: 初始化!!!</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs namenode -format</div></pre></td></tr></table></figure>
</li>
<li><p>Start NameNode daemon and DataNode daemon:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<p>The hadoop daemon log output is written to the $(HADOOP_LOG_DIR) directory (defaults to $(HADOOP_HOME)/logs).</p>
<ol>
<li>Browse the web interface for the NameNode; by default it is available at:<br> NameNode - <a href="http://localhost:50070/" target="_blank" rel="external">http://localhost:50070/</a></li>
<li><p>Make the HDFS directories required to execute MapReduce jobs:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -mkdir /user</div><div class="line">bin/hdfs dfs -mkdir /user/&lt;username&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>Copy the input files into the distributed filesystem:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -put etc/hadoop input</div></pre></td></tr></table></figure>
</li>
<li><p>Run some of the examples provided:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div></pre></td></tr></table></figure>
</li>
<li><p>Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -get output output</div><div class="line">cat output/*</div></pre></td></tr></table></figure>
</li>
</ol>
<p>or<br>View the output files on the distributed filesystem:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -cat output/*</div></pre></td></tr></table></figure></p>
<ol>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="配置Yarn"><a href="#配置Yarn" class="headerlink" title="配置Yarn"></a>配置Yarn</h4><p><strong>配置Yarn on a Single Node</strong><br><strong>etc/hadoop/mapred-site.xml</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/yarn-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<ol>
<li><p>Start ResourceManager daemon and NodeManager daemon:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-yarn.sh</div></pre></td></tr></table></figure>
</li>
<li><p>Browse the web interface for the ResourceManager; by default it is available at:<br>ResourceManager - <a href="http://localhost:8088/" target="_blank" rel="external">http://localhost:8088/</a></p>
</li>
<li>Run a MapReduce job.<br>与不配置Yarn的HDFS使用命令类似，可参考上面的例子。</li>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-yarn.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Cluster-Setup"><a href="#Cluster-Setup" class="headerlink" title="Cluster Setup"></a>Cluster Setup</h3><p>参考：<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</a><br>下载 hadoop2.7.3 版本的压缩包，解压缩到master节点上， 解压路径为 ${Hadoop_Install} .<br>配置 hadoop cluster 中各个节点之间的passwordless 无密码访问。</p>
<h4 id="Configure-Hadoop-Cluster"><a href="#Configure-Hadoop-Cluster" class="headerlink" title="Configure Hadoop Cluster"></a>Configure Hadoop Cluster</h4><p>到 ${Hadoop_Install}/etc/hadoop/ 目录下 编辑配置文件： <strong>core-site.xml</strong>  <strong>hdfs-site.xml</strong>  <strong>mapred-site.xml</strong>  <strong>yarn-site.xml</strong> .<br><strong>core-site.xml :  configure important parameters</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-nn:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>hdfs-site.xml :  configure for NameNode + DataNode</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>10240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>mapred-site.xml :  Configure for MapReduce Applications + MapReduce JobHistory Server</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>yarn-site.xml :  Configure for ResourceManager + NodeManager + History Server</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8025<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8035<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8050<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h4 id="Hadoop-Cluster-Startup"><a href="#Hadoop-Cluster-Startup" class="headerlink" title="Hadoop Cluster Startup"></a>Hadoop Cluster Startup</h4><p><strong>Format a new distributed filesystem:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format &lt;cluster_name&gt;</div></pre></td></tr></table></figure></p>
<p>会生成一个name文件夹，里面存储fsimage和editlog文件，记录整个cluster中的文件系统。<br><strong>Start HDFS NameNode :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start namenode</div></pre></td></tr></table></figure></p>
<p><strong>Start HDFS DataNode :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start datanode</div></pre></td></tr></table></figure></p>
<p><strong>Start all Hadoop slaves * :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-dfs.sh</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  ResourceManager :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start resourcemanager</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  NodeManager :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start nodemanager</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  WebAppProxy server if necessary:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start proxyserver</div></pre></td></tr></table></figure></p>
<p><strong>Start all  Yarn  slaves *:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-yarn.sh</div></pre></td></tr></table></figure></p>
<p><strong>Start MapReduce JobHistory server :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[mapred]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/mr-jobhistory-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start historyserver</div></pre></td></tr></table></figure></p>
<h4 id="Hadoop-Cluster-Web-Interfaces"><a href="#Hadoop-Cluster-Web-Interfaces" class="headerlink" title="Hadoop Cluster Web Interfaces"></a>Hadoop Cluster Web Interfaces</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NameNode     http://hadoop_namenode:50070/</div><div class="line">ResourceManager     http://hadoop_resourcemanager:8088/</div><div class="line">MapReduce JobHistory Server     http://jobhistory_serevr:19888/</div></pre></td></tr></table></figure>
<h4 id="Hadoop-Cluster-exclude-decommision-Datanodes"><a href="#Hadoop-Cluster-exclude-decommision-Datanodes" class="headerlink" title="Hadoop Cluster exclude/decommision Datanodes"></a>Hadoop Cluster exclude/decommision Datanodes</h4><p><strong>configure  hdfs-site.xml :</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hdfs_exclude.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>DFS exclude<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>Then write the decommission data node(slave2) to <strong>hdfs_exclude.txt</strong> file.<br>Last, force configure reload:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop dfsadmin  -refreshNodes</div><div class="line">hadoop dfsadmin  -report</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS安装(三种模式)/" class="archive-article-date">
  	<time datetime="2017-07-19T05:40:33.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop介绍/">Hadoop介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop主要包含以下部分"><a href="#Hadoop主要包含以下部分" class="headerlink" title="Hadoop主要包含以下部分"></a>Hadoop主要包含以下部分</h2><ul>
<li><strong>Hadoop Common</strong>: The common utilities that support the other Hadoop modules.</li>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
<li><strong>Hadoop YARN</strong>: A framework for job scheduling and cluster resource management.</li>
<li><strong>Hadoop MapReduce</strong>: A YARN-based system for parallel processing of large data sets.</li>
</ul>
<h2 id="Hadoop生态圈"><a href="#Hadoop生态圈" class="headerlink" title="Hadoop生态圈"></a>Hadoop生态圈</h2><ul>
<li><strong>Ambari™</strong>: 部署服务. A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.</li>
<li><strong>Avro™</strong>: 数据序列化. A data serialization system.</li>
<li><strong>Cassandra™</strong>: 分布式NoSQL. A scalable multi-master database with no single points of failure.</li>
<li><strong>Chukwa™</strong>: 数据收集. A data collection system for managing large distributed systems.</li>
<li><strong>HBase™</strong>: 分布式NoSQL. A scalable, distributed database that supports structured data storage for large tables.</li>
<li><strong>Hive™</strong>: 数据仓库. A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><strong>Mahout™:</strong> 数据挖掘. A Scalable machine learning and data mining library.</li>
<li><strong>Pig™</strong>: 可转化为MapReduce的数据查询. A high-level data-flow language and execution framework for parallel computation.</li>
<li><strong>Spark™</strong>: 内存加速的运算框架. A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.</li>
<li><strong>Tez™</strong>: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.</li>
<li><strong>ZooKeeper™</strong>: 分布式协同框架. A high-performance coordination service for distributed applications.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop介绍/" class="archive-article-date">
  	<time datetime="2017-07-19T05:18:07.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-ZooKeeper RESTful" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/ZooKeeper RESTful/">ZooKeeper RESTful</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Enable-ZooKeeper-RESTful-service："><a href="#Enable-ZooKeeper-RESTful-service：" class="headerlink" title="Enable ZooKeeper RESTful service："></a>Enable ZooKeeper RESTful service：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">vim  rest.properties</div><div class="line"><span class="built_in">cd</span>   /root/zookeeper-3.4.8/src/contrib/rest</div><div class="line">nohup ant run &amp;    </div><div class="line">curl -H<span class="string">'Accept: application/json'</span> http://localhost:9998/znodes/v1/                    json</div><div class="line">curl -H<span class="string">'Accept: application/xml'</span> http://localhost:9998/znodes/v1/                    xml</div></pre></td></tr></table></figure>
<h3 id="RESTful-用法："><a href="#RESTful-用法：" class="headerlink" title="RESTful 用法："></a>RESTful 用法：</h3><p>#get children of the root node<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl http://localhost:9998/znodes/v1/?view=children</div></pre></td></tr></table></figure></p>
<p>#get “/cluster1/leader” as xml (default is json)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -H<span class="string">'Accept: application/xml'</span> http://localhost:9998/znodes/v1/cluster1/leader</div></pre></td></tr></table></figure></p>
<p>#get the data as text<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster1/leader?dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<p>#set a node (data.txt contains the ascii text you want to set on the node)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -T data.txt -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster1/leader?dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<p>#create a node<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">"data1"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/?op=create&amp;name=cluster2&amp;dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">"data2"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster2?op=create&amp;name=leader&amp;dataformat=utf8"</span></div></pre></td></tr></table></figure>
<p>#create a new session<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">""</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/?op=create&amp;expire=10"</span></div></pre></td></tr></table></figure></p>
<p>#session heartbeat<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X <span class="string">"PUT"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/02dfdcc8-8667-4e53-a6f8-ca5c2b495a72"</span></div></pre></td></tr></table></figure></p>
<p>#delete a session<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X <span class="string">"DELETE"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/02dfdcc8-8667-4e53-a6f8-ca5c2b495a72"</span></div></pre></td></tr></table></figure></p>
<h2 id="service-启动命令"><a href="#service-启动命令" class="headerlink" title="service 启动命令"></a>service 启动命令</h2><p><strong>RestAPI源码入门：</strong><br>RestAPI 入口main函数所在文件： <strong>org.apache.zookeeper.server.jersey.RestMain</strong></p>
<p><strong>ZooKeeper Source Code 解析：</strong><br>1.zkServer 脚本启动命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ZOOMAIN=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=<span class="variable">$JMXLOCALONLY</span> org.apache.zookeeper.server.quorum.QuorumPeerMain"</span></div><div class="line"></div><div class="line">nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</div><div class="line">        -agentlib:jdwp=transport=dt_socket,server=y,<span class="built_in">suspend</span>=y,address=7778 \                         </div><div class="line">        -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</div></pre></td></tr></table></figure></p>
<p>2.zkCli 脚本启动命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</div><div class="line">     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$CLIENT_JVMFLAGS</span> <span class="variable">$JVMFLAGS</span> \</div><div class="line">     org.apache.zookeeper.ZooKeeperMain <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure></p>
<p>由上可知， ZooKeeper的server启动入口函数为 <strong>QuorumPeerMain</strong> ，而client的启动入口函数为 <strong>ZooKeeperMain</strong>。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/ZooKeeper RESTful/" class="archive-article-date">
  	<time datetime="2017-07-19T02:45:39.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ZooKeeper/">ZooKeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-分布式协同" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/分布式协同/">分布式协同技术</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="分布式协同技术"><a href="#分布式协同技术" class="headerlink" title="分布式协同技术"></a>分布式协同技术</h2><p>分布式协同技术诞生于分布式系统中，致力于解决各大分布式系统或分布式计算平台点到点的同步问题。 代表性的有 etcd, ZooKeeper, Consul, Doozerd。 其中：</p>
<h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><ul>
<li>golang 语言编写</li>
<li>coreos 公司研发</li>
<li>被mesos kubernetes等 热门分布式平台所应用</li>
<li>支持RESTful api</li>
<li>基于 Raft 算法<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3></li>
<li>java 语言编写</li>
<li>Apache 基金会maintain</li>
<li>被Hadoop Kafka等 热门分布式平台所应用</li>
<li>支持RESTful api</li>
<li>基于 Paxos 算法</li>
</ul>
<h2 id="分布式协同算法-Raft-和-Paxos"><a href="#分布式协同算法-Raft-和-Paxos" class="headerlink" title="分布式协同算法 Raft 和 Paxos"></a>分布式协同算法 Raft 和 Paxos</h2><p>推荐一个Raft算法动态描述的网站： <a href="https://raft.github.io/" target="_blank" rel="external">https://raft.github.io/</a></p>
<p>至于 Raft 和 Paxos 算法的区别，网上文章有一些，可以阅读一下，但是本人至今没仔细钻研过两个算法的区别，以后如果有时间再补上。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/分布式协同/" class="archive-article-date">
  	<time datetime="2017-07-19T01:36:10.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式协同/">分布式协同</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kubernetes源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Kubectl-go"><a href="#Kubectl-go" class="headerlink" title="Kubectl.go"></a>Kubectl.go</h1><p>1.logs.InitLogs()<br>k8s.io/kubernetes/pkg/util/logs 日志管理代码<br>每30秒刷新一次日志。 使用了github.com/golang/glog</p>
<ol>
<li>NewDefaultClientConfigLoadingRules<br>currentMigrationRules</li>
</ol>
<p>type Factory struct {<br>    clients <em>ClientCache<br>    flags   </em>pflag.FlagSet</p>
<pre><code>// Returns interfaces for dealing with arbitrary runtime.Objects.
Object func() (meta.RESTMapper, runtime.ObjectTyper)
// Returns interfaces for dealing with arbitrary
// runtime.Unstructured. This performs API calls to discover types.
UnstructuredObject func() (meta.RESTMapper, runtime.ObjectTyper, error)
// Returns interfaces for decoding objects - if toInternal is set, decoded objects will be converted
// into their internal form (if possible). Eventually the internal form will be removed as an option,
// and only versioned objects will be returned.
Decoder func(toInternal bool) runtime.Decoder
// Returns an encoder capable of encoding a provided object into JSON in the default desired version.
JSONEncoder func() runtime.Encoder
// ClientSet gives you back an internal, generated clientset
ClientSet func() (*internalclientset.Clientset, error)
// Returns a RESTClient for accessing Kubernetes resources or an error.
RESTClient func() (*restclient.RESTClient, error)
// Returns a client.Config for accessing the Kubernetes server.
ClientConfig func() (*restclient.Config, error)
// Returns a RESTClient for working with the specified RESTMapping or an error. This is intended
// for working with arbitrary resources and is not guaranteed to point to a Kubernetes APIServer.
ClientForMapping func(mapping *meta.RESTMapping) (resource.RESTClient, error)
// Returns a RESTClient for working with Unstructured objects.
UnstructuredClientForMapping func(mapping *meta.RESTMapping) (resource.RESTClient, error)
// Returns a Describer for displaying the specified RESTMapping type or an error.
Describer func(mapping *meta.RESTMapping) (kubectl.Describer, error)
// Returns a Printer for formatting objects of the given type or an error.
Printer func(mapping *meta.RESTMapping, options kubectl.PrintOptions) (kubectl.ResourcePrinter, error)
// Returns a Scaler for changing the size of the specified RESTMapping type or an error
Scaler func(mapping *meta.RESTMapping) (kubectl.Scaler, error)
// Returns a Reaper for gracefully shutting down resources.
Reaper func(mapping *meta.RESTMapping) (kubectl.Reaper, error)
// Returns a HistoryViewer for viewing change history
HistoryViewer func(mapping *meta.RESTMapping) (kubectl.HistoryViewer, error)
// Returns a Rollbacker for changing the rollback version of the specified RESTMapping type or an error
Rollbacker func(mapping *meta.RESTMapping) (kubectl.Rollbacker, error)
// Returns a StatusViewer for printing rollout status.
StatusViewer func(mapping *meta.RESTMapping) (kubectl.StatusViewer, error)
// MapBasedSelectorForObject returns the map-based selector associated with the provided object. If a
// new set-based selector is provided, an error is returned if the selector cannot be converted to a
// map-based selector
MapBasedSelectorForObject func(object runtime.Object) (string, error)
// PortsForObject returns the ports associated with the provided object
PortsForObject func(object runtime.Object) ([]string, error)
// ProtocolsForObject returns the &lt;port, protocol&gt; mapping associated with the provided object
ProtocolsForObject func(object runtime.Object) (map[string]string, error)
// LabelsForObject returns the labels associated with the provided object
LabelsForObject func(object runtime.Object) (map[string]string, error)
// LogsForObject returns a request for the logs associated with the provided object
LogsForObject func(object, options runtime.Object) (*restclient.Request, error)
// PauseObject marks the provided object as paused ie. it will not be reconciled by its controller.
PauseObject func(object runtime.Object) (bool, error)
// ResumeObject resumes a paused object ie. it will be reconciled by its controller.
ResumeObject func(object runtime.Object) (bool, error)
// Returns a schema that can validate objects stored on disk.
Validator func(validate bool, cacheDir string) (validation.Schema, error)
// SwaggerSchema returns the schema declaration for the provided group version kind.
SwaggerSchema func(unversioned.GroupVersionKind) (*swagger.ApiDeclaration, error)
// Returns the default namespace to use in cases where no
// other namespace is specified and whether the namespace was
// overridden.
DefaultNamespace func() (string, bool, error)
// Generators returns the generators for the provided command
Generators func(cmdName string) map[string]kubectl.Generator
// Check whether the kind of resources could be exposed
CanBeExposed func(kind unversioned.GroupKind) error
// Check whether the kind of resources could be autoscaled
CanBeAutoscaled func(kind unversioned.GroupKind) error
// AttachablePodForObject returns the pod to which to attach given an object.
AttachablePodForObject func(object runtime.Object) (*api.Pod, error)
// UpdatePodSpecForObject will call the provided function on the pod spec this object supports,
// return false if no pod spec is supported, or return an error.
UpdatePodSpecForObject func(obj runtime.Object, fn func(*api.PodSpec) error) (bool, error)
// EditorEnvs returns a group of environment variables that the edit command
// can range over in order to determine if the user has specified an editor
// of their choice.
EditorEnvs func() []string
// PrintObjectSpecificMessage prints object-specific messages on the provided writer
PrintObjectSpecificMessage func(obj runtime.Object, out io.Writer)
</code></pre><p>}</p>
<p>const (<br>    RunV1GeneratorName                          = “run/v1”<br>    RunPodV1GeneratorName                       = “run-pod/v1”<br>    ServiceV1GeneratorName                      = “service/v1”<br>    ServiceV2GeneratorName                      = “service/v2”<br>    ServiceNodePortGeneratorV1Name              = “service-nodeport/v1”<br>    ServiceClusterIPGeneratorV1Name             = “service-clusterip/v1”<br>    ServiceLoadBalancerGeneratorV1Name          = “service-loadbalancer/v1”<br>    ServiceAccountV1GeneratorName               = “serviceaccount/v1”<br>    HorizontalPodAutoscalerV1Beta1GeneratorName = “horizontalpodautoscaler/v1beta1”<br>    HorizontalPodAutoscalerV1GeneratorName      = “horizontalpodautoscaler/v1”<br>    DeploymentV1Beta1GeneratorName              = “deployment/v1beta1”<br>    DeploymentBasicV1Beta1GeneratorName         = “deployment-basic/v1beta1”<br>    JobV1Beta1GeneratorName                     = “job/v1beta1”<br>    JobV1GeneratorName                          = “job/v1”<br>    ScheduledJobV2Alpha1GeneratorName           = “scheduledjob/v2alpha1”<br>    NamespaceV1GeneratorName                    = “namespace/v1”<br>    ResourceQuotaV1GeneratorName                = “resourcequotas/v1”<br>    SecretV1GeneratorName                       = “secret/v1”<br>    SecretForDockerRegistryV1GeneratorName      = “secret-for-docker-registry/v1”<br>    SecretForTLSV1GeneratorName                 = “secret-for-tls/v1”<br>    ConfigMapV1GeneratorName                    = “configmap/v1”<br>)</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kubernetes源码/" class="archive-article-date">
  	<time datetime="2016-12-15T12:28:08.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-12-15</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka 源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka 源码/">Kafka 启动＋源代码import进Eclipse</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafka-源代码-import进Eclipse"><a href="#Kafka-源代码-import进Eclipse" class="headerlink" title="Kafka 源代码 import进Eclipse"></a>Kafka 源代码 import进Eclipse</h3><p><strong>Eclipse 安装 scala IDE 插件：</strong><br><a href="http://download.scala-ide.org/sdk/lithium/e44/scala211/stable/site" target="_blank" rel="external">http://download.scala-ide.org/sdk/lithium/e44/scala211/stable/site</a></p>
<p><strong>源代码 import进Eclipse</strong></p>
<ul>
<li>1.brew install gradle</li>
<li>2.gradle</li>
<li>3./gradlew eclipse      对应  ./gradlew idea</li>
</ul>
<h3 id="Kafka-启动"><a href="#Kafka-启动" class="headerlink" title="Kafka 启动"></a>Kafka 启动</h3><p>Step 1: Download the code</p>
<p>Download the 0.10.0.0 release and un-tar it.</p>
<blockquote>
<p>tar -xzf kafka_2.11-0.10.0.0.tgz</p>
<p>cd kafka_2.11-0.10.0.0</p>
</blockquote>
<p>Step 2: Start the server</p>
<p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p>
<blockquote>
<p>bin/zookeeper-server-start.sh config/zookeeper.properties<br>[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)<br>…</p>
</blockquote>
<p>Now start the Kafka server:</p>
<blockquote>
<p>bin/kafka-server-start.sh config/server.properties<br>[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)<br>[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)<br>…</p>
</blockquote>
<p>Step 3: Create a topic</p>
<p>Let’s create a topic named “test” with a single partition and only one replica:</p>
<blockquote>
<p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test</p>
</blockquote>
<p>We can now see that topic if we run the list topic command:</p>
<blockquote>
<p>bin/kafka-topics.sh –list –zookeeper localhost:2181</p>
</blockquote>
<p>test</p>
<p>Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.</p>
<p>Step 4: Send some messages</p>
<p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.</p>
<p>Run the producer and then type a few messages into the console to send to the server.</p>
<blockquote>
<p>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test<br>This is a message</p>
</blockquote>
<p>This is another message</p>
<p>Step 5: Start a consumer</p>
<p>Kafka also has a command line consumer that will dump out messages to standard output.</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –topic test –from-beginning<br>This is a message<br>This is another message</p>
</blockquote>
<p>If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.</p>
<p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p>
<p>Step 6: Setting up a multi-broker cluster</p>
<p>So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).</p>
<p>First we make a config file for each of the brokers:</p>
<blockquote>
<p>cp config/server.properties config/server-1.properties</p>
<p>cp config/server.properties config/server-2.properties</p>
</blockquote>
<p>Now edit these new files and set the following properties:</p>
<p>config/server-1.properties:<br>    broker.id=1<br>    listeners=PLAINTEXT://:9093<br>    log.dir=/tmp/kafka-logs-1</p>
<p>config/server-2.properties:<br>    broker.id=2<br>    listeners=PLAINTEXT://:9094<br>    log.dir=/tmp/kafka-logs-2</p>
<p>The broker.id property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.</p>
<p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p>
<blockquote>
<p>bin/kafka-server-start.sh config/server-1.properties &amp;<br>…<br>bin/kafka-server-start.sh config/server-2.properties &amp;<br>…</p>
</blockquote>
<p>Now create a new topic with a replication factor of three:</p>
<blockquote>
<p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 3 –partitions 1 –topic my-replicated-topic</p>
</blockquote>
<p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the “describe topics” command:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topic<br>Topic:my-replicated-topic      PartitionCount:1        ReplicationFactor:3    Configs:<br>        Topic: my-replicated-topic      Partition: 0    Leader: 1      Replicas: 1,2,0 Isr: 1,2,0</p>
</blockquote>
<p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p>
<ul>
<li>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li>
<li>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li>
<li>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li>
</ul>
<p>Note that in my example node 1 is the leader for the only partition of the topic.</p>
<p>We can run the same command on the original topic we created to see where it is:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test<br>Topic:test      PartitionCount:1        ReplicationFactor:1    Configs:<br>        Topic: test    Partition: 0    Leader: 0      Replicas: 0    Isr: 0</p>
</blockquote>
<p>So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p>
<p>Let’s publish a few messages to our new topic:</p>
<blockquote>
<p>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Now let’s consume these messages:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –from-beginning –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:</p>
<blockquote>
<p>ps | grep server-1.properties<br>7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java…<br>kill -9 7564</p>
</blockquote>
<p>Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topic<br>Topic:my-replicated-topic      PartitionCount:1        ReplicationFactor:3    Configs:<br>        Topic: my-replicated-topic      Partition: 0    Leader: 2      Replicas: 1,2,0 Isr: 2,0</p>
</blockquote>
<p>But the messages are still be available for consumption even though the leader that took the writes originally is down:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –from-beginning –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Step 7: Use Kafka Connect to import/export data</p>
<p>Writing data from the console and writing it back to the console is a convenient place to start, but you’ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data. Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs connectors, which implement the custom logic for interacting with an external system. In this quickstart we’ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file. First, we’ll start by creating some seed data to test with:</p>
<blockquote>
<p>echo -e “foo\nbar” &gt; test.txt</p>
</blockquote>
<p>Next, we’ll start two connectors running in standalone mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p>
<blockquote>
<p>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</p>
</blockquote>
<p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file. During startup you’ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from</p>
<p>test.txt</p>
<p>and producing them to the topic</p>
<p>connect-test</p>
<p>, and the sink connector should start reading messages from the topic</p>
<p>connect-test</p>
<p>and write them to the file</p>
<p>test.sink.txt</p>
<p>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p>
<blockquote>
<p>cat test.sink.txt<br>foo<br>bar</p>
</blockquote>
<p>Note that the data is being stored in the Kafka topic</p>
<p>connect-test</p>
<p>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –topic connect-test –from-beginning<br>{“schema”:{“type”:”string”,”optional”:false},”payload”:”foo”}<br>{“schema”:{“type”:”string”,”optional”:false},”payload”:”bar”}<br>…</p>
</blockquote>
<p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p>
<blockquote>
<p>echo “Another line” &gt;&gt; test.txt</p>
</blockquote>
<p>You should see the line appear in the console consumer output and in the sink file.</p>
<p>Step 8: Use Kafka Streams to process data</p>
<p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).</p>
<p>KTable wordCounts = textLines<br>    // Split each text line, by whitespace, into words.<br>    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(“\W+”)))</p>
<pre><code>// Ensure the words are available as record keys for the next aggregate operation.
.map((key, value) -&gt; new KeyValue&lt;&gt;(value, value))

// Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;.
.countByKey(&quot;Counts&quot;)
</code></pre><p>It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on aninfinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data.</p>
<p>We will now prepare input data to a Kafka topic, which will subsequently processed by a Kafka Streams application.</p>
<blockquote>
<p>echo -e “all streams lead to kafka\nhello kafka streams\njoin kafka summit” &gt; file-input.txt</p>
</blockquote>
<p>Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):</p>
<blockquote>
<p>bin/kafka-topics.sh –create \<br>            –zookeeper localhost:2181 \<br>            –replication-factor 1 \<br>            –partitions 1 \<br>            –topic streams-file-input</p>
<p>cat file-input.txt | bin/kafka-console-producer.sh –broker-list localhost:9092 –topic streams-file-input</p>
</blockquote>
<p>We can now run the WordCount demo application to process the input data:</p>
<blockquote>
<p>bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo</p>
</blockquote>
<p>There won’t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p>
<p>We can now inspect the output of the WordCount demo application by reading from its output topic:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 \<br>            –topic streams-wordcount-output \<br>            –from-beginning \<br>            –formatter kafka.tools.DefaultMessageFormatter \<br>            –property print.key=true \<br>            –property print.value=true \<br>            –property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \<br>            –property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</p>
</blockquote>
<p>with the following output data being printed to the console:</p>
<p>all    1<br>streams 1<br>lead    1<br>to      1<br>kafka  1<br>hello  1<br>kafka  2<br>streams 2<br>join    1<br>kafka  3<br>summit  1</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka 源码/" class="archive-article-date">
  	<time datetime="2016-11-10T06:08:52.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka Rest API" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka Rest API/">Kafka Rest API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafak-Connect-REST-API"><a href="#Kafak-Connect-REST-API" class="headerlink" title="Kafak Connect  REST API"></a>Kafak Connect  REST API</h3><p>Since Kafka Connect is intended to be run as a service, it also provides a REST API for managing connectors. By default this service runs on port 8083. The following are the currently supported endpoints:</p>
<ul>
<li><code>GET /connectors</code> - return a list of active connectors</li>
<li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string name field and a object config field with the connector configuration parameters</li>
<li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
<li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
<li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
<li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
<li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
<li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
<li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
<li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
<li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
<li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
<li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
</ul>
<p>Kafka Connect also provides a REST API for getting information about connector plugins:</p>
<ul>
<li><code>GET /connector-plugins-</code> return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
<li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka Rest API/" class="archive-article-date">
  	<time datetime="2016-11-10T05:59:23.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka/">Kafka</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafka-具有3大功能"><a href="#Kafka-具有3大功能" class="headerlink" title="Kafka 具有3大功能:"></a>Kafka 具有3大功能:</h3><ul>
<li>1.Publish &amp; Subscribe: to streams of data like a messaging system</li>
<li>2.Process: streams of data efficiently</li>
<li>3.Store: streams of data safely in a distributed replicated cluster</li>
</ul>
<p><img src="/images/Kafka_1.png" alt=""></p>
<p><img src="/images/Kafka_2.png" alt=""></p>
<h3 id="Producer-amp-Consumer-导图"><a href="#Producer-amp-Consumer-导图" class="headerlink" title="Producer &amp; Consumer 导图"></a>Producer &amp; Consumer 导图</h3><p><img src="/images/Kafka_3.png" alt=""></p>
<p><img src="/images/Kafka_4.png" alt=""></p>
<p><img src="/images/Kafka_5.png" alt=""></p>
<h3 id="Kafka-four-core-APIs"><a href="#Kafka-four-core-APIs" class="headerlink" title="Kafka four core APIs"></a>Kafka four core APIs</h3><p>Kafka has four core APIs:</p>
<ul>
<li>The Producer API allows an application to publish a stream records to one or more Kafka topics.</li>
<li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
<li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
<li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems.</li>
</ul>
<h3 id="Producer-APIs"><a href="#Producer-APIs" class="headerlink" title="Producer APIs"></a>Producer APIs</h3><p><em>kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.</em><br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/* Sends the data, partitioned by key to the topic using either the */</span></div><div class="line">  <span class="comment">/* synchronous or the asynchronous producer */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(kafka.javaapi.producer.ProducerData&lt;K,V&gt; producerData)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Sends a list of data, partitioned by key to the topic using either */</span></div><div class="line">  <span class="comment">/* the synchronous or the asynchronous producer */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(java.util.List&lt;kafka.javaapi.producer.ProducerData&lt;K,V&gt;&gt; producerData)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Closes the producer and cleans up */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li><p><strong>can handle queueing/buffering of multiple producer requests and asynchronous dispatch of the batched data</strong><br><code>kafka.producer.Producer</code> provides the ability to batch multiple produce requests (producer.type=async), before serializing and dispatching them to the appropriate kafka broker partition. The size of the batch can be controlled by a few config parameters. As events enter a queue, they are buffered in a queue, until either queue.time or batch.size is reached. A background thread (<code>kafka.producer.async.ProducerSendThread</code>) dequeues the batch of data and lets the <code>kafka.producer.EventHandler</code> serialize and send the data to the appropriate kafka broker partition. A custom event handler can be plugged in through the event.handler config parameter. At various stages of this producer queue pipeline, it is helpful to be able to inject callbacks, either for plugging in custom logging/tracing code or custom monitoring logic. This is possible by implementing the <code>kafka.producer.async.CallbackHandler</code> interface and setting <code>callback.handlerconfig</code> parameter to that class.</p>
</li>
<li><p><strong>handles the serialization of data through a user-specified Encoder:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">interface Encoder&lt;T&gt; &#123;</div><div class="line">  public Message toMessage(T data);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>The default is the no-op <code>kafka.serializer.DefaultEncoder</code></p>
<ul>
<li><strong>provides software load balancing through an optionally user-specified Partitioner:</strong></li>
</ul>
<p>The routing decision is influenced by the kafka.producer.Partitioner.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">interface Partitioner&lt;T&gt; &#123;</div><div class="line">   int partition(T key, int numPartitions);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Consumer-APIs"><a href="#Consumer-APIs" class="headerlink" title="Consumer APIs"></a>Consumer APIs</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleConsumer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/* Send fetch request to a broker and get back a set of messages. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> ByteBufferMessageSet <span class="title">fetch</span><span class="params">(FetchRequest request)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Send a list of fetch requests to a broker and get back a response set. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> MultiFetchResponse <span class="title">multifetch</span><span class="params">(List&lt;FetchRequest&gt; fetches)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Get a list of valid offsets (up to maxSize) before the given time.</div><div class="line">   * The result is a list of offsets, in descending order.</div><div class="line">   * <span class="doctag">@param</span> time: time in millisecs,</div><div class="line">   *              if set to OffsetRequest$.MODULE$.LATEST_TIME(), get from the latest offset available.</div><div class="line">   *              if set to OffsetRequest$.MODULE$.EARLIEST_TIME(), get from the earliest offset available.</div><div class="line">   */</div><div class="line">  <span class="keyword">public</span> <span class="keyword">long</span>[] getOffsetsBefore(String topic, <span class="keyword">int</span> partition, <span class="keyword">long</span> time, <span class="keyword">int</span> maxNumOffsets);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* create a connection to the cluster */</span></div><div class="line">ConsumerConnector connector = Consumer.create(consumerConfig);</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">interface</span> <span class="title">ConsumerConnector</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * This method is used to get a list of KafkaStreams, which are iterators over</div><div class="line">   * MessageAndMetadata objects from which you can obtain messages and their</div><div class="line">   * associated metadata (currently only topic).</div><div class="line">   *  Input: a map of &lt;topic, #streams&gt;</div><div class="line">   *  Output: a map of &lt;topic, list of message streams&gt;</div><div class="line">   */</div><div class="line">  <span class="keyword">public</span> Map&lt;String,List&lt;KafkaStream&gt;&gt; createMessageStreams(Map&lt;String,Int&gt; topicCountMap);</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * You can also obtain a list of KafkaStreams, that iterate over messages</div><div class="line">   * from topics that match a TopicFilter. (A TopicFilter encapsulates a</div><div class="line">   * whitelist or a blacklist which is a standard Java regex.)</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">public</span> List&lt;KafkaStream&gt; <span class="title">createMessageStreamsByFilter</span><span class="params">(</span></span></div><div class="line">      TopicFilter topicFilter, <span class="keyword">int</span> numStreams);</div><div class="line"></div><div class="line">  <span class="comment">/* Commit the offsets of all messages consumed so far. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">commitOffsets</span><span class="params">()</span></span></div><div class="line"></div><div class="line">  <span class="comment">/* Shut down the connector */</span></div><div class="line">  <span class="keyword">public</span> <span class="title">shutdown</span><span class="params">()</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Kafka-ZooKeeper文件目录"><a href="#Kafka-ZooKeeper文件目录" class="headerlink" title="Kafka ZooKeeper文件目录"></a>Kafka ZooKeeper文件目录</h3><p><strong>Broker Node Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/brokers/ids/[0...N] --&gt; &#123;&quot;jmx_port&quot;:...,&quot;timestamp&quot;:...,&quot;endpoints&quot;:[...],&quot;host&quot;:...,&quot;version&quot;:...,&quot;port&quot;:...&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Broker Topic Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/brokers/topics/[topic]/partitions/[0...N]/state --&gt; &#123;&quot;controller_epoch&quot;:...,&quot;leader&quot;:...,&quot;version&quot;:...,&quot;leader_epoch&quot;:...,&quot;isr&quot;:[...]&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Consumer Id Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/ids/[consumer_id] --&gt; &#123;&quot;version&quot;:...,&quot;subscription&quot;:&#123;...:...&#125;,&quot;pattern&quot;:...,&quot;timestamp&quot;:...&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Consumer Offsets:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/offsets/[topic]/[partition_id] --&gt; offset_counter_value ((persistent node)</div></pre></td></tr></table></figure></p>
<p><strong>Partition Owner registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/owners/[topic]/[partition_id] --&gt; consumer_node_id (ephemeral node)</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka/" class="archive-article-date">
  	<time datetime="2016-11-10T05:43:33.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-etcd介绍及源码分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/etcd介绍及源码分析/">etcd介绍及源码解析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="etcd-相关网站"><a href="#etcd-相关网站" class="headerlink" title="etcd 相关网站"></a>etcd 相关网站</h3><p><a href="https://coreos.com/etcd/docs/latest/" target="_blank" rel="external">https://coreos.com/etcd/docs/latest/</a><br><a href="https://coreos.com/etcd/docs/latest/api.html" target="_blank" rel="external">https://coreos.com/etcd/docs/latest/api.html</a><br><a href="https://coreos.com/etcd/docs/latest/libraries-and-tools.html" target="_blank" rel="external">https://coreos.com/etcd/docs/latest/libraries-and-tools.html</a></p>
<h3 id="Eclipse安装golang语言plugin："><a href="#Eclipse安装golang语言plugin：" class="headerlink" title="Eclipse安装golang语言plugin："></a>Eclipse安装golang语言plugin：</h3><p>Installation Requirements:</p>
<ul>
<li>Java VM version 8 or later.</li>
<li>Eclipse 4.6 (Neon) or later.</li>
<li>CDT 9.0 or later (this will be installed or updated automatically as part of the steps below).</li>
</ul>
<p>Instructions:</p>
<ul>
<li>Use your existing Eclipse, or download a new Eclipse package from <a href="http://www.eclipse.org/downloads/" target="_blank" rel="external">http://www.eclipse.org/downloads/</a>.<ul>
<li>For an Eclipse package without any other IDEs or extras (such a VCS tools), download the “Platform Runtime Binary”.</li>
</ul>
</li>
<li>Start Eclipse, go to Help -&gt; Install New Software…</li>
<li>Click the Add… button, then enter the Update Site URL: <a href="http://goclipse.github.io/releases/" target="_blank" rel="external">http://goclipse.github.io/releases/</a> in the Location field, click OK.</li>
<li>Select the recently added update site in the Work with: dropdown. Type GoClipse in the filter box. Now the Goclipse feature should appear below.</li>
<li>Select the GoClipse feature, and complete the wizard.<ul>
<li>Dependencies such as CDT will automatically be added during installation.</li>
</ul>
</li>
<li>Restart Eclipse.</li>
<li>Follow the instructions from the User Guide’s Configuration section to configure the required external tools. It is recommended you read the rest of the guide too.</li>
</ul>
<h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p><strong>客户端：client.go</strong></p>
<p>etcdmain/main.go  Main():<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div></pre></td><td class="code"><pre><div class="line">—&gt; etcdmain/etcd.go  startEtcdOrProxyV2():</div><div class="line">          —&gt; etcdmain/config.go  cfg := newConfig():               etcd初始化对象配置</div><div class="line">          —&gt; etcdmain/config.go  cfg.parse(Args[1:]):               解析etcd启动参数</div><div class="line">          —&gt; etcdmain/etcd.go  setupLogging(cfg)</div><div class="line">          —&gt; etcdmain/etcd.go  which := identifyDataDirOrDie(cfg.Dir)       分析Dir参数，解析etcd启动方式，如果Dir参数为空，也会走下面2个分支</div><div class="line">                    —&gt; case dirMember:</div><div class="line">                              —&gt; etcdmain/etcd.go  startEtcd(&amp;cfg.Config)</div><div class="line">                                        —&gt; embed/etcd.go  e, err := embed.StartEtcd(cfg)                         启动etcd服务</div><div class="line">                                                  —&gt; embed/config.go  cfg.Validate()                                             验证config值</div><div class="line">                                                  —&gt; embed/etcd.go  e.Peers := startPeerListeners(cfg)</div><div class="line">                                                            —&gt; plns = make([]net.Listener, len(cfg.LPUrls))</div><div class="line">                                                            —&gt; for i, u := range cfg.LPUrls</div><div class="line">                                                                      —&gt; pkg/transport/listener.go  tlscfg = cfg.PeerTLSInfo.ServerConfig()               CA file config</div><div class="line">                                                                      —&gt; rafthttp/util.go  plns[i] = rafthttp.NewListener(u, tlscfg)</div><div class="line">                                                                                —&gt; transport/timeout_listener.go  transport.NewTimeoutListener(u.Host, u.Scheme, tlscfg, ConnReadTimeout, ConnWriteTimeout)</div><div class="line">                                                                                          —&gt; pkg/transport/listener.go  ln := newListener(u.Host, u.Scheme)</div><div class="line">                                                                                                    —&gt; unix_listener.go       if (u.Scheme == “unix” | “unixs”)     return NewUnixListener(u.Host)               unix socket via unix://laddr</div><div class="line">                                                                                                    —&gt; dial.go                         else      return net.Listen(“tcp”, u.Host)                                        tcp socket</div><div class="line">                                                                                          —&gt; transport/timeout_listener.go  ln = &amp;rwTimeoutListener&#123;&#125;</div><div class="line">                                                                                          —&gt; pkg/transport/listener.go  ln = wrapTLS(u.Host, u.Scheme, tlscfg, ln)</div><div class="line">                                                                                                    —&gt; tls.go  tlscfg.NewListener(ln, tlscfg)                                                  创建listener，该listener接受inner连接               tls：Transport Layer Security 传输层安全协议</div><div class="line">                                                  —&gt; embed/etcd.go  e.sctxs := startClientListeners(cfg)</div><div class="line">                                                            —&gt; if  cfg.ClientAutoTLS &amp;&amp; cfg.ClientTLSInfo.Empty()     cfg.ClientTLSInfo = transport.SelfCert(path.Join(cfg.Dir, “fixtures/client"), cfg.LCurls)               配置tls信息</div><div class="line">                                                            —&gt; sctxs = make(map[string] *serveCtx)</div><div class="line">                                                            —&gt; sctx.l = net.Listen(proto, cfg.LCurls)                         proto=“tcp|unix|unixs"      创建socket 进行listen</div><div class="line">                                                            —&gt; sctx.l = transport.LimitListener(sctx.l, int(fdLimit - reservedInternalFDNum))                    设置listen limit上限值</div><div class="line">                                                            —&gt; pkg/transport/keepalive_listener.go  sctx.l = transport.NewKeepAliveListener(sctx.l, “tcp”, cfg)                                             创建socket</div><div class="line">                                                                      —&gt; newTLSKeepaliveListener(sctx.l, cfg)</div><div class="line">                                                  —&gt; e.Clients = append(e.Clients, e.sctxs.l)</div><div class="line">                                                  —&gt; srvcfg := &amp;etcdserver.ServerConfig&#123;cfg&#125;</div><div class="line">                                                  —&gt; etcdserver/server.go  e.Server = etcdserver.NewServer(srvcfg)</div><div class="line">                                                            —&gt; store/store.go  st := store.New(StoreClusterPrefix, StoreKeysPrefix)</div><div class="line">                                                                      —&gt; store/store.go  s := newStore(namespaces ...)</div><div class="line">                                                                                —&gt; s := new(store)</div><div class="line">                                                                                —&gt; s.Root = newDir(s, “/“, s.CurrentIndex, Permanent)</div><div class="line">                                                                                —&gt; for namespace := range namespaces        s.Root.Add(newDir(s, namespace, s.CurrentIndex, s.Root, Permanent))</div><div class="line">                                                                                —&gt; s.Stats = newStats()</div><div class="line">                                                                                —&gt; s.WatcherHub = newWatchHub(1000)</div><div class="line">                                                                                —&gt; s.ttlKeyHeap = newTtlKeyHeap()</div><div class="line">                                                                                —&gt; s.readonlySet =  types.NewUnsafeSet(append(namespaces, “/“) ...)</div><div class="line">                                                                      —&gt; s.clock = clockwork.NewRealClock()</div><div class="line">                                                            —&gt; haveWAL := wal.Exist(cfg.WALDir())</div><div class="line">                                                            —&gt; ss := snap.New(cfg.SnapDir())</div><div class="line">                                                            —&gt; bepath := path.Join(cfg.SnapDir(), databaseFilename)</div><div class="line">                                                            —&gt; mvcc/backend/backend.go  be := backend.NewDefaultBackend(bepath)</div><div class="line">                                                                      —&gt; newBackend(bepath, defaultBatchInterval, defaultBatchLimit)</div><div class="line">                                                                                —&gt; db := bolt.Open(bepath, 0600, boltOpenOptions)</div><div class="line">                                                                                —&gt; b := &amp;backend&#123;&#125;</div><div class="line">                                                                                —&gt; mvcc/backend/batch_tx.go  b.batchTx = newBatchTx(b)</div><div class="line">                                                                                          —&gt; tx := &amp;batchTx&#123;b&#125;</div><div class="line">                                                                                          —&gt; tx.Commit()</div><div class="line">                                                                                —&gt; go b.run()</div><div class="line">                                                                                          —&gt; t := time.NewTimer(b.batchInterval)</div><div class="line">                                                                                          —&gt; for</div><div class="line">                                                                                                    —&gt; select</div><div class="line">                                                                                                              —&gt; case &lt;- t,C:</div><div class="line">                                                                                                              —&gt; case &lt;- b.stopc:</div><div class="line">                                                                                                                        —&gt; b.batchTx.CommitAndStop()</div><div class="line">                                                                                                    —&gt; b.batchTx.Commit()</div><div class="line">                                                                                                    —&gt; b.Reset(b.batchInterval)</div><div class="line">                                                            —&gt; rafthttp/util.go  prt := rafthttp.NewRoundTripper(cfg.PeerTLSInfo, cfg.peerDialTimeout())</div><div class="line">                                                                      —&gt; pkg/transport/timeout_transport.go  transport.NewTimoutTransport(cfg.PeerTLSInfo, cfg.peerDialTimeout())</div><div class="line">                                                                                —&gt; tr := NewTransport(info, dialtimeoutd)</div><div class="line">                                                                                —&gt; tr.Dial =(&amp;rwTimeoutDialer&#123;&#125;).Dial</div><div class="line">                                                            —&gt; swith</div><div class="line">                                                                      —&gt; case !haveWAL &amp;&amp; !cfg.NewCluster</div><div class="line">                                                                                —&gt; cfg.VerifyJoinExisting()</div><div class="line">                                                                                —&gt; cl = membership.NewClusterFromURLsMap(cfg.InitialClusterToken, cfg.InitialPeerURLsMap)</div><div class="line">                                                                                —&gt; existingCluster = GetClusterFromRemotePeers(getRemotePeerURLs(cl, cfg.Name), prt)</div><div class="line">                                                                                —&gt; membership.ValidateClusterAndAssignIDs(cl, existingCluster)</div><div class="line">                                                                                —&gt; isCompatibleWithCluster(cl, cl.MemberByName(cfg.Name).ID, prt)</div><div class="line">                                                                                —&gt; remotes = existingCluster.Members()</div><div class="line">                                                                                —&gt; cl.SetID(existingCluster.ID())</div><div class="line">                                                                                —&gt; cl.SetStore(st)</div><div class="line">                                                                                —&gt; cl.SetBackend(be)</div><div class="line">                                                                                —&gt; cfg.Print()</div><div class="line">                                                                                —&gt; id, n, s, w = startNode(cfg, cl)</div><div class="line">                                                                      —&gt; case !haveWAL &amp;&amp; cfg.NewCluster</div><div class="line">                                                                                —&gt; cfg.VerifyBootstrape()</div><div class="line">                                                                                —&gt; cl = membership.NewClusterFromURLsMap(cfg.InitialClusterToken, cfg.InitialPeerURLsMap)</div><div class="line">                                                                                —&gt; m := cl.MemberByName(cfg.Name)</div><div class="line">                                                                                —&gt; isMemberBootstrapped(cl, cfg.Name, prt, cfg.bootstrapTimeout())</div><div class="line">                                                                                —&gt; if cfg.ShouldDiscover()</div><div class="line">                                                                                          —&gt; str = discovery.JoinCluster(cfg.DiscoveryURL, cfg.DiscoveryProxy, m.ID, cfg.InitialPeerURLsMap.String())</div><div class="line">                                                                                          —&gt; urlsmap = types.NewURLsMap(str)</div><div class="line">                                                                                          —&gt; checkDuplicateURL(urlsmap)</div><div class="line">                                                                                          —&gt; cl = membership.NewClusterFromURLsMap(cfg.InitoalClusterToken, urlsmap)</div><div class="line">                                                                                —&gt; cl.SetStore(st)</div><div class="line">                                                                                —&gt; cl.SetBackend(be)</div><div class="line">                                                                                —&gt; cfg.PrintWithInitial()</div><div class="line">                                                                                —&gt; id, n, s, w = startNode(cfg, cl, cl.MemberIDs())</div><div class="line">                                                                      —&gt; case haveWAL:</div><div class="line">                                                                                —&gt; fileutil.IsDirWriteable(cfg.MemberDir())</div><div class="line">                                                                                —&gt; fileutil.IsDirWriteable(cfg.WALDir())</div><div class="line">                                                                                —&gt; snapshot = ss.Load()</div><div class="line">                                                                                —&gt; st.Recovery(snapshot.Data)</div><div class="line">                                                                                —&gt; cfg.Print()</div><div class="line">                                                                                —&gt; if !cfg.ForceNewCluster</div><div class="line">                                                                                          —&gt; id, cl, n, s, w = restartNode(cfg, snapshot)</div><div class="line">                                                                                —&gt; else</div><div class="line">                                                                                          —&gt; id, cl, n, s, w = restartAsStandaloneNode(cfg, snapshot)</div><div class="line">                                                                                —&gt; cl.SetStore(st)</div><div class="line">                                                                                —&gt; cl.SetBackend(be)</div><div class="line">                                                                                —&gt; cl.Recover()</div><div class="line">                                                            —&gt; sstats := &amp;stats.ServerStats&#123;&#125;</div><div class="line">                                                            —&gt; sstats.Initialize()</div><div class="line">                                                            —&gt; lstats := stats.NewLeaderStats(id.String())</div><div class="line">                                                            —&gt; srv = &amp;EtcdServer&#123;&#125;</div><div class="line">                                                            —&gt; srv.applyV2 = &amp;applierV2store&#123;&#125;</div><div class="line">                                                            —&gt; srv.be = be</div><div class="line">                                                            —&gt; srv.lessor = lease.NewLessor(srv.be)</div><div class="line">                                                            —&gt; srv.kv = mvcc.New(src.be, srv.lessor, &amp;srv.consistIndex)</div><div class="line">                                                            —&gt; srv.consistIndex.setConsistentIndex(srv.kv.ConsistentIndex())</div><div class="line">                                                            —&gt; srv.authStore = auth.NewAuthStore(srv.be)</div><div class="line">                                                            —&gt; h := cfg.AutoCompactionRetention</div><div class="line">                                                            —&gt; srv.compactor = compactor.NewPeriodic(h, srv,kv, srv)</div><div class="line">                                                            —&gt; tr := &amp;rafthttp.Transport&#123;&#125;</div><div class="line">                                                            —&gt; tr.Start()</div><div class="line">                                                            —&gt; for m := range  remotes      tr.AddRemote(m.ID, m.PeerURLs)</div><div class="line">                                                            —&gt; for m := range  cl.Members()      tr.AddPeer(m.ID, m.PeerURLs)</div><div class="line">                                                            —&gt; srv.r.transport = tr</div><div class="line">                                                  —&gt; etcdserver/server.go  e.Server.Start()</div><div class="line">                                        —&gt; interrupt_unix.go  osutil.RegisterInterruptHandler(e.Server.Stop)               向系统注册中断事件</div><div class="line">                    —&gt; case dirProxy:</div><div class="line">                              —&gt; etcdmain/etcd.go  startProxy(&amp;cfg.Config)</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/etcd介绍及源码分析/" class="archive-article-date">
  	<time datetime="2016-11-10T05:31:40.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/etcd/">etcd</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-ZooKeeper源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/ZooKeeper源码/">ZooKeeper源码</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="ZooKeeper-Eclipse-编译："><a href="#ZooKeeper-Eclipse-编译：" class="headerlink" title="ZooKeeper Eclipse 编译："></a>ZooKeeper Eclipse 编译：</h2><p>1.源码下载：git clone <a href="https://github.com/apache/zookeeper.git" target="_blank" rel="external">https://github.com/apache/zookeeper.git</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git checkout branch-3.4  (zookeeper版本选择3.4.8进行学习)</div></pre></td></tr></table></figure></p>
<p>2.通过Eclipse新建一个Java project, 将project prosition 设置为source code download的位置，java project 创建成果之后，右键build.xml run as Ant build，会自动下载build project的jar包。<br>多个文件目录下都有build.xml文件，都可以尝试run as Ant build，下载build project所需要的jar包。<br>3.运行build.xml之后，jar包或许仍不完整，需要自己下载jar包，目录截图如下：<br><img src="/images/ZooKeeper_Code_1.png" alt=""><br>4.至此，zookeeper java工程的编译任务完成。</p>
<h3 id="Linux-下同样需要ant编译："><a href="#Linux-下同样需要ant编译：" class="headerlink" title="Linux 下同样需要ant编译："></a>Linux 下同样需要ant编译：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">apt-get install openjdk-7-jdk</div><div class="line"><span class="built_in">cd</span> /root/zookeeper-3.4.8</div><div class="line">ant</div></pre></td></tr></table></figure>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p><strong>RestAPI源码入门：</strong><br>RestAPI 入口main函数所在文件： org.apache.zookeeper.server.jersey.RestMain</p>
<p><strong>ZooKeeper Source Code 解析：</strong><br>1.zkServer 脚本启动命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \</div><div class="line">-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=7778 \                         （此行是用作java remote debug使用的，是我后加的）</div><div class="line">-cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp;</div><div class="line"> ZOOMAIN=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY org.apache.zookeeper.server.quorum.QuorumPeerMain&quot;</div></pre></td></tr></table></figure></p>
<p>2.zkCli 脚本启动命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \</div><div class="line"> -cp &quot;$CLASSPATH&quot; $CLIENT_JVMFLAGS $JVMFLAGS \</div><div class="line"> org.apache.zookeeper.ZooKeeperMain &quot;$@&quot;</div></pre></td></tr></table></figure></p>
<p>由上可知， ZooKeeper的server启动入口函数为 <strong>QuorumPeerMain</strong> ，而client的启动入口函数为 <strong>ZooKeeperMain</strong>。</p>
<h3 id="QuorumPeerMain解析源码："><a href="#QuorumPeerMain解析源码：" class="headerlink" title="QuorumPeerMain解析源码："></a>QuorumPeerMain解析源码：</h3><p>main函数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div></pre></td><td class="code"><pre><div class="line">—&gt; QuorumPeerMain main = new QuorumPeerMain()</div><div class="line">—&gt; main.initializeAndRun(args)</div><div class="line">          —&gt; QuorumPeerConfig config = new QuorumPeerConfig()</div><div class="line">          —&gt; config.parse(args)</div><div class="line">                    —&gt; parseProperties(cfg)</div><div class="line">          —&gt; DatadirCleanupManager purgeMgr = new DatadirCleanupManager(config)</div><div class="line">          —&gt; purgeMgr.start()</div><div class="line">                    —&gt; timer = new Timer(“PurgeTask<span class="string">")</span></div><div class="line">                    —&gt; TimerTask task = new PurgeTask(dataLogDir, snapDir)     —&gt; PurgeTxnLog.purge(dataLogDir, snapDir)</div><div class="line">                    —&gt; timer.scheduleAtFixedRate(task)</div><div class="line">          —&gt; runFromConfig(config)</div><div class="line">                    —&gt; ServerCnxnFactory cnxnFactory =  ServerCnxnFactory.createFactory()               NIOServerCnxnfactory</div><div class="line">                    —&gt; cnxnFactory.configure</div><div class="line">                              —&gt; thread = new ZooKeeperThread (Runnable)     参数Runnable是一个线程，传入的是cnxnFactory</div><div class="line">                              —&gt; ServerSocketChannel.open()</div><div class="line">                    —&gt; quorumPeer = new QuorumPeer()          是一个线程</div><div class="line">                    —&gt; quorumPeer.setZKDatabase (new ZKDatabase(quorumPeer.getTxnFactory()) )</div><div class="line">                              —&gt; new ZKDatabase()</div><div class="line">                                        —&gt; dataTree = new DataTree()</div><div class="line">                    —&gt; quorumPeer.start()</div><div class="line">                              —&gt; loadDataBase()</div><div class="line">                                        —&gt; zkDb.loadDataBase()</div><div class="line">                                                  —&gt; zxid = snapLog.restore(dataTree, listener)</div><div class="line">                                                            —&gt; processTransaction (hdr, dt, txnLog.itor)</div><div class="line">                                                                      —&gt; switch (hdr)</div><div class="line">                                                                                —&gt; OpCode.createSession</div><div class="line">                                                                                          —&gt; dt.processTxn (itor.getTxn)</div><div class="line">                                                                                                    —&gt; switch (header.getType())</div><div class="line">                                                                                                              —&gt; OpCode.create</div><div class="line">                                                                                                              —&gt; OpCode.delete</div><div class="line">                                                                                                              —&gt; OpCode.setData</div><div class="line">                                                                                                              —&gt; OpCode.setACL</div><div class="line">                                                                                                              —&gt; OpCode.closeSession</div><div class="line">                                                                                                              —&gt; OpCode.error</div><div class="line">                                                                                                              —&gt; OpCode.check</div><div class="line">                                                                                                              —&gt; OpCode.multi</div><div class="line">                                                                                —&gt; OpCode.closeSession</div><div class="line">                                                                                          —&gt; dt.processTxn (itor.getTxn)</div><div class="line">                                        —&gt; currentEpoch = readLongFromFile(“currentEpoch")</div><div class="line">                                        —&gt; acceptedEpoch = readLongFromFile(“acceptEpoch<span class="string">")</span></div><div class="line">                              —&gt; cnxnFactory.start()</div><div class="line">                                        —&gt; thread.start()</div><div class="line">                              —&gt; startLeaderElection()</div><div class="line">                              —&gt; super.start()</div><div class="line"></div><div class="line">NIOServerCnxnfactory.run()</div><div class="line">     —&gt; while (! ServerSockChannel.socket().isClosed())</div><div class="line">               —&gt; selector.select(1000)</div><div class="line">               —&gt; selectedList = selector.selectedKeys()</div><div class="line">               —&gt; for (SelectedKey k : selectedList)</div><div class="line">                         —&gt; if ( k.readyOps() &amp; SelectionKey.OP_ACCEPT != 0)</div><div class="line">                                   —&gt; socketChannel = ((ServerSocketChannel) key.channel()).accept()</div><div class="line">                                   —&gt; NIOServerCnxn cnxn =  createConnection(socketChannel, k)</div><div class="line">                         —&gt; else if (k.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE) != 0)</div><div class="line">                                   —&gt; NIOServerCnxn cnxn = (NIOServerCnxn) k.attachment()</div><div class="line">                                   —&gt; cnxn.doIO(k)</div><div class="line">                                             —&gt; if (k.isReadable())</div><div class="line">                                                       —&gt; socketChannel.read (incomingBuffer)</div><div class="line">                                                       —&gt; zkServer.processPacket (incomingBuffer)</div><div class="line">                                                                 —&gt; OpCode.auth                                             解析incomingBuffer，判断请求类型</div><div class="line">                                                                 —&gt; OpCode.sasl</div><div class="line">                                                                 —&gt; req = new Request (incomingBuffer, ServerCnxn)</div><div class="line">                                                                 —&gt; processRequest(req)</div><div class="line">                                                                           —&gt; submittedRequest.add(req)</div><div class="line">                                             —&gt; if (k.isWritable())</div><div class="line">                                                       —&gt; if (outgoingBuffer.size() &gt; 0)</div><div class="line">                                                                 —&gt; sockChannel.write(outgoingBuffer)</div><div class="line">               —&gt; selector.clear()</div><div class="line"></div><div class="line">quorumPeer.run()</div><div class="line">     —&gt; jmxQuorumBean = new QuorumBean(this)</div><div class="line">     —&gt; MBeanRegistry.getInstance().register(jmxQuorumBean)</div><div class="line">     —&gt; while (running)</div><div class="line">               —&gt; switch (peerState)</div><div class="line">                         —&gt; case LOOKING:</div><div class="line">                                   —&gt; if (readOnly)</div><div class="line">                                             —&gt; roZk = new ReadOnlyZooKeeperServer (logFactory, this, new ZooKeeperServer.BasicDataTreeBuilder(), this.zkDb)</div><div class="line">                                             —&gt; roZkMgr = new Thread()</div><div class="line">                                             —&gt; roZkMgr.start()</div><div class="line">                                                       —&gt; roZk.startup()</div><div class="line">                                                                 —&gt; registerJMX()</div><div class="line">                                                                 —&gt; startSessionTracker()</div><div class="line">                                                                 —&gt; setupRequestProcessors()</div><div class="line">                                                                 —&gt; state = RUNNING</div><div class="line">                                   —&gt; setBCVote</div><div class="line">                                   —&gt; setCurrentVote</div><div class="line">                         —&gt; case OBSERVING:</div><div class="line">                                   —&gt; setObserver (makeObserver (logFactory) )</div><div class="line">                                             —&gt; new ZooKeeperServer.BasicDataTreeBuilder()</div><div class="line">                                             —&gt; new ObserverZooKeeperServer(logFactory, zkDatabase)</div><div class="line">                                             —&gt; new Observer()</div><div class="line">                                   —&gt; observer.observeLeader()</div><div class="line">                                             —&gt; zk.registerJMX</div><div class="line">                                             —&gt; addr = findLeader()</div><div class="line">                                             —&gt; connectToLeader (addr)          socket连接leader</div><div class="line">                                             —&gt; syncWithLeader()</div><div class="line">                                                       —&gt; synchronized (zk)</div><div class="line">                                                                 —&gt; zk.getZKDatabase.deserializeSnapshot(leaderIs)</div><div class="line">                                             —&gt; while (self.isRunning())</div><div class="line">                                                       —&gt; readPacket(qp)</div><div class="line">                                                       —&gt; processPacket(qp)</div><div class="line">                                                                 —&gt; switch (qp.getType())</div><div class="line">                                                                           —&gt; case Leader.PING</div><div class="line">                                                                           —&gt; case Leader.PROPOSAL</div><div class="line">                                                                           —&gt; case Leader.COMMIT</div><div class="line">                                                                           —&gt; …...</div><div class="line">                         —&gt; case FOLLOWING:</div><div class="line">                                   —&gt; setFollower( makeFollower(logFactory) )</div><div class="line">                                   —&gt; follower.followLeader()</div><div class="line">                                             —&gt; connectToLeader()</div><div class="line">                                             —&gt; syncWithLeader()</div><div class="line">                                             —&gt; while (self.isRunning())</div><div class="line">                                                       —&gt; readPacket(qp)</div><div class="line">                                                       —&gt; processPacket(qp)</div><div class="line">                                                                 —&gt; switch (qp.getType())</div><div class="line">                                                                           —&gt; case Leader.PING</div><div class="line">                                                                           —&gt; case Leader.PROPOSAL</div><div class="line">                                                                           —&gt; case Leader.COMMIT</div><div class="line">                                                                           —&gt; …...</div><div class="line">                         —&gt; case LEADING:</div><div class="line">                                   —&gt; setLeader ( makeLeader(logFactory) )</div><div class="line">                                   —&gt; leader.lead()</div><div class="line">                                             —&gt; zk.registerJMX()</div><div class="line">                                             —&gt; cnxAccepter = new LearnerAcceptor()</div><div class="line">                                             —&gt; cnxAccepter.run()</div><div class="line">                                                       —&gt; LearnerHandler.run()</div><div class="line">                                             —&gt; zk.setZxid(ZxidUtils.makeZxid(epoch))</div><div class="line">                                             —&gt; startZkServer()</div><div class="line">                                                       —&gt; zk.startup ()</div><div class="line">                                                                 —&gt; CommitProcessor.run()                     处理已完成的客户端请求</div><div class="line">                                                                 —&gt; PrepRequestProcessor.run()               处理未完成客户端请求</div><div class="line">                                                                           —&gt; while (true)</div><div class="line">                                                                                     —&gt; request = submittedRequests.take()</div><div class="line">                                                                                     —&gt; pRequest (request)</div><div class="line">                                                                                               —&gt; switch (request.type)</div><div class="line">                                                                                                         —&gt; case OpCode.create:          pRequest2Txn(处理所有的请求)</div><div class="line">                                                                                                         —&gt; case OpCode.delete:          pRequest2Txn</div><div class="line">                                                                                                         —&gt; case OpCode.setData:       pRequest2Txn</div><div class="line">                                                                                                         —&gt; case OpCode.setACL:        pRequest2Txn</div><div class="line">                                                                                                         —&gt; case OpCode.check:           pRequest2Txn</div><div class="line">                                                                                                         —&gt; case OpCode.multi:           pRequest2Txn</div><div class="line">                                                                                                         —&gt; …...</div><div class="line">                                   —&gt; setLeader(null)</div><div class="line"></div><div class="line">QuorumPeer :</div><div class="line">    public Follower follower;          Follower extends Leader</div><div class="line">    public Leader leader;</div><div class="line">    public Observer observer;          Obsearver extends Learner</div><div class="line"></div><div class="line">class Leader &#123;</div><div class="line">    final LeaderZooKeeperServer zk;</div><div class="line">    final QuorumPeer self;</div><div class="line">    private boolean quorumFormed = false;</div><div class="line">    // the follower acceptor thread</div><div class="line">    LearnerCnxAcceptor cnxAcceptor;</div><div class="line">    // list of all the followers</div><div class="line">    private final HashSet&lt;LearnerHandler&gt; learners = new HashSet&lt;LearnerHandler&gt;();</div><div class="line">&#125;</div><div class="line">class Follower extends Learner&#123;</div><div class="line">    private long lastQueued;</div><div class="line">    // This is the same object as this.zk, but we cache the downcast op</div><div class="line">    final FollowerZooKeeperServer fzk;</div><div class="line">&#125;</div><div class="line">class Learner &#123;</div><div class="line">    QuorumPeer self;</div><div class="line">    LearnerZooKeeperServer zk;</div><div class="line">    protected BufferedOutputStream bufferedOutput;</div><div class="line">    protected Socket sock;</div><div class="line">    protected InputArchive leaderIs;</div><div class="line">    protected OutputArchive leaderOs;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝<br><strong>ZooKeeperMain解析源码：</strong><br>main函数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">—&gt; main = new ZooKeeperMain</div><div class="line">             —&gt; parseOptions (-server, -timeout, -<span class="built_in">readonly</span>)</div><div class="line">             —&gt; connectToZk</div><div class="line">                         —&gt; zk = new ZooKeeper</div><div class="line">                                        —&gt; new ConnectStringParser (属性：chrootPath, serverAddress)</div><div class="line">                                        —&gt; new StaticHostProvider (属性：serverAddress)</div><div class="line">                                        —&gt; getClientCnxnSocket (返回 ClientCnxnSocket的子类ClientCnxnSocketNIO对象 (属性：selector, sockKey) )</div><div class="line">                                        —&gt; new ClientCnxn (参数：chrootPath, hostProvider, zookeeper, watchManager, clientCnxnSocket, 属性：authInfo, pendingQueue, outgoingQueue, zookeeper, clientWatchManager)</div><div class="line">                                                        —&gt;  SendThread (参数：ClientCnxnSocketNIO对象，属性：clientCnxnSocket, lastPingSentNs, isFirstConnect)</div><div class="line">                                                        —&gt;  EventThread (属性： waitingEvents, sessionState)</div><div class="line">                                        —&gt; ClientCnxn.start()</div><div class="line">—&gt; main.run()</div><div class="line">          —&gt; processCmd (cl)</div><div class="line">                    —&gt; processZKCmd (co)</div><div class="line">                              —&gt; cmd equals create/delete/rmr/<span class="built_in">set</span>/aget/get/ls/ls2/getAcl/setAcl/<span class="built_in">stat</span>/listquota/setquota/delquota</div><div class="line">                              —&gt; zk.  create/delete/rmr/<span class="built_in">set</span>/aget/get/ls/ls2/getAcl/setAcl/<span class="built_in">stat</span>/listquota/setquota/delquota</div><div class="line">                                        —&gt; cnxn. submitRequest (request, response)</div><div class="line">                                                  —&gt; queuePacket (request, response)</div><div class="line">                                                            —&gt; synchronized (outgoingQueue)</div><div class="line">                                                                      —&gt; outgoingQueue.add ( new Packet(request, response) )</div><div class="line">                                                            —&gt; sendThread.getClientCnxnScoket().wakeupCnxn()</div></pre></td></tr></table></figure></p>
<p>==============================================================================================================================<br><strong>SendThred run函数：</strong><br>属性： state: socket连接状态，isFirstConnect: socket连接是否为第一次初始化连接，zookeeerSaslClient: SASL用户认证机制<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">run</div><div class="line">     —&gt; <span class="keyword">while</span> (state.isAlive)</div><div class="line">                    |—&gt; <span class="keyword">if</span> (! clientCnxnSocket.isConnected)</div><div class="line">                    |           —&gt;  startConnect</div><div class="line">                    |                    —&gt;  state = CONNECTING</div><div class="line">                    |                    —&gt; zookeeerSaslClient 认证</div><div class="line">                    |                    —&gt; ClientCnxnSocketNIO.connect</div><div class="line">                    |                              —&gt; SocketChannel.register(selector, OP_CONNECT)</div><div class="line">                    |                              —&gt; sock.connect(addr)</div><div class="line">                    |                              <span class="keyword">if</span> (immediateConnect)</div><div class="line">                    |                                        —&gt; sendThread.primeConnection</div><div class="line">                    |                                                  —&gt; new ConnectRequest(lastZxid)</div><div class="line">                    |                                                  —&gt; synchronized (outgoingQueue)</div><div class="line">                    |                                                            <span class="keyword">if</span> (! disabledAutoWatchReset)</div><div class="line">                    |                                                                      —&gt; 将zookeeper 的属性 dataWatches, existWatchs, childWatchs 组装成 SetWatchs (递增Zxid)</div><div class="line">                    |                                                                      —&gt; new RequestHeader (Xid=-8, OpCode.setWatches)</div><div class="line">                    |                                                                      —&gt; new Packet, 将requestHeader和setWatchs 一起组装成Packet</div><div class="line">                    |                                                                      —&gt; outgoingQueue.addFirst (packet)</div><div class="line">                    |                                                            <span class="keyword">for</span> (id : authInfo)</div><div class="line">                    |                                                                      —&gt; outgoingQueue.addFirst (new Packet(new RequestHeader(Xid=-4, OpCode.auth), new AuthPacket(id.schema, id.data)))</div><div class="line">          |                               outgoingQueue.addFirst (new Packet(ConnectRequest))</div><div class="line">                    |                                                            clientCnxnSocket.enableReadWriteOnly()</div><div class="line">                    |—&gt; <span class="keyword">if</span> (state.isConnected)</div><div class="line">                    |          —&gt; <span class="keyword">if</span> (zookeeperSaslClient != null)</div><div class="line">                    |                    —&gt; <span class="keyword">if</span> (zookeeperSaslClient.getSaslState == ZookeeperSaslClient.SaslState.INITIAL)</div><div class="line">                    |                               —&gt; zookeeperSaslClient.initialize</div><div class="line">                    |                    —&gt; <span class="keyword">if</span> (zookeeperSaslClient.getKeeperState == KeeperState.AuthFailed)</div><div class="line">                    |                               —&gt; state = States.AUTH_FAILED</div><div class="line">                    |                               —&gt; sendAuthEvent = <span class="literal">true</span></div><div class="line">                    |                    —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (zookeeperSaslClient.getKeeperState == KeeperState.SaslAuthenticated)</div><div class="line">                    |                               —&gt; sendAuthEvent = <span class="literal">true</span></div><div class="line">                    |                    —&gt; <span class="keyword">if</span> (sendAuthEvent == <span class="literal">true</span>)</div><div class="line">                    |                               —&gt; eventThread.queueEvent (new WatchedEvent(Watcher.Event.EventType.None, zookeeperSaslClient.getKeeperState) )</div><div class="line">                    |—&gt; <span class="keyword">if</span> (state.isConnected)</div><div class="line">                    |          —&gt; <span class="keyword">if</span> (clientCnxnSocket.getIdleSend  &gt;  MAX_SEND_PING_INTERVAL)</div><div class="line">                    |                    —&gt; sendPing</div><div class="line">                    |                              —&gt;  queuePacket (new RequestHeader (Xid=-2, OpCode.ping) )</div><div class="line">                    |                                        —&gt; synchronized (outgoingQueue)</div><div class="line">                    |                                                  —&gt; new Packet (RequestHeader)</div><div class="line">                    |                                                  —&gt; <span class="keyword">if</span> (RequestHeader != OpCode.closeSession)</div><div class="line">                    |                                                            —&gt; outgoingQueue.add(packet)</div><div class="line">                    |                                        —&gt; sendThread.getClientCnxnSocket().wakeupCnxn()</div><div class="line">                    |—&gt; <span class="keyword">if</span> (state == States.CONNECTEDREADONLY)</div><div class="line">                    |          —&gt; pingRwServer()</div><div class="line">                    |                    —&gt; new Socket</div><div class="line">                    |                    —&gt; new BufferedReader (new InputStreamReader(socket.getInputStream() ) )</div><div class="line">                    |                    —&gt; BufferedReader.readLine()</div><div class="line">                    |—&gt; clientCnxnSocket.doTransport (pendingQueue, outgoingQueue)</div><div class="line">                    |          —&gt; synchronize (clientCnxnSocket)</div><div class="line">                    |                    —&gt; selected = selector.selectedKeys()</div><div class="line">                    |          —&gt; <span class="keyword">for</span> (SelectionKey key : selected)</div><div class="line">                    |                     —&gt; <span class="keyword">if</span> ((key.readyOps() &amp; SelectionKey.OP_CONNECT) != 0)</div><div class="line">                    |                              —&gt; <span class="keyword">if</span> ((SocketChannel)key.channel().finishConnect() )</div><div class="line">                    |                                         —&gt; sendThread.primeConnection</div><div class="line">                    |                    —&gt; <span class="keyword">else</span> <span class="keyword">if</span> ((key.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0)</div><div class="line">                    |                              —&gt; doIO (pendingQueue, outgoingQueue)</div><div class="line">                    |                                        —&gt; socketChannel = socketKey.channel()</div><div class="line">                    |                                        —&gt; <span class="keyword">if</span> (socketKey.isReadable())</div><div class="line">                    |                                                  —&gt; socketChannel.read(incomingBuffer)</div><div class="line">                    |                                                            —&gt; <span class="keyword">if</span> (! initialized)</div><div class="line">                    |                                                                      —&gt; readConnectResult()</div><div class="line">                    |                                                                                —&gt; ConnectResponse response = new ConnectResponse()</div><div class="line">                    |                                                                                —&gt; response.deserialize (BinaryInputArchive.getArchive(new ByteBufferInputStream(incomingBuffer)) , “connect<span class="string">")</span></div><div class="line">                    |                                                                                —&gt; sendThread.onConnectd(response.getTimeout, response.getSessionId, response.getPasswd)</div><div class="line">                    |                                                                                          —&gt; eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, eventState)  )</div><div class="line">                    |                                                                      —&gt; enableRead()</div><div class="line">                    |                                                            —&gt; else</div><div class="line">                    |                                                                      —&gt; sendThread.readResponse(incomingBuffer)</div><div class="line">                    |                                                                                —&gt; ReplyHeader replyHeader = new ReplyHeader()</div><div class="line">                    |                                                                                —&gt; replyHeader.deserialize (BinaryInputArchive.getArchive(new ByteBufferInputStream(incomingBuffer)), “header")</div><div class="line">                    |                                                                                —&gt; <span class="keyword">if</span> (replyHeader.getXid == -2)    pings</div><div class="line">                    |                                                                                          —&gt; debug</div><div class="line">                    |                                                                                —&gt; <span class="keyword">if</span> (replyHeader.getXid == -4)    AuthPacket</div><div class="line">                    |                                                                                          —&gt; <span class="keyword">if</span> (replyHeader.getErr() == KeeperException.Code.AUTHFAILED)</div><div class="line">                    |                                                                                                    —&gt; eventThread.queueEvent (new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed) )</div><div class="line">                    |                                                                                —&gt; <span class="keyword">if</span> (replyHeader.getXid == -1)    notification</div><div class="line">                    |                                                                                          —&gt; WatcherEvent event = new WatcherEvent()</div><div class="line">                    |                                                                                          —&gt; event.deserialize(BinaryInputArchive.getArchive(new ByteBufferInputStream(incomingBuffer)), “response<span class="string">")</span></div><div class="line">                    |                                                                                          —&gt; event.setPath (chrootPath)</div><div class="line">                    |                                                                                          —&gt; WatchedEvent we = new WatchedEvent(event)</div><div class="line">                    |                                                                                          —&gt; eventThread.queueEvent (we)</div><div class="line">                    |                                                                      —&gt; synchronized (pendingQueue)</div><div class="line">                    |                                                                                —&gt; packet = pendingQueue.remove()</div><div class="line">                    |                                                                      —&gt; packet.replyHeader.setXid/setErr/setZxid</div><div class="line">                    |                                                                      —&gt; finishPacket (packet)</div><div class="line">                    |                                                                                —&gt; eventThread.queuePacket(packet)</div><div class="line">                    |                                                                                          —&gt; waitingEvents.add (packet)</div><div class="line">                    |                                        —&gt; if (socketKey.isWritable())</div><div class="line">                    |                                                  —&gt; Packet packet = findSendablePacket(outgoingQueue, cnxn.sendThread.clientTunneledAuthenticationInProgress)</div><div class="line">                    |                                                  —&gt; packet.createBB          byteBuffer</div><div class="line">                    |                                                  —&gt; socketChannel.write(packet.bb)</div><div class="line">                    |                                                  —&gt; outgoingQueue.removeFirstOccurrence(packet)</div><div class="line">                    |                                                  —&gt; if (packet.hasRemaining)</div><div class="line">                    |                                                            —&gt; synchronized (pendingQueue)</div><div class="line">                    |                                                                      —&gt; pendingQueue.add(packet)</div><div class="line">                    |          —&gt; if (sendThread.getZkState().isConnected)</div><div class="line">                    |                    —&gt; synchronized (outgoingQueue)</div><div class="line">                    |                              —&gt; if (findSendablePacket(outgoingQueue, cnxn.sendThread.clientTunneledAuthenticationInProgress()) != null )</div><div class="line">                    |                                        —&gt; enableWrite()</div></pre></td></tr></table></figure></p>
<p>==============================================================================================================================<br><strong>EventThred run函数：</strong><br>属性： waitingEvents<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">run</div><div class="line">      —&gt; isRunning = <span class="literal">true</span></div><div class="line">      —&gt; <span class="keyword">while</span> (<span class="literal">true</span>)</div><div class="line">               —&gt; event = waitingEvents.take()</div><div class="line">                         —&gt; <span class="keyword">if</span> (event == eventOfDeath)</div><div class="line">                                   —&gt; wasKilled = <span class="literal">true</span></div><div class="line">                         —&gt; <span class="keyword">else</span></div><div class="line">                                   —&gt; processEvent(event)</div><div class="line">                                        —&gt; <span class="keyword">if</span> (event instanceof WatcherSetEventPair)</div><div class="line">                                                       —&gt; <span class="keyword">for</span> (Watcher watcher : ((WatcherSetEventPair)event).watchers )</div><div class="line">                                                                 —&gt; watcher.process( ((WatcherSetEventPair)event).event )</div><div class="line">                                             —&gt; <span class="keyword">else</span></div><div class="line">                                                       —&gt; Packet p = (Packet) event</div><div class="line">                                                       —&gt; <span class="keyword">if</span> (p.response instanceof ExistsResponse || p.response instanceof SetDataResponse || p.response instanceof SetACLResponse)</div><div class="line">                                                                 —&gt; StatCallback cb = (StatCallback) p.cb</div><div class="line">                                                                 —&gt; cb.processResult ( clientPath, ((ExistsResponse/SetDataResponse/SetACLResponse) p.response).getStat )</div><div class="line">                                                                           —&gt; decOutstanding ()</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof GetDataResponse)</div><div class="line">                                                                 —&gt; DataCallback cb = (DataCallback) p.cb</div><div class="line">                                                                 —&gt; GetDataResponse response = (GetDataResponse) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getData(), response.getStat())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof GetACLResponse)</div><div class="line">                                                                 —&gt; ACLCallback cb = (ACLCallback) p.cb</div><div class="line">                                                                 —&gt; GetACLResponse response = (GetACLResponse) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getAcl(), response.getStat())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof GetChildrenResponse)</div><div class="line">                                                                 —&gt; ChildrenCallback cb = (ChildrenCallback) p.cb</div><div class="line">                                                                 —&gt; GetChildrenResponse response = (GetChildrenResponse) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getChildren())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof GetChildren2Response)</div><div class="line">                                                                 —&gt; Children2Callback cb = (Children2Callback) p.cb</div><div class="line">                                                                 —&gt; GetChildren2Response response = (GetChildren2Response) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getChildren())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof CreateResponse)</div><div class="line">                                                                 —&gt; StringCallback cb = (StringCallback) p.cb</div><div class="line">                                                                 —&gt; CreateResponse response = (CreateResponse) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getPath())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.response instanceof MultiResponse)</div><div class="line">                                                                 —&gt; MultiCallback cb = (MultiCallback) p.cb</div><div class="line">                                                                 —&gt; MultiResponse response = (MultiResponse) p.response</div><div class="line">                                                                 —&gt; cb.processResult (clientPath, response.getResultList())</div><div class="line">                                                       —&gt; <span class="keyword">else</span> <span class="keyword">if</span> (p.cb instanceof VoidCallback)</div><div class="line">                                                                 —&gt; VoidCallback cb = (VoidCallback) p.cb</div><div class="line">                                                                 —&gt; cb.processResult (clientPath)</div></pre></td></tr></table></figure></p>
<p><strong>ClinetCnxn是核心函数：</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div></pre></td><td class="code"><pre><div class="line">   <span class="function"><span class="keyword">public</span> <span class="title">ClientCnxn</span><span class="params">(String chrootPath, HostProvider hostProvider, intsessionTimeout, ZooKeeper zooKeeper,</span></span></div><div class="line">            ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket,</div><div class="line">            <span class="keyword">long</span> sessionId, <span class="keyword">byte</span>[] sessionPasswd, <span class="keyword">boolean</span> canBeReadOnly) &#123;</div><div class="line">        <span class="keyword">this</span>.zooKeeper = zooKeeper;</div><div class="line">        <span class="keyword">this</span>.watcher = watcher;</div><div class="line">        <span class="keyword">this</span>.hostProvider = hostProvider;</div><div class="line">        <span class="keyword">this</span>.chrootPath = chrootPath;</div><div class="line"></div><div class="line">        sendThread = <span class="keyword">new</span> SendThread(clientCnxnSocket);</div><div class="line">        eventThread = <span class="keyword">new</span> EventThread();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">   <span class="keyword">public</span> <span class="keyword">enum</span> States &#123;</div><div class="line">        CONNECTING, ASSOCIATING, CONNECTED, CONNECTEDREADONLY,</div><div class="line">        CLOSED, AUTH_FAILED, NOT_CONNECTED;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isAlive</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span> != CLOSED &amp;&amp; <span class="keyword">this</span> != AUTH_FAILED;</div><div class="line">        &#125;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isConnected</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span> == CONNECTED || <span class="keyword">this</span> == CONNECTEDREADONLY;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">        Packet(RequestHeader requestHeader, ReplyHeader replyHeader,</div><div class="line">               Record request, Record response,</div><div class="line">               WatchRegistration watchRegistration, <span class="keyword">boolean</span> readOnly) &#123;</div><div class="line">            <span class="keyword">this</span>.requestHeader = requestHeader;</div><div class="line">            <span class="keyword">this</span>.replyHeader = replyHeader;</div><div class="line">            <span class="keyword">this</span>.request = request;</div><div class="line">            <span class="keyword">this</span>.response = response;</div><div class="line">            <span class="keyword">this</span>.readOnly = readOnly;</div><div class="line">            <span class="keyword">this</span>.watchRegistration = watchRegistration;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DataTree</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="comment">/* Rather than fight it, let root have an alias */</span></div><div class="line">        nodes.put(<span class="string">""</span>, root);</div><div class="line">        nodes.put(rootZookeeper, root);</div><div class="line"></div><div class="line">        <span class="comment">/** add the proc node and quota node */</span></div><div class="line">        root.addChild(procChildZookeeper);</div><div class="line">        nodes.put(procZookeeper, procDataNode);</div><div class="line"></div><div class="line">        procDataNode.addChild(quotaChildZookeeper);</div><div class="line">        nodes.put(quotaZookeeper, quotaDataNode);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">   <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">pRequest2Txn</span><span class="params">(<span class="keyword">int</span> type, <span class="keyword">long</span> zxid, Request request, Record record, <span class="keyword">boolean</span> deserialize)</span></span></div><div class="line">        <span class="keyword">throws</span> KeeperException, IOException, RequestProcessorException</div><div class="line">    &#123;</div><div class="line">        request.hdr = <span class="keyword">new</span> TxnHeader(request.sessionId, request.cxid, zxid,</div><div class="line">                                    zks.getTime(), type);</div><div class="line"></div><div class="line">        <span class="keyword">switch</span> (type) &#123;</div><div class="line">            <span class="keyword">case</span> OpCode.create:</div><div class="line">                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</div><div class="line">                CreateRequest createRequest = (CreateRequest)record;</div><div class="line">                <span class="keyword">if</span>(deserialize)</div><div class="line">                    ByteBufferInputStream.byteBuffer2Record(request.request, createRequest);</div><div class="line">                String path = createRequest.getPath();</div><div class="line">                <span class="keyword">int</span> lastSlash = path.lastIndexOf(<span class="string">'/'</span>);</div><div class="line">                <span class="keyword">if</span> (lastSlash == -<span class="number">1</span> || path.indexOf(<span class="string">'\0'</span>) != -<span class="number">1</span> || failCreate) &#123;</div><div class="line">                    LOG.info(<span class="string">"Invalid path "</span> + path + <span class="string">" with session 0x"</span> +</div><div class="line">                            Long.toHexString(request.sessionId));</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadArgumentsException(path);</div><div class="line">                &#125;</div><div class="line">                List&lt;ACL&gt; listACL = removeDuplicates(createRequest.getAcl());</div><div class="line">                <span class="keyword">if</span> (!fixupACL(request.authInfo, listACL)) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.InvalidACLException(path);</div><div class="line">                &#125;</div><div class="line">                String parentPath = path.substring(<span class="number">0</span>, lastSlash);</div><div class="line">                ChangeRecord parentRecord = getRecordForPath(parentPath);</div><div class="line"></div><div class="line">                checkACL(zks, parentRecord.acl, ZooDefs.Perms.CREATE,</div><div class="line">                        request.authInfo);</div><div class="line">                <span class="keyword">int</span> parentCVersion = parentRecord.stat.getCversion();</div><div class="line">                CreateMode createMode =</div><div class="line">                    CreateMode.fromFlag(createRequest.getFlags());</div><div class="line">                <span class="keyword">if</span> (createMode.isSequential()) &#123;</div><div class="line">                    path = path + String.format(Locale.ENGLISH, <span class="string">"%010d"</span>, parentCVersion);</div><div class="line">                &#125;</div><div class="line">                validatePath(path, request.sessionId);</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    <span class="keyword">if</span> (getRecordForPath(path) != <span class="keyword">null</span>) &#123;</div><div class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.NodeExistsException(path);</div><div class="line">                    &#125;</div><div class="line">                &#125; <span class="keyword">catch</span> (KeeperException.NoNodeException e) &#123;</div><div class="line">                    <span class="comment">// ignore this one</span></div><div class="line">                &#125;</div><div class="line">                <span class="keyword">boolean</span> ephemeralParent = parentRecord.stat.getEphemeralOwner() != <span class="number">0</span>;</div><div class="line">                <span class="keyword">if</span> (ephemeralParent) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.NoChildrenForEphemeralsException(path);</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">int</span> newCversion = parentRecord.stat.getCversion()+<span class="number">1</span>;</div><div class="line">                request.txn = <span class="keyword">new</span> CreateTxn(path, createRequest.getData(),</div><div class="line">                        listACL,</div><div class="line">                        createMode.isEphemeral(), newCversion);</div><div class="line">                StatPersisted s = <span class="keyword">new</span> StatPersisted();</div><div class="line">                <span class="keyword">if</span> (createMode.isEphemeral()) &#123;</div><div class="line">                    s.setEphemeralOwner(request.sessionId);</div><div class="line">                &#125;</div><div class="line">                parentRecord = parentRecord.duplicate(request.hdr.getZxid());</div><div class="line">                parentRecord.childCount++;</div><div class="line">                parentRecord.stat.setCversion(newCversion);</div><div class="line">                addChangeRecord(parentRecord);</div><div class="line">                addChangeRecord(<span class="keyword">new</span> ChangeRecord(request.hdr.getZxid(), path, s,</div><div class="line">                        <span class="number">0</span>, listACL));</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.delete:</div><div class="line">                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</div><div class="line">                DeleteRequest deleteRequest = (DeleteRequest)record;</div><div class="line">                <span class="keyword">if</span>(deserialize)</div><div class="line">                    ByteBufferInputStream.byteBuffer2Record(request.request, deleteRequest);</div><div class="line">                path = deleteRequest.getPath();</div><div class="line">                lastSlash = path.lastIndexOf(<span class="string">'/'</span>);</div><div class="line">                <span class="keyword">if</span> (lastSlash == -<span class="number">1</span> || path.indexOf(<span class="string">'\0'</span>) != -<span class="number">1</span></div><div class="line">                        || zks.getZKDatabase().isSpecialPath(path)) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadArgumentsException(path);</div><div class="line">                &#125;</div><div class="line">                parentPath = path.substring(<span class="number">0</span>, lastSlash);</div><div class="line">                parentRecord = getRecordForPath(parentPath);</div><div class="line">                ChangeRecord nodeRecord = getRecordForPath(path);</div><div class="line">                checkACL(zks, parentRecord.acl, ZooDefs.Perms.DELETE,</div><div class="line">                        request.authInfo);</div><div class="line">                <span class="keyword">int</span> version = deleteRequest.getVersion();</div><div class="line">                <span class="keyword">if</span> (version != -<span class="number">1</span> &amp;&amp; nodeRecord.stat.getVersion() != version) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadVersionException(path);</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">if</span> (nodeRecord.childCount &gt; <span class="number">0</span>) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.NotEmptyException(path);</div><div class="line">                &#125;</div><div class="line">                request.txn = <span class="keyword">new</span> DeleteTxn(path);</div><div class="line">                parentRecord = parentRecord.duplicate(request.hdr.getZxid());</div><div class="line">                parentRecord.childCount--;</div><div class="line">                addChangeRecord(parentRecord);</div><div class="line">                addChangeRecord(<span class="keyword">new</span> ChangeRecord(request.hdr.getZxid(), path,</div><div class="line">                        <span class="keyword">null</span>, -<span class="number">1</span>, <span class="keyword">null</span>));</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.setData:</div><div class="line">                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</div><div class="line">                SetDataRequest setDataRequest = (SetDataRequest)record;</div><div class="line">                <span class="keyword">if</span>(deserialize)</div><div class="line">                    ByteBufferInputStream.byteBuffer2Record(request.request, setDataRequest);</div><div class="line">                path = setDataRequest.getPath();</div><div class="line">                validatePath(path, request.sessionId);</div><div class="line">                nodeRecord = getRecordForPath(path);</div><div class="line">                checkACL(zks, nodeRecord.acl, ZooDefs.Perms.WRITE,</div><div class="line">                        request.authInfo);</div><div class="line">                version = setDataRequest.getVersion();</div><div class="line">                <span class="keyword">int</span> currentVersion = nodeRecord.stat.getVersion();</div><div class="line">                <span class="keyword">if</span> (version != -<span class="number">1</span> &amp;&amp; version != currentVersion) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadVersionException(path);</div><div class="line">                &#125;</div><div class="line">                version = currentVersion + <span class="number">1</span>;</div><div class="line">                request.txn = <span class="keyword">new</span> SetDataTxn(path, setDataRequest.getData(), version);</div><div class="line">                nodeRecord = nodeRecord.duplicate(request.hdr.getZxid());</div><div class="line">                nodeRecord.stat.setVersion(version);</div><div class="line">                addChangeRecord(nodeRecord);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.setACL:</div><div class="line">                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</div><div class="line">                SetACLRequest setAclRequest = (SetACLRequest)record;</div><div class="line">                <span class="keyword">if</span>(deserialize)</div><div class="line">                    ByteBufferInputStream.byteBuffer2Record(request.request, setAclRequest);</div><div class="line">                path = setAclRequest.getPath();</div><div class="line">                validatePath(path, request.sessionId);</div><div class="line">                listACL = removeDuplicates(setAclRequest.getAcl());</div><div class="line">                <span class="keyword">if</span> (!fixupACL(request.authInfo, listACL)) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.InvalidACLException(path);</div><div class="line">                &#125;</div><div class="line">                nodeRecord = getRecordForPath(path);</div><div class="line">                checkACL(zks, nodeRecord.acl, ZooDefs.Perms.ADMIN,</div><div class="line">                        request.authInfo);</div><div class="line">                version = setAclRequest.getVersion();</div><div class="line">                currentVersion = nodeRecord.stat.getAversion();</div><div class="line">                <span class="keyword">if</span> (version != -<span class="number">1</span> &amp;&amp; version != currentVersion) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadVersionException(path);</div><div class="line">                &#125;</div><div class="line">                version = currentVersion + <span class="number">1</span>;</div><div class="line">                request.txn = <span class="keyword">new</span> SetACLTxn(path, listACL, version);</div><div class="line">                nodeRecord = nodeRecord.duplicate(request.hdr.getZxid());</div><div class="line">                nodeRecord.stat.setAversion(version);</div><div class="line">                addChangeRecord(nodeRecord);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.createSession:</div><div class="line">                request.request.rewind();</div><div class="line">                <span class="keyword">int</span> to = request.request.getInt();</div><div class="line">                request.txn = <span class="keyword">new</span> CreateSessionTxn(to);</div><div class="line">                request.request.rewind();</div><div class="line">                zks.sessionTracker.addSession(request.sessionId, to);</div><div class="line">                zks.setOwner(request.sessionId, request.getOwner());</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.closeSession:</div><div class="line">                <span class="comment">// We don't want to do this check since the session expiration thread</span></div><div class="line">                <span class="comment">// queues up this operation without being the session owner.</span></div><div class="line">                <span class="comment">// this request is the last of the session so it should be ok</span></div><div class="line">                <span class="comment">//zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</span></div><div class="line">                HashSet&lt;String&gt; es = zks.getZKDatabase()</div><div class="line">                        .getEphemerals(request.sessionId);</div><div class="line">                <span class="keyword">synchronized</span> (zks.outstandingChanges) &#123;</div><div class="line">                    <span class="keyword">for</span> (ChangeRecord c : zks.outstandingChanges) &#123;</div><div class="line">                        <span class="keyword">if</span> (c.stat == <span class="keyword">null</span>) &#123;</div><div class="line">                            <span class="comment">// Doing a delete</span></div><div class="line">                            es.remove(c.path);</div><div class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c.stat.getEphemeralOwner() == request.sessionId) &#123;</div><div class="line">                            es.add(c.path);</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                    <span class="keyword">for</span> (String path2Delete : es) &#123;</div><div class="line">                        addChangeRecord(<span class="keyword">new</span> ChangeRecord(request.hdr.getZxid(),</div><div class="line">                                path2Delete, <span class="keyword">null</span>, <span class="number">0</span>, <span class="keyword">null</span>));</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    zks.sessionTracker.setSessionClosing(request.sessionId);</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                LOG.info(<span class="string">"Processed session termination for sessionid: 0x"</span></div><div class="line">                        + Long.toHexString(request.sessionId));</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> OpCode.check:</div><div class="line">                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());</div><div class="line">                CheckVersionRequest checkVersionRequest = (CheckVersionRequest)record;</div><div class="line">                <span class="keyword">if</span>(deserialize)</div><div class="line">                    ByteBufferInputStream.byteBuffer2Record(request.request, checkVersionRequest);</div><div class="line">                path = checkVersionRequest.getPath();</div><div class="line">                validatePath(path, request.sessionId);</div><div class="line">                nodeRecord = getRecordForPath(path);</div><div class="line">                checkACL(zks, nodeRecord.acl, ZooDefs.Perms.READ,</div><div class="line">                        request.authInfo);</div><div class="line">                version = checkVersionRequest.getVersion();</div><div class="line">                currentVersion = nodeRecord.stat.getVersion();</div><div class="line">                <span class="keyword">if</span> (version != -<span class="number">1</span> &amp;&amp; version != currentVersion) &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> KeeperException.BadVersionException(path);</div><div class="line">                &#125;</div><div class="line">                version = currentVersion + <span class="number">1</span>;</div><div class="line">                request.txn = <span class="keyword">new</span> CheckVersionTxn(path, version);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processPacket</span><span class="params">(QuorumPacket qp)</span> <span class="keyword">throws</span> IOException</span>&#123;</div><div class="line">        <span class="keyword">switch</span> (qp.getType()) &#123;</div><div class="line">        <span class="keyword">case</span> Leader.PING:</div><div class="line">            ping(qp);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.PROPOSAL:</div><div class="line">            LOG.warn(<span class="string">"Ignoring proposal"</span>);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.COMMIT:</div><div class="line">            LOG.warn(<span class="string">"Ignoring commit"</span>);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.UPTODATE:</div><div class="line">            LOG.error(<span class="string">"Received an UPTODATE message after Observer started"</span>);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.REVALIDATE:</div><div class="line">            revalidate(qp);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.SYNC:</div><div class="line">            ((ObserverZooKeeperServer)zk).sync();</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> Leader.INFORM:</div><div class="line">            TxnHeader hdr = <span class="keyword">new</span> TxnHeader();</div><div class="line">            Record txn = SerializeUtils.deserializeTxn(qp.getData(), hdr);</div><div class="line">            Request request = <span class="keyword">new</span> Request (<span class="keyword">null</span>, hdr.getClientId(),</div><div class="line">                                           hdr.getCxid(),</div><div class="line">                                           hdr.getType(), <span class="keyword">null</span>, <span class="keyword">null</span>);</div><div class="line">            request.txn = txn;</div><div class="line">            request.hdr = hdr;</div><div class="line">            ObserverZooKeeperServer obs = (ObserverZooKeeperServer)zk;</div><div class="line">            obs.commitRequest(request);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="keyword">switch</span> (packetType) &#123;</div><div class="line">        <span class="keyword">case</span> DIFF:</div><div class="line">            <span class="keyword">return</span> <span class="string">"DIFF"</span>;</div><div class="line">        <span class="keyword">case</span> TRUNC:</div><div class="line">            <span class="keyword">return</span> <span class="string">"TRUNC"</span>;</div><div class="line">        <span class="keyword">case</span> SNAP:</div><div class="line">            <span class="keyword">return</span> <span class="string">"SNAP"</span>;</div><div class="line">        <span class="keyword">case</span> OBSERVERINFO:</div><div class="line">            <span class="keyword">return</span> <span class="string">"OBSERVERINFO"</span>;</div><div class="line">        <span class="keyword">case</span> NEWLEADER:</div><div class="line">            <span class="keyword">return</span> <span class="string">"NEWLEADER"</span>;</div><div class="line">        <span class="keyword">case</span> FOLLOWERINFO:</div><div class="line">            <span class="keyword">return</span> <span class="string">"FOLLOWERINFO"</span>;</div><div class="line">        <span class="keyword">case</span> UPTODATE:</div><div class="line">            <span class="keyword">return</span> <span class="string">"UPTODATE"</span>;</div><div class="line">        <span class="keyword">case</span> LEADERINFO:</div><div class="line">            <span class="keyword">return</span> <span class="string">"LEADERINFO"</span>;</div><div class="line">        <span class="keyword">case</span> ACKEPOCH:</div><div class="line">            <span class="keyword">return</span> <span class="string">"ACKEPOCH"</span>;</div><div class="line">        <span class="keyword">case</span> REQUEST:</div><div class="line">            <span class="keyword">return</span> <span class="string">"REQUEST"</span>;</div><div class="line">        <span class="keyword">case</span> PROPOSAL:</div><div class="line">            <span class="keyword">return</span> <span class="string">"PROPOSAL"</span>;</div><div class="line">        <span class="keyword">case</span> ACK:</div><div class="line">            <span class="keyword">return</span> <span class="string">"ACK"</span>;</div><div class="line">        <span class="keyword">case</span> COMMIT:</div><div class="line">            <span class="keyword">return</span> <span class="string">"COMMIT"</span>;</div><div class="line">        <span class="keyword">case</span> PING:</div><div class="line">            <span class="keyword">return</span> <span class="string">"PING"</span>;</div><div class="line">        <span class="keyword">case</span> REVALIDATE:</div><div class="line">            <span class="keyword">return</span> <span class="string">"REVALIDATE"</span>;</div><div class="line">        <span class="keyword">case</span> SYNC:</div><div class="line">            <span class="keyword">return</span> <span class="string">"SYNC"</span>;</div><div class="line">        <span class="keyword">case</span> INFORM:</div><div class="line">            <span class="keyword">return</span> <span class="string">"INFORM"</span>;</div><div class="line">        <span class="keyword">default</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">"UNKNOWN"</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/ZooKeeper源码/" class="archive-article-date">
  	<time datetime="2016-11-10T05:15:34.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ZooKeeper/">ZooKeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-ZooKeeper简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/ZooKeeper简介/">ZooKeeper简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="ZooKeeper导图"><a href="#ZooKeeper导图" class="headerlink" title="ZooKeeper导图"></a>ZooKeeper导图</h2><p><strong>ZooKeeper 服务:</strong><br><img src="/images/ZooKeeper_1.png" alt=""></p>
<p><strong>ZooKeeper 名字空间:</strong><br><img src="/images/ZooKeeper_2.png" alt=""></p>
<h2 id="推荐一本书"><a href="#推荐一本书" class="headerlink" title="推荐一本书"></a>推荐一本书</h2><p>《ZooKeeper分布式过程协同技术详解》</p>
<h2 id="基于-Paxos-算法"><a href="#基于-Paxos-算法" class="headerlink" title="基于 Paxos 算法"></a>基于 Paxos 算法</h2><p>wiki: <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science" target="_blank" rel="external">https://en.wikipedia.org/wiki/Paxos_(computer_science</a>)</p>
<h2 id="ZooKeeper-支持的api"><a href="#ZooKeeper-支持的api" class="headerlink" title="ZooKeeper 支持的api"></a>ZooKeeper 支持的api</h2><p><strong>ZooKeeper API:</strong><br>1.create /path data<br>2.delete /path<br>3.exists /path<br>4.setData /path data<br>5.getData /path<br>6.getChildren /path</p>
<h2 id="单点ZooKeeper-与-ZooKeeper集群"><a href="#单点ZooKeeper-与-ZooKeeper集群" class="headerlink" title="单点ZooKeeper 与 ZooKeeper集群"></a>单点ZooKeeper 与 ZooKeeper集群</h2><p><strong>ZooKeeper服务器的两种工作模式： 独立模式(standalone)和仲裁模式(quorum)</strong><br><strong>独立模式(standalone)</strong>: 单独的ZooKeeper服务器。<br><strong>仲裁模式(quorum)</strong>: ZooKeeper集合(ZooKeeper ensemble)。 仲裁模式中，为减少ZooKeeper Server数据同步的延迟，法定人数 的概念被使用，法定人数的大小设置非常重要。</p>
<p><strong>仲裁模式试验：</strong><br><strong>configure 文件：</strong><br><em>z1.cfg</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tickTime=2000</div><div class="line">initLimit=10</div><div class="line">syncLimit=5</div><div class="line">dataDir=/root/zookeeper-3.4.8/conf/z1/data</div><div class="line">clientPort=2181</div><div class="line">server.1=127.0.0.1:2222:2223</div><div class="line">server.2=127.0.0.1:3333:3334</div><div class="line">server.3=127.0.0.1:4444:4445</div></pre></td></tr></table></figure></p>
<p><em>z2.cfg</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tickTime=2000</div><div class="line">initLimit=10</div><div class="line">syncLimit=5</div><div class="line">dataDir=/root/zookeeper-3.4.8/conf/z2/data</div><div class="line">clientPort=2182</div><div class="line">server.1=127.0.0.1:2222:2223</div><div class="line">server.2=127.0.0.1:3333:3334</div><div class="line">server.3=127.0.0.1:4444:4445</div></pre></td></tr></table></figure></p>
<p><em>z3.cfg</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tickTime=2000</div><div class="line">initLimit=10</div><div class="line">syncLimit=5</div><div class="line">dataDir=/root/zookeeper-3.4.8/conf/z3/data</div><div class="line">clientPort=2183</div><div class="line">server.1=127.0.0.1:2222:2223</div><div class="line">server.2=127.0.0.1:3333:3334</div><div class="line">server.3=127.0.0.1:4444:4445</div></pre></td></tr></table></figure></p>
<p><em>启动3个节点命令：</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/root/zookeeper-3.4.8/bin/zkServer.sh  start  z1/z1.cfg</div><div class="line">/root/zookeeper-3.4.8/bin/zkServer.sh  start  z2/z2.cfg</div><div class="line">/root/zookeeper-3.4.8/bin/zkServer.sh  start  z3/z3.cfg</div></pre></td></tr></table></figure></p>
<p><em>连接集群：</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/bin/zkCli.sh -server 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</div></pre></td></tr></table></figure></p>
<p>创建 <strong>ephemeral</strong> 节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create -e /master &quot;master.example.com&quot;</div></pre></td></tr></table></figure></p>
<p>创建 <strong>sequential</strong> 节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create -s /master/task-  “cmd&quot;</div></pre></td></tr></table></figure></p>
<p><strong>WatchedEvent</strong> 数据结构包含以下信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">ZooKeeper会话状态(KeeperState)：Disconnected, SyncConnected, AuthFailed, ConnectedReadOnly, SaslAuthenticated, Expired.</div><div class="line">事件类型(EventType)：NodeCreated, NodeDeleted, NodeDataChanged, NodeChildrenChanged, None.</div><div class="line">如果事件类型不是None时，返回一个znode路径.</div><div class="line">Watch监视点设置：</div><div class="line">NodeCreated：通过exists调用设置监视点。</div><div class="line">NodeDeleted：通过exists或getData调用设置监视点。</div><div class="line">NodeDataChanged：通过exists或getData调用设置监视点。</div><div class="line">NodeChildrenChanged：通过getChildren调用设置监视点。</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/ZooKeeper简介/" class="archive-article-date">
  	<time datetime="2016-11-09T03:08:50.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ZooKeeper/">ZooKeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark Streaming SQL MLlib GraphX" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark Streaming SQL MLlib GraphX/">Spark Streaming/SQL/MLlib/GraphX</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Spark-Streaming-SQL-MLlib-GraphX"><a href="#Spark-Streaming-SQL-MLlib-GraphX" class="headerlink" title="Spark Streaming/SQL/MLlib/GraphX"></a>Spark Streaming/SQL/MLlib/GraphX</h2><p>Spark 这一大数据分析框架，包含了：</p>
<ul>
<li>流计算： Streaming</li>
<li>图计算： GraphX</li>
<li>数据挖掘： MLlib</li>
<li>SQL</li>
</ul>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>参考： <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p>
<p><img src="/images/Spark_Streaming_1.png" alt=""></p>
<p><img src="/images/Spark_Streaming_2.png" alt=""></p>
<p><img src="/images/Spark_Streaming_3.png" alt=""></p>
<p><img src="/images/Spark_Streaming_4.png" alt=""></p>
<h3 id="Spark-Streaming-Example"><a href="#Spark-Streaming-Example" class="headerlink" title="Spark Streaming Example"></a>Spark Streaming Example</h3><p><strong>StreamingContext:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></div><div class="line"><span class="comment">// The master requires 2 cores to prevent from a starvation scenario.</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p><strong>socket:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</div><div class="line"><span class="comment">// Split each line into words</span></div><div class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></div><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print()</div><div class="line">ssc.start()             <span class="comment">// Start the computation</span></div><div class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure></p>
<p><strong>往9999端口里面输入：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ nc -lk 9999</div><div class="line">$ ./bin/run-example streaming.NetworkWordCount localhost 9999</div></pre></td></tr></table></figure></p>
<h2 id="Spark-GraphX"><a href="#Spark-GraphX" class="headerlink" title="Spark GraphX"></a>Spark GraphX</h2><p>参考： <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/graphx-programming-guide.html</a></p>
<p><img src="/images/Spark_GraphX_1.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.graphx._</div><div class="line"><span class="comment">// To make some of the examples work we will also need RDD</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">VertexProperty</span>(<span class="params"></span>)</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserProperty</span>(<span class="params">val name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">VertexProperty</span></span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductProperty</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span></span>) <span class="keyword">extends</span> <span class="title">VertexProperty</span></span></div><div class="line"><span class="comment">// The graph might then have the type:</span></div><div class="line"><span class="keyword">var</span> graph: <span class="type">Graph</span>[<span class="type">VertexProperty</span>, <span class="type">String</span>] = <span class="literal">null</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>[<span class="type">VD</span>, <span class="type">ED</span>] </span>&#123;</div><div class="line">  <span class="keyword">val</span> vertices: <span class="type">VertexRDD</span>[<span class="type">VD</span>]</div><div class="line">  <span class="keyword">val</span> edges: <span class="type">EdgeRDD</span>[<span class="type">ED</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/images/Spark_GraphX_2.png" alt=""></p>
<p><strong>构建graph的代码：</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> userGraph: <span class="type">Graph</span>[(<span class="type">String</span>, <span class="type">String</span>), <span class="type">String</span>]</div><div class="line"></div><div class="line"><span class="comment">// Assume the SparkContext has already been constructed</span></div><div class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span></div><div class="line"><span class="comment">// Create an RDD for the vertices</span></div><div class="line"><span class="keyword">val</span> users: <span class="type">RDD</span>[(<span class="type">VertexId</span>, (<span class="type">String</span>, <span class="type">String</span>))] =</div><div class="line">  sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>L, (<span class="string">"rxin"</span>, <span class="string">"student"</span>)), (<span class="number">7</span>L, (<span class="string">"jgonzal"</span>, <span class="string">"postdoc"</span>)),</div><div class="line">                       (<span class="number">5</span>L, (<span class="string">"franklin"</span>, <span class="string">"prof"</span>)), (<span class="number">2</span>L, (<span class="string">"istoica"</span>, <span class="string">"prof"</span>))))</div><div class="line"><span class="comment">// Create an RDD for edges</span></div><div class="line"><span class="keyword">val</span> relationships: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">String</span>]] =</div><div class="line">  sc.parallelize(<span class="type">Array</span>(<span class="type">Edge</span>(<span class="number">3</span>L, <span class="number">7</span>L, <span class="string">"collab"</span>),    <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">3</span>L, <span class="string">"advisor"</span>),</div><div class="line">                       <span class="type">Edge</span>(<span class="number">2</span>L, <span class="number">5</span>L, <span class="string">"colleague"</span>), <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">7</span>L, <span class="string">"pi"</span>)))</div><div class="line"><span class="comment">// Define a default user in case there are relationship with missing user</span></div><div class="line"><span class="keyword">val</span> defaultUser = (<span class="string">"John Doe"</span>, <span class="string">"Missing"</span>)</div><div class="line"><span class="comment">// Build the initial Graph</span></div><div class="line"><span class="keyword">val</span> graph = <span class="type">Graph</span>(users, relationships, defaultUser)</div><div class="line"><span class="keyword">val</span> graph: <span class="type">Graph</span>[(<span class="type">String</span>, <span class="type">String</span>), <span class="type">String</span>] <span class="comment">// Constructed from above</span></div><div class="line"><span class="comment">// Count all users which are postdocs</span></div><div class="line">graph.vertices.filter &#123; <span class="keyword">case</span> (id, (name, pos)) =&gt; pos == <span class="string">"postdoc"</span> &#125;.count</div><div class="line"><span class="comment">// Count all the edges where src &gt; dst</span></div><div class="line">graph.edges.filter(e =&gt; e.srcId &gt; e.dstId).count</div><div class="line">graph.edges.filter &#123; <span class="keyword">case</span> <span class="type">Edge</span>(src, dst, prop) =&gt; src &gt; dst &#125;.count</div><div class="line"></div><div class="line"><span class="keyword">val</span> graph: <span class="type">Graph</span>[(<span class="type">String</span>, <span class="type">String</span>), <span class="type">String</span>] <span class="comment">// Constructed from above</span></div><div class="line"><span class="comment">// Use the triplets view to create an RDD of facts.</span></div><div class="line"><span class="keyword">val</span> facts: <span class="type">RDD</span>[<span class="type">String</span>] =</div><div class="line">  graph.triplets.map(triplet =&gt;</div><div class="line">    triplet.srcAttr._1 + <span class="string">" is the "</span> + triplet.attr + <span class="string">" of "</span> + triplet.dstAttr._1)</div><div class="line">facts.collect.foreach(println(_))</div></pre></td></tr></table></figure></p>
<p><strong>Graph  Operators:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> graph: <span class="type">Graph</span>[(<span class="type">String</span>, <span class="type">String</span>), <span class="type">String</span>]</div><div class="line"><span class="comment">// Use the implicit GraphOps.inDegrees operator</span></div><div class="line"><span class="keyword">val</span> inDegrees: <span class="type">VertexRDD</span>[<span class="type">Int</span>] = graph.inDegrees</div></pre></td></tr></table></figure></p>
<p><strong>Shortest Path:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.graphx.&#123;<span class="type">Graph</span>, <span class="type">VertexId</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.graphx.util.<span class="type">GraphGenerators</span></div><div class="line"></div><div class="line"><span class="comment">// A graph with edge attributes containing distances</span></div><div class="line"><span class="keyword">val</span> graph: <span class="type">Graph</span>[<span class="type">Long</span>, <span class="type">Double</span>] =</div><div class="line">  <span class="type">GraphGenerators</span>.logNormalGraph(sc, numVertices = <span class="number">100</span>).mapEdges(e =&gt; e.attr.toDouble)</div><div class="line"><span class="keyword">val</span> sourceId: <span class="type">VertexId</span> = <span class="number">42</span> <span class="comment">// The ultimate source</span></div><div class="line"><span class="comment">// Initialize the graph such that all vertices except the root have distance infinity.</span></div><div class="line"><span class="keyword">val</span> initialGraph = graph.mapVertices((id, _) =&gt;</div><div class="line">    <span class="keyword">if</span> (id == sourceId) <span class="number">0.0</span> <span class="keyword">else</span> <span class="type">Double</span>.<span class="type">PositiveInfinity</span>)</div><div class="line"><span class="keyword">val</span> sssp = initialGraph.pregel(<span class="type">Double</span>.<span class="type">PositiveInfinity</span>)(</div><div class="line">  (id, dist, newDist) =&gt; math.min(dist, newDist), <span class="comment">// Vertex Program</span></div><div class="line">  triplet =&gt; &#123;  <span class="comment">// Send Message</span></div><div class="line">    <span class="keyword">if</span> (triplet.srcAttr + triplet.attr &lt; triplet.dstAttr) &#123;</div><div class="line">      <span class="type">Iterator</span>((triplet.dstId, triplet.srcAttr + triplet.attr))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">Iterator</span>.empty</div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  (a, b) =&gt; math.min(a, b) <span class="comment">// Merge Message</span></div><div class="line">)</div><div class="line">println(sssp.vertices.collect.mkString(<span class="string">"\n"</span>))</div></pre></td></tr></table></figure></p>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>参考： <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">val jdbcDF = spark.read</div><div class="line">  .format(&quot;jdbc&quot;)</div><div class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</div><div class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</div><div class="line">  .option(&quot;user&quot;, &quot;username&quot;)</div><div class="line">  .option(&quot;password&quot;, &quot;password&quot;)</div><div class="line">  .load()</div></pre></td></tr></table></figure>
<p><strong>Running the Thrift JDBC/ODBC server:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh</div><div class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</div><div class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</div><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --master &lt;master-uri&gt; \</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p><strong>Running the Spark SQL CLI:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-sql</div></pre></td></tr></table></figure></p>
<h2 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib"></a>Spark MLlib</h2><p>参考： <a href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/ml-guide.html</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark Streaming SQL MLlib GraphX/" class="archive-article-date">
  	<time datetime="2016-11-09T02:44:54.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark Quick Start" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark Quick Start/">Spark Quick Start examples</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Spark-Quick-Start"><a href="#Spark-Quick-Start" class="headerlink" title="Spark Quick Start"></a>Spark Quick Start</h3><p><strong>Scala:</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell</div><div class="line">scala&gt; <span class="keyword">val</span> textFile = sc.textFile(<span class="string">"README.md"</span>)</div><div class="line">scala&gt; textFile.count()</div><div class="line">scala&gt; textFile.first()</div><div class="line">scala&gt; <span class="keyword">val</span> linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>))</div><div class="line">scala&gt; textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>)).count()</div></pre></td></tr></table></figure></p>
<p>more on RDD operations:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.map(line =&gt; line.split(<span class="string">" "</span>).size).reduce((a, b) =&gt; <span class="keyword">if</span> (a &gt; b) a <span class="keyword">else</span> b)</div><div class="line">scala&gt; <span class="keyword">import</span> java.lang.<span class="type">Math</span></div><div class="line">scala&gt; textFile.map(line =&gt; line.split(<span class="string">" "</span>).size).reduce((a, b) =&gt; <span class="type">Math</span>.max(a, b))</div><div class="line">scala&gt; <span class="keyword">val</span> wordCounts = textFile.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)</div><div class="line">scala&gt; wordCounts.collect()</div></pre></td></tr></table></figure></p>
<p>Self-Contained  Applications:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    <span class="keyword">val</span> logFile = <span class="string">"YOUR_SPARK_HOME/README.md"</span> <span class="comment">// Should be some file on your system</span></div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Simple Application"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    <span class="keyword">val</span> logData = sc.textFile(logFile, <span class="number">2</span>).cache()</div><div class="line">    <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">"a"</span>)).count()</div><div class="line">    <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">"b"</span>)).count()</div><div class="line">    println(<span class="string">"Lines with a: %s, Lines with b: %s"</span>.format(numAs, numBs))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>SparkConf:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">name := &quot;Simple Project&quot;</div><div class="line">version := &quot;1.0&quot;</div><div class="line">scalaVersion := &quot;2.11.7&quot;</div><div class="line">libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.0.1&quot;</div></pre></td></tr></table></figure></p>
<p><strong>Python:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">./bin/pyspark</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile = sc.textFile(<span class="string">"README.md"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.count()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.first()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>linesWithSpark = textFile.filter(<span class="keyword">lambda</span> line: <span class="string">"Spark"</span> <span class="keyword">in</span> line)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.filter(<span class="keyword">lambda</span> line: <span class="string">"Spark"</span> <span class="keyword">in</span> line).count()</div></pre></td></tr></table></figure></p>
<p>more on RDD operations:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.map(<span class="keyword">lambda</span> line: len(line.split())).reduce(<span class="keyword">lambda</span> a, b: a <span class="keyword">if</span> (a &gt; b) <span class="keyword">else</span> b)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">max</span><span class="params">(a, b)</span>:</span></div><div class="line"><span class="meta">... </span>    <span class="keyword">if</span> a &gt; b:</div><div class="line"><span class="meta">... </span>        <span class="keyword">return</span> a</div><div class="line"><span class="meta">... </span>    <span class="keyword">else</span>:</div><div class="line"><span class="meta">... </span>        <span class="keyword">return</span> b</div><div class="line">...</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.map(<span class="keyword">lambda</span> line: len(line.split())).reduce(max)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>wordCounts = textFile.flatMap(<span class="keyword">lambda</span> line: line.split()).map(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>wordCounts.collect()</div></pre></td></tr></table></figure></p>
<p>Self-Contained  Applications:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</div><div class="line"></div><div class="line">logFile = <span class="string">"YOUR_SPARK_HOME/README.md"</span>  <span class="comment"># Should be some file on your system</span></div><div class="line">sc = SparkContext(<span class="string">"local"</span>, <span class="string">"Simple App"</span>)</div><div class="line">logData = sc.textFile(logFile).cache()</div><div class="line">numAs = logData.filter(<span class="keyword">lambda</span> s: <span class="string">'a'</span> <span class="keyword">in</span> s).count()</div><div class="line">numBs = logData.filter(<span class="keyword">lambda</span> s: <span class="string">'b'</span> <span class="keyword">in</span> s).count()</div><div class="line">print(<span class="string">"Lines with a: %i, lines with b: %i"</span> % (numAs, numBs))</div><div class="line"></div><div class="line">$ YOUR_SPARK_HOME/bin/spark-submit \</div><div class="line">  --master local[<span class="number">4</span>] \</div><div class="line">  SimpleApp.py</div></pre></td></tr></table></figure></p>
<h2 id="Spark-Programming-Guide"><a href="#Spark-Programming-Guide" class="headerlink" title="Spark Programming Guide"></a>Spark Programming Guide</h2><p>参考： <a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark Quick Start/" class="archive-article-date">
  	<time datetime="2016-11-09T02:32:10.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark Cluster三种模式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark Cluster三种模式/">Spark Cluster 三种模式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Spark-Cluster-三种模式"><a href="#Spark-Cluster-三种模式" class="headerlink" title="Spark Cluster 三种模式"></a>Spark Cluster 三种模式</h2><ul>
<li><strong>Standalone</strong> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><strong>Apache Mesos</strong> – a general cluster manager that can also run Hadoop MapReduce and service applications.</li>
<li><strong>Hadoop YARN</strong> – the resource manager in Hadoop 2.</li>
</ul>
<h3 id="Standalone-模式："><a href="#Standalone-模式：" class="headerlink" title="Standalone 模式："></a>Standalone 模式：</h3><p>start Master:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-master.sh</div></pre></td></tr></table></figure></p>
<p>start Slave:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-slave.sh &lt;master-spark-URL&gt;</div></pre></td></tr></table></figure></p>
<ul>
<li>sbin/start-master.sh - Starts a master instance on the machine the script is executed on.</li>
<li>sbin/start-slaves.sh - Starts a slave instance on each machine specified in the conf/slaves file.</li>
<li>sbin/start-slave.sh - Starts a slave instance on the machine the script is executed on.</li>
<li>sbin/start-all.sh - Starts both a master and a number of slaves as described above.</li>
<li>sbin/stop-master.sh - Stops the master that was started via the bin/start-master.sh script.</li>
<li>sbin/stop-slaves.sh - Stops all slave instances on the machines specified in the conf/slaves file.</li>
<li>sbin/stop-all.sh - Stops both the master and the slaves as described above.</li>
</ul>
<p>Connecting an Application to the Cluster:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell --master spark://IP:PORT</div></pre></td></tr></table></figure></p>
<p>Launching Spark Applications:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-class org.apache.spark.deploy.Client <span class="built_in">kill</span> &lt;master url&gt; &lt;driver ID&gt;</div></pre></td></tr></table></figure></p>
<p>Resource Scheduling:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">             .setMaster(...)</div><div class="line">             .setAppName(...)</div><div class="line">             .set(<span class="string">"spark.cores.max"</span>, <span class="string">"10"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<h3 id="Running-Spark-on-Mesos-模式："><a href="#Running-Spark-on-Mesos-模式：" class="headerlink" title="Running Spark on Mesos 模式："></a>Running Spark on Mesos 模式：</h3><p>参考： <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/running-on-mesos.html</a></p>
<p><strong>Installing Mesos:</strong></p>
<ul>
<li>Spark 2.0.1 is designed for use with Mesos 0.21.0. <a href="http://mesos.apache.org/gettingstarted/" target="_blank" rel="external">http://mesos.apache.org/gettingstarted/</a><br><strong>Connecting  Spark  to  Mesos:</strong></li>
<li>To use Mesos from Spark, you need a Spark binary package available in a place accessible by Mesos, and a Spark driver program configured to connect to Mesos.<br><strong>Uploading  Spark Package:</strong></li>
<li>Download a Spark binary package from the Spark download page</li>
<li>Upload to hdfs/http/s3<br><strong>To host on HDFS, use the Hadoop fs put command:</strong><br><code>hadoop fs -put spark-2.0.1.tar.gz /path/to/spark-2.0.1.tar.gz</code><br><strong>Using a  Mesos Master URL:</strong></li>
<li>The Master URLs for Mesos are in the form <code>mesos://host:5050</code> for a single-master Mesos cluster, or <code>mesos://zk://host1:2181,host2:2181,host3:2181/mesos</code> for a multi-master Mesos cluster using ZooKeeper.<br><strong>Client Mode:</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">       .setMaster(<span class="string">"mesos://HOST:5050"</span>)</div><div class="line">       .setAppName(<span class="string">"My app"</span>)</div><div class="line">       .set(<span class="string">"spark.executor.uri"</span>, <span class="string">"&lt;path to spark-2.0.1.tar.gz uploaded above&gt;"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">     ./bin/spark-shell --master mesos:<span class="comment">//host:5050</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Cluster Mode:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">       --class org.apache.spark.examples.SparkPi \</div><div class="line">       --master mesos://207.184.161.138:7077 \</div><div class="line">       --deploy-mode cluster \</div><div class="line">       --supervise \</div><div class="line">       --executor-memory 20G \</div><div class="line">       --total-executor-cores 100 \</div><div class="line">       http://path/to/examples.jar \</div><div class="line">       1000 \</div></pre></td></tr></table></figure></p>
<h3 id="Running-Spark-on-YARN-模式："><a href="#Running-Spark-on-YARN-模式：" class="headerlink" title="Running Spark on YARN 模式："></a>Running Spark on YARN 模式：</h3><p><strong>Cluster mode:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</div><div class="line"></div><div class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">    --master yarn \</div><div class="line">    --deploy-mode cluster \</div><div class="line">    --driver-memory 4g \</div><div class="line">    --executor-memory 2g \</div><div class="line">    --executor-cores 1 \</div><div class="line">    --queue thequeue \</div><div class="line">    lib/spark-examples*.jar \</div><div class="line">    10</div></pre></td></tr></table></figure></p>
<p><strong>Client mode:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-shell --master yarn --deploy-mode client</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark Cluster三种模式/" class="archive-article-date">
  	<time datetime="2016-11-09T02:11:16.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark介绍/">Spark介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Spark-运行架构"><a href="#Spark-运行架构" class="headerlink" title="Spark 运行架构"></a>Spark 运行架构</h3><p><img src="/images/Spark_Arch.png" alt=""></p>
<p><img src="/images/Spark_Arch_1.png" alt=""></p>
<p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行。</p>
<p>Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。<strong>Driver 程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。</strong></p>
<h3 id="Spark-组件"><a href="#Spark-组件" class="headerlink" title="Spark 组件"></a>Spark 组件</h3><ul>
<li>ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。在YARN模式中为资源管理器。</li>
<li>Worker：从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制。</li>
<li>Driver：运行Application的main()函数并创建SparkContext。</li>
<li>Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。</li>
<li>SparkContext：整个应用的上下文，控制应用的生命周期。</li>
<li>RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。</li>
<li>DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler。</li>
<li>TaskScheduler：将任务（Task）分发给Executor执行。</li>
<li>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。</li>
<li>SparkConf：负责存储配置信息。</li>
</ul>
<h3 id="Spark-提交Job"><a href="#Spark-提交Job" class="headerlink" title="Spark 提交Job"></a>Spark 提交Job</h3><p>参考： <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html</a><br>可向 本地 或 集群 提交。</p>
<h3 id="推荐一本书"><a href="#推荐一本书" class="headerlink" title="推荐一本书"></a>推荐一本书</h3><p>《Advanced Analytics with Spark. 2015.4》</p>
<h3 id="生态"><a href="#生态" class="headerlink" title="生态"></a>生态</h3><p><img src="/images/Spark_Eco.jpg" alt=""></p>
<p>Spark can integaration with Hadoop ecosystem.</p>
<ul>
<li>1.Avro and Parquet can store data on Hadoop.</li>
<li>2.Can read and write to NoSQL databases like HBase and Cassandra.</li>
<li>3.Spark Streaming can ingest data from Flume and Kafka.</li>
<li>4.SparkSQL can interact with Hive Metastore.</li>
<li>5.It can run inside YARN, Hadoop’s scheduler and resource manager.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark介绍/" class="archive-article-date">
  	<time datetime="2016-11-09T01:48:14.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Cassandra" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Cassandra/">Cassandra 数据库</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Cassandra-数据库"><a href="#Cassandra-数据库" class="headerlink" title="Cassandra 数据库"></a>Cassandra 数据库</h3><p>Apache Cassandra是一套开源分布式NoSQL数据库系统。它最初由Facebook开发，用于储存收件箱等简单格式数据，集Google BigTable的数据模型与AmazonDynamo的完全分布式架构于一身。Facebook于2008将 Cassandra 开源，此后，由于Cassandra良好的可扩展性和性能，被Apple, Comcast,Instagram, Spotify, eBay, Rackspace, Netflix等知名网站所采用，成为了一种流行的分布式结构化数据存储方案。</p>
<p>在数据库排行榜“DB-Engines Ranking”中，Cassandra排在第七位，是非关系型数据库中排名第二高的（仅次于MongoDB）。</p>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>Cassandra使用了Google 设计的 BigTable的数据模型，Cassandra使用的是宽列存储模型(Wide Column Stores)，每行数据由row key唯一标识之后，可以有最多20亿个列，每个列由一个column key标识，每个column key下对应若干value。这种模型可以理解为是一个二维的key-value存储，即整个数据模型被定义成一个类似 map&lt; key1, map&lt; key2,value&gt;&gt;的类型。</p>
<h3 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h3><p>与BigTable和其模仿者HBase不同，Cassandra的数据并不存储在分布式文件系统如GFS或HDFS中，而是直接存于本地。与BigTable一样，Cassandra也是日志型数据库，即把新写入的数据存储在内存的Memtable中并通过磁盘中的CommitLog来做持久化，内存填满后将数据按照key的顺序写进一个只读文件SSTable中，每次读取数据时将所有SSTable和内存中的数据进行查找和合并。这种系统的特点是写入比读取更快，因为写入一条数据是顺序计入commit log中，不需要随机读取磁盘以及搜索。</p>
<h3 id="与类似开源系统的比较"><a href="#与类似开源系统的比较" class="headerlink" title="与类似开源系统的比较"></a>与类似开源系统的比较</h3><p>HBase是Apache Hadoop项目的一个子项目，是Google BigTable的一个克隆，与Cassandra一样，它们都使用了BigTable的列族式的数据模型，但是：</p>
<ul>
<li>Cassandra只有一种节点，而HBase有多种不同角色，除了处理读写请求的region server之外，其架构在一套完整的HDFS分布式文件系统之上，并需要ZooKeeper来同步集群状态，部署上Cassandra更简单。</li>
<li>Cassandra的数据一致性策略是可配置的，可选择是强一致性还是性能更高的最终一致性；而HBase总是强一致性的。</li>
<li>Cassandra通过一致性哈希来决定一行数据存储在哪些节点，靠概率上的平均来实现负载均衡；而HBase每段数据(region)只有一个节点负责处理，由master来动态分配一个region是否大到需要拆分成两个，同时会将过热的节点上的一些region动态的分配给负载较低的节点，因此实现动态的负载均衡。</li>
<li>因为每个region同时只能有一个节点处理，一旦这个节点无响应，在系统将这个节点的所有region转移到其他节点之前这些数据便无法读写，加上master也只有一个节点，备用master的恢复也需要时间，因此HBase在一定程度上有单点问题；而Cassandra无单点问题。</li>
<li>Cassandra的读写性能优于HBase。</li>
</ul>
<h3 id="Cassandra-服务启动"><a href="#Cassandra-服务启动" class="headerlink" title="Cassandra 服务启动"></a>Cassandra 服务启动</h3><ul>
<li><p>1.启动服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./cassandra   org.apache.cassandra.service.CassandraDaemon</div></pre></td></tr></table></figure>
</li>
<li><p>2.启动用户交互              实际启动cqlsh.py来进行交互</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./cqlsh</div></pre></td></tr></table></figure>
</li>
</ul>
<p>Cassandra Shell 命令：  <a href="http://www.w3ii.com/cassandra/cassandra_shell_commands.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_shell_commands.html</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./cassandra -f           前台启动 cassandra</div></pre></td></tr></table></figure></p>
<ul>
<li>3.停止服务<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">kill</span>  pid</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Cql-使用"><a href="#Cql-使用" class="headerlink" title="Cql 使用"></a>Cql 使用</h3><p><strong>创建使用Cqlsh一个密钥空间</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_create_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_create_keyspace.html</a><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> KEYSPACE tutorialspoint</div><div class="line"><span class="keyword">WITH</span> <span class="keyword">replication</span> = &#123;<span class="string">'class'</span>:<span class="string">'SimpleStrategy'</span>, <span class="string">'replication_factor'</span> : <span class="number">3</span>&#125;;</div><div class="line"><span class="keyword">DESCRIBE</span> keyspaces;</div></pre></td></tr></table></figure></p>
<p><strong>使用Java创建API密钥空间一</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_create_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_create_keyspace.html</a></p>
<p><strong>改变使用Cqlsh KEYSPACE</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_alter_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_alter_keyspace.html</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ALTER KEYSPACE <span class="string">"KeySpace Name"</span> WITH replication = &#123;<span class="string">'class'</span>: <span class="string">'Strategy name'</span>, <span class="string">'replication_factor'</span> : <span class="string">'No.Of  replicas'</span>&#125;;</div><div class="line">ALTER KEYSPACE tutorialspoint WITH replication = &#123;<span class="string">'class'</span>:<span class="string">'SimpleStrategy'</span>, <span class="string">'replication_factor'</span> : 2&#125;;</div></pre></td></tr></table></figure></p>
<p><strong>测试密钥空间的durable_writes属性</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cqlsh&gt; SELECT * FROM system_schema.keyspaces;</div><div class="line">ALTER KEYSPACE <span class="built_in">test</span></div><div class="line">WITH REPLICATION = &#123;<span class="string">'class'</span> : <span class="string">'NetworkTopologyStrategy'</span>, <span class="string">'datacenter1'</span> : 3&#125;</div><div class="line">AND DURABLE_WRITES = <span class="literal">true</span>;</div></pre></td></tr></table></figure></p>
<p><strong>删除使用Cqlsh一个密钥空间</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DROP KEYSPACE tutorialspoint;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建表</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">USE tutorialspoint;</div><div class="line">cqlsh:tutorialspoint&gt;; CREATE TABLE emp(</div><div class="line">   emp_id int PRIMARY KEY,</div><div class="line">   emp_name text,</div><div class="line">   emp_city text,</div><div class="line">   emp_sal varint,</div><div class="line">   emp_phone varint</div><div class="line">   );</div><div class="line">cqlsh:tutorialspoint&gt; select * from emp;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra修改表</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; ALTER TABLE emp</div><div class="line">   ... ADD emp_email text;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra删除表</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; DROP TABLE emp;</div><div class="line">cqlsh:tutorialspoint&gt; DESCRIBE COLUMNFAMILIES;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra截断表</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tp&gt; TRUNCATE student;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建索引</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; CREATE INDEX name ON emp1 (emp_name);</div></pre></td></tr></table></figure></p>
<p>Cassandra DROP INDEX<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tp&gt; drop index name;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra批量</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; BEGIN BATCH</div><div class="line">     INSERT INTO emp (emp_id, emp_city, emp_name, emp_phone, emp_sal) values(  4,<span class="string">'Pune'</span>,<span class="string">'rajeev'</span>,9848022331, 30000);</div><div class="line">     UPDATE emp SET emp_sal = 50000 WHERE emp_id =3;</div><div class="line">     DELETE emp_city FROM emp WHERE emp_id = 2;</div><div class="line">     APPLY BATCH;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建数据</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(1,<span class="string">'ram'</span>, <span class="string">'Hyderabad'</span>, 9848022338, 50000);</div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(2,<span class="string">'robin'</span>, <span class="string">'Hyderabad'</span>, 9848022339, 40000);</div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(3,<span class="string">'rahman'</span>, <span class="string">'Chennai'</span>, 9848022330, 45000);</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra更新数据</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; UPDATE emp SET emp_city=<span class="string">'Delhi'</span>,emp_sal=50000</div><div class="line">   WHERE emp_id=2;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra删除数据</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; DELETE emp_sal FROM emp WHERE emp_id=3;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra CQL集合</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data(name text PRIMARY KEY, email list&lt;text&gt;);</div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data(name, email) VALUES (<span class="string">'ramu'</span>, [<span class="string">'abc@gmail.com'</span>,<span class="string">'cba@yahoo.com'</span>])</div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data SET email = email +[<span class="string">'xyz@w3ii.com'</span>] <span class="built_in">where</span> name = <span class="string">'ramu'</span>;</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data2 (name text PRIMARY KEY, phone <span class="built_in">set</span>&lt;varint&gt;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data2(name, phone)VALUES (<span class="string">'rahman'</span>,    &#123;9848022338,9848022339&#125;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data2 SET phone = phone + &#123;9848022330&#125; <span class="built_in">where</span> name = <span class="string">'rahman'</span>;</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data3 (name text PRIMARY KEY, address map&lt;timestamp, text&gt;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data3 (name, address) VALUES (<span class="string">'robin'</span>, &#123;<span class="string">'home'</span> : <span class="string">'hyderabad'</span> , <span class="string">'office'</span> : <span class="string">'Delhi'</span> &#125; );</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data3 SET address = address+&#123;<span class="string">'office'</span>:<span class="string">'mumbai'</span>&#125; WHERE name = <span class="string">'robin'</span>;</div></pre></td></tr></table></figure></p>
<h3 id="Cassandra-安装-及-源代码分析"><a href="#Cassandra-安装-及-源代码分析" class="headerlink" title="Cassandra 安装 及 源代码分析"></a>Cassandra 安装 及 源代码分析</h3><p><strong>service 启动入口函数：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">org.apache.cassandra.service.CassandraDaemon</div></pre></td></tr></table></figure></p>
<p><strong>installation:</strong><br><a href="http://cassandra.apache.org/doc/latest/getting_started/installing.html" target="_blank" rel="external">http://cassandra.apache.org/doc/latest/getting_started/installing.html</a></p>
<p><strong>import源代码进Eclipse：</strong></p>
<ul>
<li>1.ant build</li>
<li>2.ant generate-eclipse-files</li>
</ul>
<p>还有其它操作：</p>
<ul>
<li>执行ant avro-generate</li>
<li>执行ant gen-thrift-java</li>
<li>执行ant generate-eclipse-files</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Cassandra/" class="archive-article-date">
  	<time datetime="2016-11-08T13:09:51.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/">Cassandra</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/NoSQL/">NoSQL</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-NoSQL介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/NoSQL介绍/">NoSQL参考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="NoSQL-参考"><a href="#NoSQL-参考" class="headerlink" title="NoSQL 参考"></a>NoSQL 参考</h3><p><img src="/images/NoSQL.png" alt=""></p>
<h3 id="NoSQL-对比"><a href="#NoSQL-对比" class="headerlink" title="NoSQL 对比"></a>NoSQL 对比</h3><p><img src="/images/NoSQL_Compare.png" alt=""></p>
<h3 id="CAP-原理"><a href="#CAP-原理" class="headerlink" title="CAP 原理"></a>CAP 原理</h3><p><img src="/images/CAP.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/NoSQL介绍/" class="archive-article-date">
  	<time datetime="2016-11-08T12:49:48.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NoSQL/">NoSQL</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/NoSQL/">NoSQL</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Storm介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Storm介绍/">Storm(流计算)介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Storm-简介"><a href="#Storm-简介" class="headerlink" title="Storm 简介"></a>Storm 简介</h2><p>参考： <a href="http://storm.apache.org/releases/0.10.2/index.html" target="_blank" rel="external">http://storm.apache.org/releases/0.10.2/index.html</a><br>Storm是一个分布式计算框架，主要由Clojure编程语言编写。最初是由Nathan Marz[1]及其团队创建于BackType，[2]该项目在被Twitter取得后开源。[3]它使用用户创建的“管（spouts）”和“螺栓（bolts）”来定义信息源和操作来允许批量、分布式处理流式数据。</p>
<h3 id="Storm-源代码导入-Eclipse"><a href="#Storm-源代码导入-Eclipse" class="headerlink" title="Storm 源代码导入 Eclipse"></a>Storm 源代码导入 Eclipse</h3><p>下载源代码并导入Eclipse： （可参考网页：<a href="http://ylzhj02.iteye.com/blog/2162197）" target="_blank" rel="external">http://ylzhj02.iteye.com/blog/2162197）</a></p>
<ul>
<li>1.git clone git://github.com/apache/storm.git</li>
<li>2.mvn clean package install -DskipTests=true</li>
<li>3.mvn eclipse:eclipse</li>
</ul>
<h3 id="Storm-Topology-架构"><a href="#Storm-Topology-架构" class="headerlink" title="Storm Topology 架构"></a>Storm Topology 架构</h3><p><img src="/images/Storm_Topology.png" alt=""></p>
<ul>
<li>Topology：storm中运行的一个实时应用程序，因为各个组件间的消息流动形成逻辑上的一个拓扑结构。</li>
<li>Spout：在一个topology中产生源数据流的组件。通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。Spout是一个主动的角色，其接口中有个nextTuple()函数，storm框架会不停地调用此函数，用户只要在其中生成源数据即可。</li>
<li>Bolt：在一个topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。</li>
<li>Tuple：一次消息传递的基本单元。本来应该是一个key-value的map，但是由于各个组件间传递的tuple的字段名称已经事先定义好，所以tuple中只要按序填入各个value就行了，所以就是一个value list.</li>
<li>Stream：源源不断传递的tuple就组成了stream。</li>
</ul>
<h3 id="Storm-Detail"><a href="#Storm-Detail" class="headerlink" title="Storm Detail"></a>Storm Detail</h3><p>在Storm的集群里面有两种节点： 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个叫Nimbus后台程序，它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分发代码，分配计算任务给机器，并且监控状态。<br>每一个工作节点上面运行一个叫做Supervisor的节点。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭工作进程。每一个工作进程执行一个topology的一个子集；一个运行的topology由运行在很多机器上的很多工作进程组成。<br><img src="/images/Storm1.png" alt="">  <img src="/images/Storm2.jpg" alt="">  <img src="/images/Storm3.png" alt=""></p>
<ul>
<li>Storm提供的最基本的处理stream的原语是spout和bolt。你可以实现spout和bolt提供的接口来处理你的业务逻辑。</li>
<li>消息源spout是Storm里面一个topology里面的消息生产者。一般来说消息源会从一个外部源读取数据并且向topology里面发出消息：tuple。Spout可以是可靠的也可以是不可靠的。如果这个tuple没有被storm成功处理，可靠的消息源spouts可以重新发射一个tuple， 但是不可靠的消息源spouts一旦发出一个tuple就不能重发了。</li>
<li>消息源可以发射多条消息流stream。使用OutputFieldsDeclarer.declareStream来定义多个stream，然后使用SpoutOutputCollector来发射指定的stream。</li>
<li>Spout类里面最重要的方法是nextTuple。要么发射一个新的tuple到topology里面或者简单的返回如果已经没有新的tuple。要注意的是nextTuple方法不能阻塞，因为storm在同一个线程上面调用所有消息源spout的方法。</li>
<li>另外两个比较重要的spout方法是ack和fail。storm在检测到一个tuple被整个topology成功处理的时候调用ack，否则调用fail。storm只对可靠的spout调用ack和fail。</li>
<li>所有的消息处理逻辑被封装在bolts里面。Bolts可以做很多事情：过滤，聚合，查询数据库等等。</li>
<li>Bolts可以简单的做消息流的传递。复杂的消息流处理往往需要很多步骤，从而也就需要经过很多bolts。比如算出一堆图片里面被转发最多的图片就至少需要两步：第一步算出每个图片的转发数量。第二步找出转发最多的前10个图片。(如果要把这个过程做得更具有扩展性那么可能需要更多的步骤)。</li>
<li>Bolts可以发射多条消息流， 使用OutputFieldsDeclarer.declareStream定义stream，使用OutputCollector.emit来选择要发射的stream。</li>
<li>Bolts的主要方法是execute, 它以一个tuple作为输入，bolts使用OutputCollector来发射tuple，bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知Storm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。 一般的流程是： bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。</li>
<li>定义一个topology的其中一步是定义每个bolt接收什么样的流作为输入。stream grouping就是用来定义一个stream应该如果分配数据给bolts上面的多个tasks。</li>
</ul>
<h3 id="Storm里面有7种类型的stream-grouping"><a href="#Storm里面有7种类型的stream-grouping" class="headerlink" title="Storm里面有7种类型的stream grouping"></a>Storm里面有7种类型的stream grouping</h3><ul>
<li>　　Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。</li>
<li>　　Fields Grouping：按字段分组， 比如按userid来分组， 具有同样userid的tuple会被分到相同的Bolts里的一个task， 而不同的userid则会被分配到不同的bolts里的task。</li>
<li>　　All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。</li>
<li>　　Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。</li>
<li>　　Non Grouping：不分组，这个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。</li>
<li>　　Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。 只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id (OutputCollector.emit方法也会返回task的id)。</li>
<li>　　Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Storm介绍/" class="archive-article-date">
  	<time datetime="2016-11-08T12:27:38.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hive" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hive/">Hive(数据仓库)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h2><p>参考： <a href="https://cwiki.apache.org/confluence/display/Hive/Home" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Home</a><br>Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。Apache Hive起初由Facebook开发。</p>
<p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
<ul>
<li>1.hive是一个数据仓库</li>
<li>2.hive基于hadoop。</li>
</ul>
<h3 id="Hive-安装"><a href="#Hive-安装" class="headerlink" title="Hive 安装"></a>Hive 安装</h3><p>参考： <a href="http://doctuts.readthedocs.io/en/latest/hive.html" target="_blank" rel="external">http://doctuts.readthedocs.io/en/latest/hive.html</a></p>
<ul>
<li><p>1.安装hadoop</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=/root/hadoop-2.7.3</div><div class="line">export HIVE_HOME=/root/apache-hive-2.1.0-bin</div><div class="line">export PATH=$PATH:$HIVE_HOME/bin</div></pre></td></tr></table></figure>
</li>
<li><p>2.启动hadoop hdfs文件系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs   namenode   -format</div><div class="line">sbin/start-all.sh    或者   start-dfs.sh and start-yarn.sh</div></pre></td></tr></table></figure>
</li>
<li><p>3.创建文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hadoop fs -mkdir /usr/hive/warehouse</div><div class="line">bin/hadoop fs -chmod g+w /usr/hive/warehouse</div></pre></td></tr></table></figure>
</li>
<li><p>4.第一次运行hive之前，需要设置schema</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">schematool -initSchema -dbType derby</div></pre></td></tr></table></figure>
</li>
</ul>
<p>如果已经尝试运行hive出错之后，再去设置schema也会出错，需要做：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv  metastore_db  metastore_db.tmp</div></pre></td></tr></table></figure></p>
<ul>
<li>5.运行hive<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hive</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Hive-启动源代码入口："><a href="#Hive-启动源代码入口：" class="headerlink" title="Hive 启动源代码入口："></a>Hive 启动源代码入口：</h3><p>bin/hive  —&gt;  hive script中会执行  bin/ext/*.sh, 以及 bin/ext/util/*.sh 命令<br>SERVICE_LIST 变量 在bin/ext/*.sh 中增加value；<br>SERVICE 变量在启动命令时 赋值； 默认   SERVICE=”cli”</p>
<p><img src="/images/Hive_Code_1.png" alt=""> <img src="/images/Hive_Code_2.png" alt=""></p>
<p>—&gt;   execHiveCmd 启动 java 源代码的入口               org.apache.hive.beeline.cli.HiveCli   或者   org.apache.hadoop.hive.cli.CliDriver</p>
<h3 id="Hive-vs-HBase"><a href="#Hive-vs-HBase" class="headerlink" title="Hive vs HBase"></a>Hive vs HBase</h3><p>共同点：</p>
<ul>
<li>1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储</li>
</ul>
<p>区别：</p>
<ul>
<li>1.Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。</li>
<li>2.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。</li>
<li>3.Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。</li>
<li>4.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。</li>
<li>5.hive借用hadoop的MapReduce来完成一些hive中的命令的执行</li>
<li>6.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。</li>
<li>7.hbase是列存储。</li>
<li>8.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。</li>
<li>9.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。</li>
</ul>
<h3 id="improt-Hive源代码到Eclipse里面："><a href="#improt-Hive源代码到Eclipse里面：" class="headerlink" title="improt Hive源代码到Eclipse里面："></a>improt Hive源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h3 id="Hive-知识范围"><a href="#Hive-知识范围" class="headerlink" title="Hive 知识范围"></a>Hive 知识范围</h3><p><img src="/images/Hive_Related.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hive/" class="archive-article-date">
  	<time datetime="2016-11-08T09:51:43.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HBase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HBase/">HBase(非关系型分布式数据库)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBase-简介"><a href="#HBase-简介" class="headerlink" title="HBase 简介"></a>HBase 简介</h2><p>HBase 是一个开源的非关系型分布式数据库。 <a href="https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hbase-tutorial-get-started-linux/" target="_blank" rel="external">https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hbase-tutorial-get-started-linux/</a></p>
<ul>
<li>HBase架构指南： <a href="http://www.guru99.com/hbase-architecture-data-flow-usecases.html" target="_blank" rel="external">http://www.guru99.com/hbase-architecture-data-flow-usecases.html</a></li>
<li>安装指南： <a href="http://www.guru99.com/hbase-installation-guide.html" target="_blank" rel="external">http://www.guru99.com/hbase-installation-guide.html</a></li>
<li>HBase Shell 命令指南： <a href="http://www.guru99.com/hbase-shell-general-commands.html" target="_blank" rel="external">http://www.guru99.com/hbase-shell-general-commands.html</a></li>
</ul>
<h2 id="HBase-NoSQL-数据库优缺点"><a href="#HBase-NoSQL-数据库优缺点" class="headerlink" title="HBase NoSQL 数据库优缺点"></a>HBase NoSQL 数据库优缺点</h2><p>Hbase,Casandra,Bigtable都属于面向 <strong>列存储</strong> 的分布式存储系统。</p>
<p>HBase 基本单元：</p>
<ul>
<li>Table: Collection of rows present.</li>
<li>Row: Collection of column families.</li>
<li>Column Family: Collection of columns.</li>
<li>Column: Collection of key-value pairs.</li>
<li>Namespace: Logical grouping of tables.</li>
<li>Cell: A {row, column, version} tuple exactly specifies a cell definition in HBase.</li>
</ul>
<p>列存储 vs 行存储：<br><img src="/images/Column_vs_Row.png" alt=""></p>
<p>Hbase的优点：<br>1 列的可以动态增加，并且列为空就不存储数据,节省存储空间.<br>2 Hbase自动切分数据，使得数据存储自动具有水平scalability.<br>3 Hbase可以提供高并发读写操作的支持</p>
<p>Hbase的缺点：<br>1 不能支持条件查询，只支持按照Row key来查询.<br>2 暂时不能支持Master server的故障切换,当Master宕机后,整个存储系统就会挂掉.</p>
<h2 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h2><p><img src="/images/HBase_Arch.png" alt=""></p>
<p><img src="/images/HBase_Arch_1.png" alt=""></p>
<h2 id="Hbase-Data-Flow"><a href="#Hbase-Data-Flow" class="headerlink" title="Hbase Data Flow"></a>Hbase Data Flow</h2><p><img src="/images/HBase_Data_Flow.png" alt=""></p>
<p>The Read and Write operations from Client into Hfile can be shown in below diagram.</p>
<ul>
<li>Step 1) Client wants to write data and in turn first communicates with Regions server and then regions</li>
<li>Step 2) Regions contacting memstore for storing associated with the column family</li>
<li>Step 3) First data stores into Memstore, where the data is sorted and after that it flushes into HFile. The main reason for using Memstore is to store data in Distributed file system based on Row Key. Memstore will be placed in Region server main memory while HFiles are written into HDFS.</li>
<li>Step 4) Client wants to read data from Regions</li>
<li>Step 5) In turn Client can have direct access to Mem store, and it can request for data.</li>
<li>Step 6) Client approaches HFiles to get the data. The data are fetched and retrieved by the Client.</li>
</ul>
<h2 id="HBase-vs-HDFS"><a href="#HBase-vs-HDFS" class="headerlink" title="HBase vs HDFS"></a>HBase vs HDFS</h2><p><img src="/images/HBase_vs_HDFS.png" alt=""></p>
<h3 id="improt-HBase源代码到Eclipse里面："><a href="#improt-HBase源代码到Eclipse里面：" class="headerlink" title="improt HBase源代码到Eclipse里面："></a>improt HBase源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h2 id="HBase-源码入口"><a href="#HBase-源码入口" class="headerlink" title="HBase 源码入口"></a>HBase 源码入口</h2><p><img src="/images/HBase_Code.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HBase/" class="archive-article-date">
  	<time datetime="2016-11-08T09:16:42.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop 源码入口" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop 源码入口/">Hadoop 源码入口</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="improt-Hadoop源代码到Eclipse里面："><a href="#improt-Hadoop源代码到Eclipse里面：" class="headerlink" title="improt Hadoop源代码到Eclipse里面："></a>improt Hadoop源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h2 id="Hadoop-源码入口"><a href="#Hadoop-源码入口" class="headerlink" title="Hadoop 源码入口"></a>Hadoop 源码入口</h2><p>这些是Hadoop 源码阅读的入口位置，可以通过这些main函数看进源码实现。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Hadoop <span class="built_in">command</span>:</div><div class="line">    <span class="comment"># the core commands</span></div><div class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fs"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.fs.FsShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"version"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"jar"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.RunJar</div><div class="line">      <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$&#123;YARN_OPTS&#125;</span>"</span> ]] || [[ -n <span class="string">"<span class="variable">$&#123;YARN_CLIENT_OPTS&#125;</span>"</span> ]]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"WARNING: Use \"yarn jar\" to launch YARN applications."</span> 1&gt;&amp;2</div><div class="line">      <span class="keyword">fi</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"key"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.crypto.key.KeyShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"checknative"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.NativeLibraryChecker</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"distcp"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tools.DistCp</div><div class="line">      CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"daemonlog"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.log.LogLevel</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"archive"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tools.HadoopArchives</div><div class="line">      CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"credential"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.security.alias.CredentialShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"trace"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tracing.TraceAdmin</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"classpath"</span> ] ; <span class="keyword">then</span></div><div class="line">      <span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> -gt 1 ]; <span class="keyword">then</span></div><div class="line">        CLASS=org.apache.hadoop.util.Classpath</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">Hdfs <span class="built_in">command</span>:</div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"namenode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.namenode.NameNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_NAMENODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"zkfc"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.tools.DFSZKFailoverController'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_ZKFC_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"secondarynamenode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_SECONDARYNAMENODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"datanode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.datanode.DataNode'</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$starting_secure_dn</span>"</span> = <span class="string">"true"</span> ]; <span class="keyword">then</span></div><div class="line">    HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -jvm server <span class="variable">$HADOOP_DATANODE_OPTS</span>"</span></div><div class="line">  <span class="keyword">else</span></div><div class="line">    HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -server <span class="variable">$HADOOP_DATANODE_OPTS</span>"</span></div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"journalnode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.qjournal.server.JournalNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_JOURNALNODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"dfs"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.fs.FsShell</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"dfsadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"haadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSHAAdmin</div><div class="line">  CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fsck"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSck</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"balancer"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_BALANCER_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"mover"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.server.mover.Mover</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$&#123;HADOOP_OPTS&#125;</span> <span class="variable">$&#123;HADOOP_MOVER_OPTS&#125;</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"storagepolicies"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.StoragePolicyAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"jmxget"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.JMXGet</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oiv"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oiv_legacy"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oev"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fetchdt"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DelegationTokenFetcher</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"getconf"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.GetConf</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"groups"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.GetGroups</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"snapshotDiff"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"lsSnapshottableDir"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"portmap"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.portmap.Portmap</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_PORTMAP_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"nfs3"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.nfs.nfs3.Nfs3</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_NFS3_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"cacheadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.CacheAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"crypto"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.CryptoAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"version"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"debug"</span> ]; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DebugAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"classpath"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> -gt 0 ]; <span class="keyword">then</span></div><div class="line">    CLASS=org.apache.hadoop.util.Classpath</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter</div><div class="line">org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">Yarn:</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"rmadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.client.cli.RMAdminCLI'</span></div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"scmadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.client.SCMAdmin'</span></div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"application"</span> ] ||</div><div class="line">     [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"applicationattempt"</span> ] ||</div><div class="line">     [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"container"</span> ]; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.ApplicationCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line">  <span class="built_in">set</span> -- <span class="variable">$COMMAND</span> <span class="variable">$@</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"node"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.NodeCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"queue"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.QueueCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"resourcemanager"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$YARN_CONF_DIR</span>/rm-config/log4j.properties</div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.server.resourcemanager.ResourceManager’               main     —&gt; serviceInit —&gt; serviceStart</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_RESOURCEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_RESOURCEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_RESOURCEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "historyserver" ] ; then</div><div class="line">  echo "DEPRECATED: Use of this command to start the timeline server is deprecated." 1&gt;&amp;2</div><div class="line">  echo "Instead use the timelineserver command for it." 1&gt;&amp;2</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/ahs-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_HISTORYSERVER_OPTS"</div><div class="line">  if [ "$YARN_HISTORYSERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_HISTORYSERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "timelineserver" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/timelineserver-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_TIMELINESERVER_OPTS"</div><div class="line">  if [ "$YARN_TIMELINESERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_TIMELINESERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "sharedcachemanager" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/scm-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_SHAREDCACHEMANAGER_OPTS"</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_SHAREDCACHEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_SHAREDCACHEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_SHAREDCACHEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "nodemanager" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/nm-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.nodemanager.NodeManager<span class="string">'                      main     —&gt; serviceInit —&gt; serviceStart</span></div><div class="line">  YARN_OPTS="$YARN_OPTS -server $YARN_NODEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_NODEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_NODEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "proxyserver" ] ; then</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_PROXYSERVER_OPTS"</div><div class="line">  if [ "$YARN_PROXYSERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_PROXYSERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "version" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "jar" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.util.RunJar</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "logs" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.LogsCLI</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "daemonlog" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.log.LogLevel</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "cluster" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.ClusterCLI</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">else</div><div class="line">  CLASS=$COMMAND</div><div class="line">fi</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop 源码入口/" class="archive-article-date">
  	<time datetime="2016-11-08T08:54:09.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop Yarn(待补充)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop Yarn(待补充)/">Hadoop Yarn(待补充)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-Yarn-流程图"><a href="#Hadoop-Yarn-流程图" class="headerlink" title="Hadoop Yarn 流程图"></a>Hadoop Yarn 流程图</h2><p>Hadoop v1 资源调度：<br><img src="/images/Hadoop_Yarn_1.png" alt=""></p>
<p>Hadoop v2 Yarn 资源调度：<br><img src="/images/Hadoop_Yarn_2.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop Yarn(待补充)/" class="archive-article-date">
  	<time datetime="2016-11-08T08:50:52.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop MapReduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop MapReduce/">Hadoop MapReduce</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-MapReduce-流程图"><a href="#Hadoop-MapReduce-流程图" class="headerlink" title="Hadoop MapReduce 流程图"></a>Hadoop MapReduce 流程图</h2><p><img src="/images/Hadoop_MapReduce.png" alt=""></p>
<p><img src="/images/Hadoop_MapReduce_1.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce.jpg" alt=""></p>
<p><strong>在hadoop中，map-&gt;combine-&gt;partition-&gt;shuffle-&gt;reduce，五个步骤的作用分别是什么？</strong></p>
<ul>
<li>combine和partition都是函数，中间的步骤应该只有shuffle！</li>
<li>combine分为map端和reduce端，作用是把同一个key的键值对合并在一起，可以自定义的。</li>
<li>combine函数把一个map函数产生的<key,value>对（多个key,value）合并成一个新的<key2,value2>.将新的<key2,value2>作为输入到reduce函数中</key2,value2></key2,value2></key,value></li>
<li>这个value2亦可称之为values，因为有多个。这个合并的目的是为了减少网络传输。</li>
<li>partition是分割map每个节点的结果，按照key分别映射给不同的reduce，也是可以自定义的。这里其实可以理解归类。</li>
<li>partition的作用就是把这些数据归类。只不过在写程序的时候，mapreduce使用哈希HashPartitioner帮我们归类了。这个我们也可以自定义。</li>
<li>shuffle就是map和reduce之间的过程，包含了两端的combine和partition。</li>
<li>Map的结果，会通过partition分发到Reducer上，Reducer做完Reduce操作后，通过OutputFormat，进行输出</li>
<li>shuffle阶段的主要函数是fetchOutputs(),这个函数的功能就是将map阶段的输出，copy到reduce 节点本地。</li>
</ul>
<p><em>摘自aboutyun社区，一些值得思考的问题</em><br><strong>1.Shuffle的定义是什么？</strong><br><strong>2.map task与reduce task的执行是否在不同的节点上？</strong><br><strong>3.Shuffle产生的意义是什么？</strong><br><strong>4.每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据该如何处理？</strong><br><strong>5.在map task执行时，它是如何读取HDFS的？</strong><br><strong>6.读取的Split与block的对应关系可能是什么？</strong><br><strong>7.MapReduce提供Partitioner接口，它的作用是什么？</strong><br><strong>8.溢写是在什么情况下发生？</strong><br><strong>9.溢写是为什么不影响往缓冲区写map结果的线程？</strong><br><strong>10.当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对谁的排序？</strong><br><strong>11.哪些场景才能使用Combiner呢？</strong><br><strong>12.Merge的作用是什么？</strong><br><strong>13.reduce中Copy过程采用是什么协议？</strong><br><strong>14.reduce中merge过程有几种方式，与map有什么相似之处？</strong><br><strong>15.溢写过程中如果有很多个key/value对需要发送到某个reduce端去，那么如何处理这些key/value值</strong></p>
<p><strong>Shuffle产生的意义是什么？</strong><br>在Hadoop这样的集群环境中，大部分map task与reduce task的执行是在不同的节点上。当然很多情况下Reduce执行时需要跨节点去拉取其它节点上的map task结果。如果集群正在运行的job有很多，那么task的正常执行对集群内部的网络资源消耗会很严重。这种网络消耗是正常的，我们不能限制，能做的就是最大化地减少不必要的消耗。还有在节点内，相比于内存，磁盘IO对job完成时间的影响也是可观的。从最基本的要求来说，Shuffle过程的期望可以有：<br>完整地从map task端拉取数据到reduce 端。<br>在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗。<br>减少磁盘IO对task执行的影响。</p>
<p><strong>每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据该如何处理？</strong><br>每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p>
<p><strong>MapReduce提供Partitioner接口，它的作用是什么？</strong><br>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。</p>
<p><strong>什么是溢写？</strong><br>在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写。</p>
<p><strong>溢写是为什么不影响往缓冲区写map结果的线程？</strong><br>溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</p>
<p><strong>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对谁的排序？</strong><br>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。</p>
<p><strong>溢写过程中如果有很多个key/value对需要发送到某个reduce端去，那么如何处理这些key/value值？</strong><br>如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p>
<p><strong>哪些场景才能使用Combiner呢？</strong><br>Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。</p>
<p><strong>Merge的作用是什么？</strong><br>最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge</p>
<p><strong>每个reduce task不断的通过什么协议从JobTracker那里获取map task是否完成的信息？</strong><br>每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息</p>
<p><strong>reduce中Copy过程采用是什么协议？</strong><br>Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。</p>
<p><strong>reduce中merge过程有几种方式？</strong><br>merge有三种形式：1)内存到内存  2)内存到磁盘  3)磁盘到磁盘。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。</p>
<h3 id="Map-过程"><a href="#Map-过程" class="headerlink" title="Map 过程"></a>Map 过程</h3><p><img src="/images/Hadoop_Map.jpg" alt=""><br>整个流程分了四步。简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p>
<p>当然这里的每一步都可能包含着多个步骤与细节，下面对细节来说明：</p>
<ul>
<li><p>1.在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。在WordCount例子里，假设map的输入数据都是像“aaa”这样的字符串。</p>
</li>
<li><p>2.在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。这个job有多个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。</p>
</li>
</ul>
<p>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。</p>
<p>在WordCount例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。</p>
<ul>
<li>3.这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写，字面意思很直观。这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</li>
</ul>
<p>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。</p>
<p>在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节是，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p>
<p>在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。</p>
<p>如果client设置过Combiner，那么现在就是使用Combiner的时候了。将有相同key的key/value对的value加起来，减少溢写到磁盘的数据量。Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用。那哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。</p>
<ul>
<li>4.每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个溢写文件存在。当map task真正完成时，内存缓冲区中的数据也全部溢写到磁盘中形成一个溢写文件。最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge。Merge是怎样的？如前面的例子，“aaa”从某个map task读取过来时值是5，从另外一个map 读取时值是8，因为它们有相同的key，所以得merge成group。什么是group。对于“aaa”就是像这样的：{“aaa”, [5, 8, 2, …]}，数组中的值就是从不同溢写文件中读取出来的，然后再把这些值加起来。请注意，因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在，在这个过程中如果client设置过Combiner，也会使用Combiner来合并相同的key。</li>
</ul>
<p>至此，map端的所有工作都已结束，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。</p>
<h3 id="Reduce-过程"><a href="#Reduce-过程" class="headerlink" title="Reduce 过程"></a>Reduce 过程</h3><p><img src="/images/Hadoop_Reduce.jpg" alt=""></p>
<p>如map端的细节图，Shuffle在reduce端的过程也能用图上标明的三点来概括。当前reduce copy数据的前提是它要从JobTracker获得有哪些map task已执行结束。Reducer真正运行之前，所有的时间都是在拉取数据，做merge，且不断重复地在做。如前面的方式一样，下面也分段地描述reduce 端的Shuffle细节：</p>
<ul>
<li><p>1.Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</p>
</li>
<li><p>2.Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：1)内存到内存  2)内存到磁盘  3)磁盘到磁盘。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。</p>
</li>
<li><p>3.Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。</p>
</li>
</ul>
<h3 id="Map-Reduce-过程图片"><a href="#Map-Reduce-过程图片" class="headerlink" title="Map-Reduce 过程图片"></a>Map-Reduce 过程图片</h3><p><img src="/images/Hadoop_Map_Reduce.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce_1.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce_2.png" alt=""></p>
<h3 id="Map-Reduce-过程例子"><a href="#Map-Reduce-过程例子" class="headerlink" title="Map-Reduce 过程例子"></a>Map-Reduce 过程例子</h3><p><img src="/images/Hadoop_Map_Reduce_Example.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop MapReduce/" class="archive-article-date">
  	<time datetime="2016-11-08T08:05:53.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS Command" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS Command/">HDFS Command</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-FS-Command"><a href="#Hadoop-FS-Command" class="headerlink" title="Hadoop FS Command"></a>Hadoop FS Command</h2><p>可以参考： <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
<p>The File System (FS) shell includes various shell-like commands that directly interact with the Hadoop Distributed File System (HDFS) as well as other file systems that Hadoop supports, such as Local FS, HFTP FS, S3 FS, and others. The FS shell is invoked by:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hadoop fs &lt;args&gt;</div></pre></td></tr></table></figure></p>
<p>All FS shell commands take path URIs as arguments. The URI format is<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scheme://authority/path</div></pre></td></tr></table></figure></p>
<p>For <strong>HDFS</strong> the scheme is <strong>hdfs</strong> , and for the <strong>Local FS</strong> the scheme is <strong>file</strong>. The scheme and authority are optional. If not specified, the default scheme specified in the configuration is used. An HDFS file or directory such as /parent/child can be specified as <strong>hdfs://namenodehost/parent/child</strong> or simply as <strong>/parent/child</strong> (given that your configuration is set to point to <strong>hdfs://namenodehost</strong> ).</p>
<p>args 可以为：</p>
<ul>
<li>appendToFile</li>
<li>cat</li>
<li>checksum</li>
<li>chgrp</li>
<li>chmod</li>
<li>chown</li>
<li>copyFromLocal</li>
<li>copyToLocal</li>
<li>count</li>
<li>cp</li>
<li>createSnapshot</li>
<li>deleteSnapshot</li>
<li>df</li>
<li>du</li>
<li>dus</li>
<li>expunge</li>
<li>find</li>
<li>get</li>
<li>getfacl</li>
<li>getfattr</li>
<li>getmerge</li>
<li>help</li>
<li>ls</li>
<li>lsr</li>
<li>mkdir</li>
<li>moveFromLocal</li>
<li>moveToLocal</li>
<li>mv</li>
<li>put</li>
<li>renameSnapshot</li>
<li>rm</li>
<li>rmdir</li>
<li>rmr</li>
<li>setfacl</li>
<li>setfattr</li>
<li>setrep</li>
<li>stat</li>
<li>tail</li>
<li>test</li>
<li>text</li>
<li>touchz</li>
<li>truncate</li>
<li>usage</li>
</ul>
<hr>
<h3 id="Hadoop-FS-命令："><a href="#Hadoop-FS-命令：" class="headerlink" title="Hadoop FS 命令："></a>Hadoop FS 命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hadoop fs :</div><div class="line">          [-ls &lt;path&gt;]</div><div class="line">          [-lsr &lt;path&gt;]</div><div class="line">           [-du &lt;path&gt;]</div><div class="line">           [-dus &lt;path&gt;]</div><div class="line">           [-count[-q] &lt;path&gt;]</div><div class="line">           [-mv &lt;src&gt; &lt;dst&gt;]</div><div class="line">           [-cp &lt;src&gt; &lt;dst&gt;]</div><div class="line">           [-rm [-skipTrash] &lt;path&gt;]</div><div class="line">           [-rmr [-skipTrash] &lt;path&gt;]</div><div class="line">           [-put &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">           [-copyFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">           [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line"></div><div class="line">           [-getmerge &lt;src&gt; &lt;localdst&gt; [addnl]]</div><div class="line">           [-cat &lt;src&gt;]</div><div class="line">           [-text &lt;src&gt;]</div><div class="line">           [-copyToLocal [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]</div><div class="line">           [-moveToLocal [-crc] &lt;src&gt; &lt;localdst&gt;]</div><div class="line">           [-mkdir &lt;path&gt;]</div><div class="line">           [-tail [-f] &lt;file&gt;]</div><div class="line">           [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</div><div class="line">           [-chown [-R] [OWNER][:[GROUP]] PATH...]</div><div class="line">           [-chgrp [-R] GROUP PATH...]</div><div class="line">           [-<span class="built_in">help</span> [cmd]]</div></pre></td></tr></table></figure>
<h3 id="Hadoop-DFS-Admin-命令："><a href="#Hadoop-DFS-Admin-命令：" class="headerlink" title="Hadoop DFS Admin 命令："></a>Hadoop DFS Admin 命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">hadoop dfsadmin :</div><div class="line">           [-report]</div><div class="line">             报告文件系统的基本信息</div><div class="line">           [-safemode enter | leave | get | <span class="built_in">wait</span>]</div><div class="line">           安全模式维护命令</div><div class="line">           [-saveNamespace]</div><div class="line">             保存当前的命名空间</div><div class="line">           [-refreshNodes]</div><div class="line">              重新读取 Hosts 和 eclude 文件，使新的节点或需要退出集群的节点能够重新被 NameNode 识别。</div><div class="line">           [-finalizeUpgrade]</div><div class="line">            终结 HDFS 的升级操作</div><div class="line">           [-upgradeProgress status | details | force]</div><div class="line">          [-metasave filename]</div><div class="line">           保存 Namenode 的主要数据结构到 Hadoop.log.dir 属性指定目录下的 filename 上</div><div class="line">           [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">           为每个目录设定配额，强制限定目录树下的名字个数。</div><div class="line">           [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">          为每个目录清除配额设定。</div><div class="line">           [-setBalancerBandwidth &lt;bandwidth <span class="keyword">in</span> bytes per second&gt;]</div><div class="line">           设定负载均衡时使用的带宽</div></pre></td></tr></table></figure>
<hr>
<p><strong>appendToFile</strong></p>
<p>Usage: hadoop fs -appendToFile <localsrc> … <dst></dst></localsrc></p>
<p>Append single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and appends to destination file system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop fs -appendToFile localfile /user/hadoop/hadoopfile</div><div class="line">hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile</div><div class="line">hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile</div><div class="line">hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and 1 on error.</p>
<p><strong>cat</strong></p>
<p>Usage: hadoop fs -cat URI [URI …]</p>
<p>Copies source paths to stdout.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2</div><div class="line">hadoop fs -cat file:///file3 /user/hadoop/file4</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>checksum</strong></p>
<p>Usage: hadoop fs -checksum URI</p>
<p>Returns the checksum information of a file.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -checksum hdfs://nn1.example.com/file1</div><div class="line">hadoop fs -checksum file:///etc/hosts</div></pre></td></tr></table></figure></p>
<p><strong>chgrp</strong></p>
<p>Usage: hadoop fs -chgrp [-R] GROUP URI [URI …]</p>
<p>Change group association of files. The user must be the owner of files, or else a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>chmod</strong></p>
<p>Usage: hadoop fs -chmod [-R] <mode[,mode]... |="" octalmode=""> URI [URI …]</mode[,mode]...></p>
<p>Change the permissions of files. With -R, make the change recursively through the directory structure. The user must be the owner of the file, or else a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>chown</strong></p>
<p>Usage: hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</p>
<p>Change the owner of files. The user must be a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>copyFromLocal</strong></p>
<p>Usage: hadoop fs -copyFromLocal <localsrc> URI</localsrc></p>
<p>Similar to put command, except that the source is restricted to a local file reference.</p>
<p>Options:</p>
<ul>
<li>The -f option will overwrite the destination if it already exists.</li>
</ul>
<p><strong>copyToLocal</strong></p>
<p>Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst></localdst></p>
<p>Similar to get command, except that the destination is restricted to a local file reference.</p>
<p><strong>count</strong></p>
<p>Usage: hadoop fs -count [-q] [-h] [-v] <paths></paths></p>
<p>Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns with -count are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME</p>
<p>The output columns with -count -q are: QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME</p>
<p>The -h option shows sizes in human readable format.</p>
<p>The -v option displays a header line.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2</div><div class="line">hadoop fs -count -q hdfs://nn1.example.com/file1</div><div class="line">hadoop fs -count -q -h hdfs://nn1.example.com/file1</div><div class="line">hdfs dfs -count -q -h -v hdfs://nn1.example.com/file1</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>cp</strong></p>
<p>Usage: hadoop fs -cp [-f] [-p | -p[topax]] URI [URI …] <dest></dest></p>
<p>Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory.</p>
<p>‘raw.*’ namespace extended attributes are preserved if (1) the source and destination filesystems support them (HDFS only), and (2) all source and destination pathnames are in the /.reserved/raw hierarchy. Determination of whether raw.* namespace xattrs are preserved is independent of the -p (preserve) flag.</p>
<p>Options:</p>
<ul>
<li>The -f option will overwrite the destination if it already exists.</li>
<li>The -p option will preserve file attributes [topx] (timestamps, ownership, permission, ACL, XAttr). If -p is specified with no arg, then preserves timestamps, ownership, permission. If -pa is specified, then preserves permission also because ACL is a super-set of permission. Determination of whether raw namespace extended attributes are preserved is independent of the -p flag.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2</div><div class="line">hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>createSnapshot</strong></p>
<p>See HDFS Snapshots Guide.</p>
<p>deleteSnapshot</p>
<p>See HDFS Snapshots Guide.</p>
<p><strong>df</strong></p>
<p>Usage: hadoop fs -df [-h] URI [URI …]</p>
<p>Displays free space.</p>
<p>Options:</p>
<ul>
<li>The -h option will format file sizes in a “human-readable” fashion (e.g 64.0m instead of 67108864)</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop dfs -df /user/hadoop/dir1</div></pre></td></tr></table></figure></p>
<p><strong>du</strong></p>
<p>Usage: hadoop fs -du [-s] [-h] URI [URI …]</p>
<p>Displays sizes of files and directories contained in the given directory or the length of a file in case its just a file.</p>
<p>Options:</p>
<ul>
<li>The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files.</li>
<li>The -h option will format file sizes in a “human-readable” fashion (e.g 64.0m instead of 67108864)</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1</div></pre></td></tr></table></figure></p>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>dus</strong></p>
<p>Usage: hadoop fs -dus <args></args></p>
<p>Displays a summary of file lengths.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -du -s.</p>
<p><strong>expunge</strong></p>
<p>Usage: hadoop fs -expunge</p>
<p>Empty the Trash. Refer to the HDFS Architecture Guide for more information on the Trash feature.</p>
<p><strong>find</strong></p>
<p>Usage: hadoop fs -find <path></path> … <expression> …</expression></p>
<p>Finds all files that match the specified expression and applies selected actions to them. If no path is specified then defaults to the current working directory. If no expression is specified then defaults to -print.</p>
<p>The following primary expressions are recognised:</p>
<ul>
<li>-name pattern</li>
<li><p>-iname pattern<br>Evaluates as true if the basename of the file matches the pattern using standard file system globbing. If -iname is used then the match is case insensitive.</p>
</li>
<li><p>-print</p>
</li>
<li>-print0Always<br>evaluates to true. Causes the current pathname to be written to standard output. If the -print0 expression is used then an ASCII NULL character is appended.</li>
</ul>
<p>The following operators are recognised:</p>
<ul>
<li>expression -a expression</li>
<li>expression -and expression</li>
<li>expression expression<br>Logical AND operator for joining two expressions. Returns true if both child expressions return true. Implied by the juxtaposition of two expressions and so does not need to be explicitly specified. The second expression will not be applied if the first fails.</li>
</ul>
<p>Example:</p>
<p><strong>hadoop fs -find / -name test -print</strong></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>get</strong></p>
<p>Usage: hadoop fs -get [-ignorecrc] [-crc] <src> <localdst></localdst></src></p>
<p>Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -get /user/hadoop/file localfile</div><div class="line">hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>getfacl</strong></p>
<p>Usage: hadoop fs -getfacl [-R] <path></path></p>
<p>Displays the Access Control Lists (ACLs) of files and directories. If a directory has a default ACL, then getfacl also displays the default ACL.</p>
<p>Options:</p>
<ul>
<li>-R: List the ACLs of all files and directories recursively.</li>
<li>path: File or directory to list.</li>
</ul>
<p>Examples:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -getfacl /file</div><div class="line">hadoop fs -getfacl -R /dir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>getfattr</strong></p>
<p>Usage: hadoop fs -getfattr [-R] -n name | -d [-e en] <path></path></p>
<p>Displays the extended attribute names and values (if any) for a file or directory.</p>
<p>Options:</p>
<ul>
<li>-R: Recursively list the attributes for all files and directories.</li>
<li>-n name: Dump the named extended attribute value.</li>
<li>-d: Dump all extended attribute values associated with pathname.</li>
<li>-e encoding: Encode values after retrieving them. Valid encodings are “text”, “hex”, and “base64”. Values encoded as text strings are enclosed in double quotes (“), and values encoded as hexadecimal and base64 are prefixed with 0x and 0s, respectively.</li>
<li>path: The file or directory.</li>
</ul>
<p>Examples:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -getfattr -d /file</div><div class="line">hadoop fs -getfattr -R -n user.myAttr /dir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>getmerge</strong></p>
<p>Usage: hadoop fs -getmerge [-nl] <src> <localdst></localdst></src></p>
<p>Takes a source directory and a destination file as input and concatenates files in src into the destination local file. Optionally -nl can be set to enable adding a newline character (LF) at the end of each file.</p>
<p>Examples:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -getmerge -nl /src /opt/output.txt</div><div class="line">hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txt</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>help</strong></p>
<p>Usage: hadoop fs -help</p>
<p>Return usage output.</p>
<p><strong>ls</strong></p>
<p>Usage: hadoop fs -ls [-d] [-h] [-R] <args></args></p>
<p>Options:</p>
<ul>
<li>-d: Directories are listed as plain files.</li>
<li>-h: Format file sizes in a human-readable fashion (eg 64.0m instead of 67108864).</li>
<li>-R: Recursively list subdirectories encountered.</li>
</ul>
<p>For a file ls returns stat on the file with the following format:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">permissions number_of_replicas userid groupid filesize modification_date modification_time filename</div></pre></td></tr></table></figure></p>
<p>For a directory it returns list of its direct children as in Unix. A directory is listed as:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">permissions userid groupid modification_date modification_time dirname</div></pre></td></tr></table></figure></p>
<p>Files within a directory are order by filename by default.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -ls /user/hadoop/file1</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>lsr</strong></p>
<p>Usage: hadoop fs -lsr <args></args></p>
<p>Recursive version of ls.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -ls -R</p>
<p><strong>mkdir</strong></p>
<p>Usage: hadoop fs -mkdir [-p] <paths></paths></p>
<p>Takes path uri’s as argument and creates directories.</p>
<p>Options:</p>
<ul>
<li>The -p option behavior is much like Unix mkdir -p, creating parent directories along the path.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2</div><div class="line">hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>moveFromLocal</strong></p>
<p>Usage: hadoop fs -moveFromLocal <localsrc> <dst></dst></localsrc></p>
<p>Similar to put command, except that the source localsrc is deleted after it’s copied.</p>
<p><strong>moveToLocal</strong></p>
<p>Usage: hadoop fs -moveToLocal [-crc] <src> <dst></dst></src></p>
<p>Displays a “Not implemented yet” message.</p>
<p><strong>mv</strong></p>
<p>Usage: hadoop fs -mv URI [URI …] <dest></dest></p>
<p>Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across file systems is not permitted.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</div><div class="line">hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>put</strong></p>
<p>Usage: hadoop fs -put <localsrc> … <dst></dst></localsrc></p>
<p>Copy single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and writes to destination file system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop fs -put localfile /user/hadoop/hadoopfile</div><div class="line">hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</div><div class="line">hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile</div><div class="line">hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>renameSnapshot</strong></p>
<p>See HDFS Snapshots Guide.</p>
<p><strong>rm</strong></p>
<p>Usage: hadoop fs -rm [-f] [-r |-R] [-skipTrash] URI [URI …]</p>
<p>Delete files specified as args.</p>
<p>Options:</p>
<ul>
<li>The -f option will not display a diagnostic message or modify the exit status to reflect an error if the file does not exist.</li>
<li>The -R option deletes the directory and any content under it recursively.</li>
<li>The -r option is equivalent to -R.</li>
<li>The -skipTrash option will bypass trash, if enabled, and delete the specified file(s) immediately. This can be useful when it is necessary to delete files from an over-quota directory.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>rmdir</strong></p>
<p>Usage: hadoop fs -rmdir [–ignore-fail-on-non-empty] URI [URI …]</p>
<p>Delete a directory.</p>
<p>Options:</p>
<ul>
<li>–ignore-fail-on-non-empty: When using wildcards, do not fail if a directory still contains files.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -rmdir /user/hadoop/emptydir</div></pre></td></tr></table></figure></p>
<p><strong>rmr</strong></p>
<p>Usage: hadoop fs -rmr [-skipTrash] URI [URI …]</p>
<p>Recursive version of delete.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -rm -r</p>
<p><strong>setfacl</strong></p>
<p>Usage: hadoop fs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path></path>] |[–set <acl_spec> <path></path>]</acl_spec></acl_spec></p>
<p>Sets Access Control Lists (ACLs) of files and directories.</p>
<p>Options:</p>
<ul>
<li>-b: Remove all but the base ACL entries. The entries for user, group and others are retained for compatibility with permission bits.</li>
<li>-k: Remove the default ACL.</li>
<li>-R: Apply operations to all files and directories recursively.</li>
<li>-m: Modify ACL. New entries are added to the ACL, and existing entries are retained.</li>
<li>-x: Remove specified ACL entries. Other ACL entries are retained.</li>
<li>–set: Fully replace the ACL, discarding all existing entries. The acl_spec must include entries for user, group, and others for compatibility with permission bits.</li>
<li>acl_spec: Comma separated list of ACL entries.</li>
<li>path: File or directory to modify.</li>
</ul>
<p>Examples:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop fs -setfacl -m user:hadoop:rw- /file</div><div class="line">hadoop fs -setfacl -x user:hadoop /file</div><div class="line">hadoop fs -setfacl -b /file</div><div class="line">hadoop fs -setfacl -k /dir</div><div class="line">hadoop fs -setfacl --<span class="built_in">set</span> user::rw-,user:hadoop:rw-,group::r--,other::r-- /file</div><div class="line">hadoop fs -setfacl -R -m user:hadoop:r-x /dir</div><div class="line">hadoop fs -setfacl -m default:user:hadoop:r-x /dir</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>setfattr</strong></p>
<p>Usage: hadoop fs -setfattr -n name [-v value] | -x name <path></path></p>
<p>Sets an extended attribute name and value for a file or directory.</p>
<p>Options:</p>
<ul>
<li>-b: Remove all but the base ACL entries. The entries for user, group and others are retained for compatibility with permission bits.</li>
<li>-n name: The extended attribute name.</li>
<li>-v value: The extended attribute value. There are three different encoding methods for the value. If the argument is enclosed in double quotes, then the value is the string inside the quotes. If the argument is prefixed with 0x or 0X, then it is taken as a hexadecimal number. If the argument begins with 0s or 0S, then it is taken as a base64 encoding.</li>
<li>-x name: Remove the extended attribute.</li>
<li>path: The file or directory.</li>
</ul>
<p>Examples:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop fs -setfattr -n user.myAttr -v myValue /file</div><div class="line">hadoop fs -setfattr -n user.noValue /file</div><div class="line">hadoop fs -setfattr -x user.myAttr /file</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>setrep</strong></p>
<p>Usage: hadoop fs -setrep [-R] [-w] <numreplicas> <path></path></numreplicas></p>
<p>Changes the replication factor of a file. If path is a directory then the command recursively changes the replication factor of all files under the directory tree rooted at path.</p>
<p>Options:</p>
<ul>
<li>The -w flag requests that the command wait for the replication to complete. This can potentially take a very long time.</li>
<li>The -R flag is accepted for backwards compatibility. It has no effect.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -setrep -w 3 /user/hadoop/dir1</div></pre></td></tr></table></figure></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>stat</strong></p>
<p>Usage: hadoop fs -stat [format] <path></path> …</p>
<p>Print statistics about the file/directory at <path></path> in the specified format. Format accepts filesize in blocks (%b), type (%F), group name of owner (%g), name (%n), block size (%o), replication (%r), user name of owner(%u), and modification date (%y, %Y). %y shows UTC date as “yyyy-MM-dd HH:mm:ss” and %Y shows milliseconds since January 1, 1970 UTC. If the format is not specified, %y is used by default.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -stat “%F %u:%g %b %y %n” /file</li>
</ul>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>tail</strong></p>
<p>Usage: hadoop fs -tail [-f] URI</p>
<p>Displays last kilobyte of the file to stdout.</p>
<p>Options:</p>
<ul>
<li>The -f option will output appended data as the file grows, as in Unix.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -tail pathname</div></pre></td></tr></table></figure></p>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>test</strong></p>
<p>Usage: hadoop fs -test -[defsz] URI</p>
<p>Options:</p>
<ul>
<li>-d: f the path is a directory, return 0.</li>
<li>-e: if the path exists, return 0.</li>
<li>-f: if the path is a file, return 0.</li>
<li>-s: if the path is not empty, return 0.</li>
<li>-z: if the file is zero length, return 0.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -<span class="built_in">test</span> -e filename</div></pre></td></tr></table></figure></p>
<p><strong>text</strong></p>
<p>Usage: hadoop fs -text <src></src></p>
<p>Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.</p>
<p><strong>touchz</strong></p>
<p>Usage: hadoop fs -touchz URI [URI …]</p>
<p>Create a file of zero length.</p>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -touchz pathname</div></pre></td></tr></table></figure></p>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>truncate</strong></p>
<p>Usage: hadoop fs -truncate [-w] <length> <paths></paths></length></p>
<p>Truncate all files that match the specified file pattern to the specified length.</p>
<p>Options:</p>
<ul>
<li>The -w flag requests that the command waits for block recovery to complete, if necessary. Without -w flag the file may remain unclosed for some time while the recovery is in progress. During this time file cannot be reopened for append.</li>
</ul>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2</div><div class="line">hadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1</div></pre></td></tr></table></figure></p>
<p><strong>usage</strong></p>
<p>Usage: hadoop fs -usage command</p>
<p>Return the help for an individual command.</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS Command/" class="archive-article-date">
  	<time datetime="2016-11-08T05:41:32.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS介绍/">HDFS介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS 分布式文件系统"></a>HDFS 分布式文件系统</h2><ul>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
</ul>
<h2 id="Hadoop-HDFS-1-x-vs-2-x"><a href="#Hadoop-HDFS-1-x-vs-2-x" class="headerlink" title="Hadoop HDFS 1.x vs 2.x"></a>Hadoop HDFS 1.x vs 2.x</h2><p><img src="/images/Hadoop_1_x_vs_2_x.png" alt=""><br>Hadoop HDFS有2个版本，1.x和2.x，两者之间还有比较大的差距：</p>
<ol>
<li>1.x只支持MapReduce运算. 2.x除了MapReduce之外，还支持Spark，MPI，Hama，Giraph等多种类型运算。</li>
<li>1.x中的MR组件既分发任务还需要进行资源调度。 2.x由另一个组件YARN来进行资源调度，MR只进行任务分发。</li>
<li>1.x无法实现HA，只有一个NameNode管理所有节点。 2.x支持HA，有SecondNameNode，但也只支持冷迁移。2.x还支持了Federation联邦，通过文件系统挂载点，将一个NameNode的负载分发下去。注意：HA和Federation可以结合使用。</li>
</ol>
<p>网上资料中包含了10点差异，但是从技术／架构角度上来说，这3点变化是核心的。</p>
<h2 id="Hadoop-HDFS-2-x-架构图"><a href="#Hadoop-HDFS-2-x-架构图" class="headerlink" title="Hadoop HDFS 2.x 架构图"></a>Hadoop HDFS 2.x 架构图</h2><p><img src="/images/Hadoop_HDFS_2_x.png" alt=""></p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>HDFS分布式文件系统中的管理者，负责管理文件系统的命名空间，集群配置信息，存储的复制。</p>
<h4 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h4><p>并非NameNode的热备份，辅助NameNode，定期合并FSimage和EditLog。在NameNode失效的情况下，可以依据Secondary NameNode本地存储的FSimage和EditLog，恢复自身作为NameNode，但可能会有部分文件丢失，原因在于Secondary NameNode上的FSimage和EditLog并不是实时更新的。</p>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode是文件存储的节点，用于存放文件的Block，并且周期性的将Block信息发送给NameNode。值得提一句的是：HDFS中的Block大小设置很有讲究，通常为64M／128M，过小的Block会给NameNode带来巨大的管理压力，过大的Block可能会导致磁盘空间的浪费。</p>
<h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><p>与NameNode交互，获取文件存放位置；再与DataNode交互，读取或写入数据；并且可以管理整个HDFS文件系统。</p>
<h4 id="文件写入过程"><a href="#文件写入过程" class="headerlink" title="文件写入过程"></a>文件写入过程</h4><ol>
<li>Client向NameNode发起文件写入请求。</li>
<li>NameNode根据文件大小和文件块配置情况，返回给Client它所管理的DataNode信息。</li>
<li>Client将文件划分为多个Block，更具DataNode的地址信息，按顺序写入到每一个DataNode块中。<h4 id="文件读取过程"><a href="#文件读取过程" class="headerlink" title="文件读取过程"></a>文件读取过程</h4></li>
<li>Client向NameNode发起文件读取请求。</li>
<li>NameNode返回文件存储的DataNode信息。</li>
<li>Client读取文件信息。</li>
</ol>
<h2 id="HDFS-优缺点"><a href="#HDFS-优缺点" class="headerlink" title="HDFS 优缺点"></a>HDFS 优缺点</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>处理超大文件</li>
<li>流式访问数据，一次写入，多次访问</li>
<li>运行于廉价的商用机器上</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>不适合低延迟的数据访问</li>
<li>无法高效存储大量的小文件</li>
<li>不支持多用户写入及任意修改文件</li>
</ol>
<h2 id="Hadoop-HDFS-2-x-安装-关于安装-请参考博客中的另一篇文章"><a href="#Hadoop-HDFS-2-x-安装-关于安装-请参考博客中的另一篇文章" class="headerlink" title="Hadoop HDFS 2.x 安装, 关于安装, 请参考博客中的另一篇文章"></a>Hadoop HDFS 2.x 安装, 关于安装, 请参考博客中的另一篇文章</h2><p>Hadoop HDFS 2.x 包含了3种安装模式：</p>
<ol>
<li>Standalone. 独立模式</li>
<li>Pseudo-Distributed Operation. 伪分布式</li>
<li>Cluster. 集群</li>
</ol>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS介绍/" class="archive-article-date">
  	<time datetime="2016-11-07T12:45:12.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-personal-profile" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/personal-profile/">介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>欢迎浏览 <a href="https://sstar1314.github.io/" target="_blank" rel="external">SStar1314</a>! 我的个人博客. 以及 <a href="https://github.com/SStar1314/" target="_blank" rel="external">SStar1314</a>，我的github主页. 我的主页开通于2016年11月，一直想写点什么，两年多的时间，在Evernote上记录很多自己研究的东西，一直想开通一个博客，把Evernote上的东西搬上来，顺道梳理一下以前的知识点.</p>
<h3 id="个人关心的方向或技术"><a href="#个人关心的方向或技术" class="headerlink" title="个人关心的方向或技术"></a>个人关心的方向或技术</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">OpenStack云计算框架: Keystone／Nova／Neutron／Glance／Cinder／Swift／Ironic</div><div class="line">	Keystone: RabbitMQ,Qpid,AMQP(Producer+Consumer+Exchange+Queue)</div><div class="line">	Nova: nova-scheduler,nova-compute,虚拟化</div><div class="line">	Neutron: OVS,Linux Bridge,vlan,vxlan,gre.</div><div class="line">		Provider network ---- Self-defined network</div><div class="line">虚拟化: KVM／Xen</div><div class="line">大数据: Headoop／Spark</div><div class="line">	Headoop: MapReduce,HDFS,Yarn,Hbase,Hive,Storm. Hadoop1.x vs 2.x</div><div class="line">	Spark: RDD,Spark-shell,Spark SQL,Spark Streaming,MLlib,GraphX</div><div class="line">NoSQL: HBase, Cassandra, MongoDB, Redis</div><div class="line">容器化: Docker, Kubernetes, rkt, CoreOS</div><div class="line">数据中心操作系统: DCOS, Mesos, Marathon, Chronos</div><div class="line">分布式协同: ZooKeeper, etcd</div><div class="line">其它: Kafka, Elastic Search, Logstash</div></pre></td></tr></table></figure>
<p>More info: <a href="https://github.com/SStar1314/" target="_blank" rel="external">我的github主页</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/personal-profile/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/介绍/">介绍</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/介绍/">介绍</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-大数据" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/大数据/">大数据相关目录</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>推荐一本书<a href="http://item.jd.com/11966465.html" target="_blank" rel="external">《云计算架构 技术与实践 第2版》</a>，华为 顾炯炯 编著，一定要是第2版，这本书很全面，很赞！</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>列表：</p>
<ol>
<li>Hadoop<br>MapReduce, HDFS, Yarn, Hbase, Hive, Storm.<br>Hadoop1.x vs 2.x</li>
<li>Spark<br> RDD, Spark-shell, Spark SQL, Spark Streaming, MLlib, GraphX</li>
<li>NoSQL<br>HBase, Cassandra, MongoDB, Redis</li>
</ol>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/大数据/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Xia, MingXing
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: false
	}
</script>

<script src="/./main.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cassandra/" style="font-size: 10px;">Cassandra</a> <a href="/tags/Hadoop/" style="font-size: 20px;">Hadoop</a> <a href="/tags/Kafka/" style="font-size: 12px;">Kafka</a> <a href="/tags/Linux/" style="font-size: 14px;">Linux</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/Spark/" style="font-size: 16px;">Spark</a> <a href="/tags/ZooKeeper/" style="font-size: 12px;">ZooKeeper</a> <a href="/tags/etcd/" style="font-size: 10px;">etcd</a> <a href="/tags/介绍/" style="font-size: 10px;">介绍</a> <a href="/tags/分布式协同/" style="font-size: 10px;">分布式协同</a> <a href="/tags/大数据/" style="font-size: 18px;">大数据</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">平均每天保证几小时的学习时间，用一万小时定律激励自己</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>