<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>SStar1314</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SStar1314">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="SStar1314">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SStar1314">
  
    <link rel="alternative" href="/atom.xml" title="SStar1314" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/main.css">
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/favicon.ico" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Xia,MingXing</a></h1>
		</hgroup>

		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">所有文章</a></li>
	        
				<li><a href="/tags/随笔">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="#" title="rss">rss</a>
		        
					<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Xia,MingXing</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/favicon.ico" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Xia,MingXing</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/tags/随笔">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-Hadoop MapReduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop MapReduce/">Hadoop MapReduce</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-MapReduce"><a href="#Hadoop-MapReduce" class="headerlink" title="Hadoop MapReduce"></a>Hadoop MapReduce</h2><p><img src="/images/Hadoop_MapReduce.png" alt=""></p>
<p><img src="/images/Hadoop_MapReduce_1.png" alt=""></p>
<p>可以参考： <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop MapReduce/" class="archive-article-date">
  	<time datetime="2016-11-08T08:05:53.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop HDFS command" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop HDFS command/">Hadoop HDFS FS Command</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-FS-Command"><a href="#Hadoop-FS-Command" class="headerlink" title="Hadoop FS Command"></a>Hadoop FS Command</h2><p>可以参考： <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
<p>The File System (FS) shell includes various shell-like commands that directly interact with the Hadoop Distributed File System (HDFS) as well as other file systems that Hadoop supports, such as Local FS, HFTP FS, S3 FS, and others. The FS shell is invoked by:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hadoop fs &lt;args&gt;</div></pre></td></tr></table></figure></p>
<p>All FS shell commands take path URIs as arguments. The URI format is<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scheme://authority/path</div></pre></td></tr></table></figure></p>
<p>For <strong>HDFS</strong> the scheme is <strong>hdfs</strong> , and for the <strong>Local FS</strong> the scheme is <strong>file</strong>. The scheme and authority are optional. If not specified, the default scheme specified in the configuration is used. An HDFS file or directory such as /parent/child can be specified as <strong>hdfs://namenodehost/parent/child</strong> or simply as <strong>/parent/child</strong> (given that your configuration is set to point to <strong>hdfs://namenodehost</strong> ).</p>
<p>args 可以为：</p>
<ul>
<li>appendToFile</li>
<li>cat</li>
<li>checksum</li>
<li>chgrp</li>
<li>chmod</li>
<li>chown</li>
<li>copyFromLocal</li>
<li>copyToLocal</li>
<li>count</li>
<li>cp</li>
<li>createSnapshot</li>
<li>deleteSnapshot</li>
<li>df</li>
<li>du</li>
<li>dus</li>
<li>expunge</li>
<li>find</li>
<li>get</li>
<li>getfacl</li>
<li>getfattr</li>
<li>getmerge</li>
<li>help</li>
<li>ls</li>
<li>lsr</li>
<li>mkdir</li>
<li>moveFromLocal</li>
<li>moveToLocal</li>
<li>mv</li>
<li>put</li>
<li>renameSnapshot</li>
<li>rm</li>
<li>rmdir</li>
<li>rmr</li>
<li>setfacl</li>
<li>setfattr</li>
<li>setrep</li>
<li>stat</li>
<li>tail</li>
<li>test</li>
<li>text</li>
<li>touchz</li>
<li>truncate</li>
<li>usage</li>
</ul>
<hr>
<h3 id="Hadoop-FS-命令："><a href="#Hadoop-FS-命令：" class="headerlink" title="Hadoop FS 命令："></a>Hadoop FS 命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hadoop fs :</div><div class="line">          [-ls &lt;path&gt;]</div><div class="line">          [-lsr &lt;path&gt;]</div><div class="line">           [-du &lt;path&gt;]</div><div class="line">           [-dus &lt;path&gt;]</div><div class="line">           [-count[-q] &lt;path&gt;]</div><div class="line">           [-mv &lt;src&gt; &lt;dst&gt;]</div><div class="line">           [-cp &lt;src&gt; &lt;dst&gt;]</div><div class="line">           [-rm [-skipTrash] &lt;path&gt;]</div><div class="line">           [-rmr [-skipTrash] &lt;path&gt;]</div><div class="line">           [-put &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">           [-copyFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">           [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line"></div><div class="line">           [-getmerge &lt;src&gt; &lt;localdst&gt; [addnl]]</div><div class="line">           [-cat &lt;src&gt;]</div><div class="line">           [-text &lt;src&gt;]</div><div class="line">           [-copyToLocal [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]</div><div class="line">           [-moveToLocal [-crc] &lt;src&gt; &lt;localdst&gt;]</div><div class="line">           [-mkdir &lt;path&gt;]</div><div class="line">           [-tail [-f] &lt;file&gt;]</div><div class="line">           [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</div><div class="line">           [-chown [-R] [OWNER][:[GROUP]] PATH...]</div><div class="line">           [-chgrp [-R] GROUP PATH...]</div><div class="line">           [-help [cmd]]</div></pre></td></tr></table></figure>
<h3 id="Hadoop-DFS-Admin-命令："><a href="#Hadoop-DFS-Admin-命令：" class="headerlink" title="Hadoop DFS Admin 命令："></a>Hadoop DFS Admin 命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">hadoop dfsadmin :</div><div class="line">           [-report]</div><div class="line">             报告文件系统的基本信息</div><div class="line">           [-safemode enter | leave | get | wait]</div><div class="line">           安全模式维护命令</div><div class="line">           [-saveNamespace]</div><div class="line">             保存当前的命名空间</div><div class="line">           [-refreshNodes]</div><div class="line">              重新读取 Hosts 和 eclude 文件，使新的节点或需要退出集群的节点能够重新被 NameNode 识别。</div><div class="line">           [-finalizeUpgrade]</div><div class="line">            终结 HDFS 的升级操作</div><div class="line">           [-upgradeProgress status | details | force]</div><div class="line">          [-metasave filename]</div><div class="line">           保存 Namenode 的主要数据结构到 Hadoop.log.dir 属性指定目录下的 filename 上</div><div class="line">           [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">           为每个目录设定配额，强制限定目录树下的名字个数。</div><div class="line">           [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">          为每个目录清除配额设定。</div><div class="line">           [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]</div><div class="line">           设定负载均衡时使用的带宽</div></pre></td></tr></table></figure>
<hr>
<p><strong>appendToFile</strong></p>
<p>Usage: hadoop fs -appendToFile <localsrc> … <dst></dst></localsrc></p>
<p>Append single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and appends to destination file system.</p>
<ul>
<li>hadoop fs -appendToFile localfile /user/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and 1 on error.</p>
<p><strong>cat</strong></p>
<p>Usage: hadoop fs -cat URI [URI …]</p>
<p>Copies source paths to stdout.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2</li>
<li>hadoop fs -cat file:///file3 /user/hadoop/file4</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>checksum</strong></p>
<p>Usage: hadoop fs -checksum URI</p>
<p>Returns the checksum information of a file.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -checksum hdfs://nn1.example.com/file1</li>
<li>hadoop fs -checksum file:///etc/hosts</li>
</ul>
<p><strong>chgrp</strong></p>
<p>Usage: hadoop fs -chgrp [-R] GROUP URI [URI …]</p>
<p>Change group association of files. The user must be the owner of files, or else a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>chmod</strong></p>
<p>Usage: hadoop fs -chmod [-R] <mode[,mode]... |="" octalmode=""> URI [URI …]</mode[,mode]...></p>
<p>Change the permissions of files. With -R, make the change recursively through the directory structure. The user must be the owner of the file, or else a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>chown</strong></p>
<p>Usage: hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</p>
<p>Change the owner of files. The user must be a super-user. Additional information is in the Permissions Guide.</p>
<p>Options</p>
<ul>
<li>The -R option will make the change recursively through the directory structure.</li>
</ul>
<p><strong>copyFromLocal</strong></p>
<p>Usage: hadoop fs -copyFromLocal <localsrc> URI</localsrc></p>
<p>Similar to put command, except that the source is restricted to a local file reference.</p>
<p>Options:</p>
<ul>
<li>The -f option will overwrite the destination if it already exists.</li>
</ul>
<p><strong>copyToLocal</strong></p>
<p>Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst></localdst></p>
<p>Similar to get command, except that the destination is restricted to a local file reference.</p>
<p><strong>count</strong></p>
<p>Usage: hadoop fs -count [-q] [-h] [-v] <paths></paths></p>
<p>Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns with -count are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME</p>
<p>The output columns with -count -q are: QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME</p>
<p>The -h option shows sizes in human readable format.</p>
<p>The -v option displays a header line.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2</li>
<li>hadoop fs -count -q hdfs://nn1.example.com/file1</li>
<li>hadoop fs -count -q -h hdfs://nn1.example.com/file1</li>
<li>hdfs dfs -count -q -h -v hdfs://nn1.example.com/file1</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>cp</strong></p>
<p>Usage: hadoop fs -cp [-f] [-p | -p[topax]] URI [URI …] <dest></dest></p>
<p>Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory.</p>
<p>‘raw.*’ namespace extended attributes are preserved if (1) the source and destination filesystems support them (HDFS only), and (2) all source and destination pathnames are in the /.reserved/raw hierarchy. Determination of whether raw.* namespace xattrs are preserved is independent of the -p (preserve) flag.</p>
<p>Options:</p>
<ul>
<li>The -f option will overwrite the destination if it already exists.</li>
<li>The -p option will preserve file attributes [topx] (timestamps, ownership, permission, ACL, XAttr). If -p is specified with no arg, then preserves timestamps, ownership, permission. If -pa is specified, then preserves permission also because ACL is a super-set of permission. Determination of whether raw namespace extended attributes are preserved is independent of the -p flag.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2</li>
<li>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>createSnapshot</strong></p>
<p>See HDFS Snapshots Guide.</p>
<p>deleteSnapshot</p>
<p>See HDFS Snapshots Guide.</p>
<p><strong>df</strong></p>
<p>Usage: hadoop fs -df [-h] URI [URI …]</p>
<p>Displays free space.</p>
<p>Options:</p>
<ul>
<li>The -h option will format file sizes in a “human-readable” fashion (e.g 64.0m instead of 67108864)</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop dfs -df /user/hadoop/dir1</li>
</ul>
<p><strong>du</strong></p>
<p>Usage: hadoop fs -du [-s] [-h] URI [URI …]</p>
<p>Displays sizes of files and directories contained in the given directory or the length of a file in case its just a file.</p>
<p>Options:</p>
<ul>
<li>The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files.</li>
<li>The -h option will format file sizes in a “human-readable” fashion (e.g 64.0m instead of 67108864)</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1</li>
</ul>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>dus</strong></p>
<p>Usage: hadoop fs -dus <args></args></p>
<p>Displays a summary of file lengths.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -du -s.</p>
<p><strong>expunge</strong></p>
<p>Usage: hadoop fs -expunge</p>
<p>Empty the Trash. Refer to the HDFS Architecture Guide for more information on the Trash feature.</p>
<p><strong>find</strong></p>
<p>Usage: hadoop fs -find <path></path> … <expression> …</expression></p>
<p>Finds all files that match the specified expression and applies selected actions to them. If no path is specified then defaults to the current working directory. If no expression is specified then defaults to -print.</p>
<p>The following primary expressions are recognised:</p>
<ul>
<li>-name pattern</li>
<li><p>-iname pattern<br>Evaluates as true if the basename of the file matches the pattern using standard file system globbing. If -iname is used then the match is case insensitive.</p>
</li>
<li><p>-print</p>
</li>
<li>-print0Always<br>evaluates to true. Causes the current pathname to be written to standard output. If the -print0 expression is used then an ASCII NULL character is appended.</li>
</ul>
<p>The following operators are recognised:</p>
<ul>
<li>expression -a expression</li>
<li>expression -and expression</li>
<li>expression expression<br>Logical AND operator for joining two expressions. Returns true if both child expressions return true. Implied by the juxtaposition of two expressions and so does not need to be explicitly specified. The second expression will not be applied if the first fails.</li>
</ul>
<p>Example:</p>
<p><strong>hadoop fs -find / -name test -print</strong></p>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>get</strong></p>
<p>Usage: hadoop fs -get [-ignorecrc] [-crc] <src> <localdst></localdst></src></p>
<p>Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -get /user/hadoop/file localfile</li>
<li>hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>getfacl</strong></p>
<p>Usage: hadoop fs -getfacl [-R] <path></path></p>
<p>Displays the Access Control Lists (ACLs) of files and directories. If a directory has a default ACL, then getfacl also displays the default ACL.</p>
<p>Options:</p>
<ul>
<li>-R: List the ACLs of all files and directories recursively.</li>
<li>path: File or directory to list.</li>
</ul>
<p>Examples:</p>
<ul>
<li>hadoop fs -getfacl /file</li>
<li>hadoop fs -getfacl -R /dir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>getfattr</strong></p>
<p>Usage: hadoop fs -getfattr [-R] -n name | -d [-e en] <path></path></p>
<p>Displays the extended attribute names and values (if any) for a file or directory.</p>
<p>Options:</p>
<ul>
<li>-R: Recursively list the attributes for all files and directories.</li>
<li>-n name: Dump the named extended attribute value.</li>
<li>-d: Dump all extended attribute values associated with pathname.</li>
<li>-e encoding: Encode values after retrieving them. Valid encodings are “text”, “hex”, and “base64”. Values encoded as text strings are enclosed in double quotes (“), and values encoded as hexadecimal and base64 are prefixed with 0x and 0s, respectively.</li>
<li>path: The file or directory.</li>
</ul>
<p>Examples:</p>
<ul>
<li>hadoop fs -getfattr -d /file</li>
<li>hadoop fs -getfattr -R -n user.myAttr /dir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>getmerge</strong></p>
<p>Usage: hadoop fs -getmerge [-nl] <src> <localdst></localdst></src></p>
<p>Takes a source directory and a destination file as input and concatenates files in src into the destination local file. Optionally -nl can be set to enable adding a newline character (LF) at the end of each file.</p>
<p>Examples:</p>
<ul>
<li>hadoop fs -getmerge -nl /src /opt/output.txt</li>
<li>hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txt</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>help</strong></p>
<p>Usage: hadoop fs -help</p>
<p>Return usage output.</p>
<p><strong>ls</strong></p>
<p>Usage: hadoop fs -ls [-d] [-h] [-R] <args></args></p>
<p>Options:</p>
<ul>
<li>-d: Directories are listed as plain files.</li>
<li>-h: Format file sizes in a human-readable fashion (eg 64.0m instead of 67108864).</li>
<li>-R: Recursively list subdirectories encountered.</li>
</ul>
<p>For a file ls returns stat on the file with the following format:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">permissions number_of_replicas userid groupid filesize modification_date modification_time filename</div></pre></td></tr></table></figure></p>
<p>For a directory it returns list of its direct children as in Unix. A directory is listed as:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">permissions userid groupid modification_date modification_time dirname</div></pre></td></tr></table></figure></p>
<p>Files within a directory are order by filename by default.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -ls /user/hadoop/file1</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>lsr</strong></p>
<p>Usage: hadoop fs -lsr <args></args></p>
<p>Recursive version of ls.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -ls -R</p>
<p><strong>mkdir</strong></p>
<p>Usage: hadoop fs -mkdir [-p] <paths></paths></p>
<p>Takes path uri’s as argument and creates directories.</p>
<p>Options:</p>
<ul>
<li>The -p option behavior is much like Unix mkdir -p, creating parent directories along the path.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2</li>
<li>hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>moveFromLocal</strong></p>
<p>Usage: hadoop fs -moveFromLocal <localsrc> <dst></dst></localsrc></p>
<p>Similar to put command, except that the source localsrc is deleted after it’s copied.</p>
<p><strong>moveToLocal</strong></p>
<p>Usage: hadoop fs -moveToLocal [-crc] <src> <dst></dst></src></p>
<p>Displays a “Not implemented yet” message.</p>
<p><strong>mv</strong></p>
<p>Usage: hadoop fs -mv URI [URI …] <dest></dest></p>
<p>Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across file systems is not permitted.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</li>
<li>hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>put</strong></p>
<p>Usage: hadoop fs -put <localsrc> … <dst></dst></localsrc></p>
<p>Copy single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and writes to destination file system.</p>
<ul>
<li>hadoop fs -put localfile /user/hadoop/hadoopfile</li>
<li>hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</li>
<li>hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile</li>
<li>hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>renameSnapshot</strong></p>
<p>See HDFS Snapshots Guide.</p>
<p><strong>rm</strong></p>
<p>Usage: hadoop fs -rm [-f] [-r |-R] [-skipTrash] URI [URI …]</p>
<p>Delete files specified as args.</p>
<p>Options:</p>
<ul>
<li>The -f option will not display a diagnostic message or modify the exit status to reflect an error if the file does not exist.</li>
<li>The -R option deletes the directory and any content under it recursively.</li>
<li>The -r option is equivalent to -R.</li>
<li>The -skipTrash option will bypass trash, if enabled, and delete the specified file(s) immediately. This can be useful when it is necessary to delete files from an over-quota directory.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>rmdir</strong></p>
<p>Usage: hadoop fs -rmdir [–ignore-fail-on-non-empty] URI [URI …]</p>
<p>Delete a directory.</p>
<p>Options:</p>
<ul>
<li>–ignore-fail-on-non-empty: When using wildcards, do not fail if a directory still contains files.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -rmdir /user/hadoop/emptydir</li>
</ul>
<p><strong>rmr</strong></p>
<p>Usage: hadoop fs -rmr [-skipTrash] URI [URI …]</p>
<p>Recursive version of delete.</p>
<p>Note: This command is deprecated. Instead use hadoop fs -rm -r</p>
<p><strong>setfacl</strong></p>
<p>Usage: hadoop fs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path></path>] |[–set <acl_spec> <path></path>]</acl_spec></acl_spec></p>
<p>Sets Access Control Lists (ACLs) of files and directories.</p>
<p>Options:</p>
<ul>
<li>-b: Remove all but the base ACL entries. The entries for user, group and others are retained for compatibility with permission bits.</li>
<li>-k: Remove the default ACL.</li>
<li>-R: Apply operations to all files and directories recursively.</li>
<li>-m: Modify ACL. New entries are added to the ACL, and existing entries are retained.</li>
<li>-x: Remove specified ACL entries. Other ACL entries are retained.</li>
<li>–set: Fully replace the ACL, discarding all existing entries. The acl_spec must include entries for user, group, and others for compatibility with permission bits.</li>
<li>acl_spec: Comma separated list of ACL entries.</li>
<li>path: File or directory to modify.</li>
</ul>
<p>Examples:</p>
<ul>
<li>hadoop fs -setfacl -m user:hadoop:rw- /file</li>
<li>hadoop fs -setfacl -x user:hadoop /file</li>
<li>hadoop fs -setfacl -b /file</li>
<li>hadoop fs -setfacl -k /dir</li>
<li>hadoop fs -setfacl –set user::rw-,user:hadoop:rw-,group::r–,other::r– /file</li>
<li>hadoop fs -setfacl -R -m user:hadoop:r-x /dir</li>
<li>hadoop fs -setfacl -m default:user:hadoop:r-x /dir</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>setfattr</strong></p>
<p>Usage: hadoop fs -setfattr -n name [-v value] | -x name <path></path></p>
<p>Sets an extended attribute name and value for a file or directory.</p>
<p>Options:</p>
<ul>
<li>-b: Remove all but the base ACL entries. The entries for user, group and others are retained for compatibility with permission bits.</li>
<li>-n name: The extended attribute name.</li>
<li>-v value: The extended attribute value. There are three different encoding methods for the value. If the argument is enclosed in double quotes, then the value is the string inside the quotes. If the argument is prefixed with 0x or 0X, then it is taken as a hexadecimal number. If the argument begins with 0s or 0S, then it is taken as a base64 encoding.</li>
<li>-x name: Remove the extended attribute.</li>
<li>path: The file or directory.</li>
</ul>
<p>Examples:</p>
<ul>
<li>hadoop fs -setfattr -n user.myAttr -v myValue /file</li>
<li>hadoop fs -setfattr -n user.noValue /file</li>
<li>hadoop fs -setfattr -x user.myAttr /file</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and non-zero on error.</p>
<p><strong>setrep</strong></p>
<p>Usage: hadoop fs -setrep [-R] [-w] <numreplicas> <path></path></numreplicas></p>
<p>Changes the replication factor of a file. If path is a directory then the command recursively changes the replication factor of all files under the directory tree rooted at path.</p>
<p>Options:</p>
<ul>
<li>The -w flag requests that the command wait for the replication to complete. This can potentially take a very long time.</li>
<li>The -R flag is accepted for backwards compatibility. It has no effect.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -setrep -w 3 /user/hadoop/dir1</li>
</ul>
<p>Exit Code:</p>
<p>Returns 0 on success and -1 on error.</p>
<p><strong>stat</strong></p>
<p>Usage: hadoop fs -stat [format] <path></path> …</p>
<p>Print statistics about the file/directory at <path></path> in the specified format. Format accepts filesize in blocks (%b), type (%F), group name of owner (%g), name (%n), block size (%o), replication (%r), user name of owner(%u), and modification date (%y, %Y). %y shows UTC date as “yyyy-MM-dd HH:mm:ss” and %Y shows milliseconds since January 1, 1970 UTC. If the format is not specified, %y is used by default.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -stat “%F %u:%g %b %y %n” /file</li>
</ul>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>tail</strong></p>
<p>Usage: hadoop fs -tail [-f] URI</p>
<p>Displays last kilobyte of the file to stdout.</p>
<p>Options:</p>
<ul>
<li>The -f option will output appended data as the file grows, as in Unix.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -tail pathname</li>
</ul>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>test</strong></p>
<p>Usage: hadoop fs -test -[defsz] URI</p>
<p>Options:</p>
<ul>
<li>-d: f the path is a directory, return 0.</li>
<li>-e: if the path exists, return 0.</li>
<li>-f: if the path is a file, return 0.</li>
<li>-s: if the path is not empty, return 0.</li>
<li>-z: if the file is zero length, return 0.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -test -e filename</li>
</ul>
<p><strong>text</strong></p>
<p>Usage: hadoop fs -text <src></src></p>
<p>Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.</p>
<p><strong>touchz</strong></p>
<p>Usage: hadoop fs -touchz URI [URI …]</p>
<p>Create a file of zero length.</p>
<p>Example:</p>
<ul>
<li>hadoop fs -touchz pathname</li>
</ul>
<p>Exit Code: Returns 0 on success and -1 on error.</p>
<p><strong>truncate</strong></p>
<p>Usage: hadoop fs -truncate [-w] <length> <paths></paths></length></p>
<p>Truncate all files that match the specified file pattern to the specified length.</p>
<p>Options:</p>
<ul>
<li>The -w flag requests that the command waits for block recovery to complete, if necessary. Without -w flag the file may remain unclosed for some time while the recovery is in progress. During this time file cannot be reopened for append.</li>
</ul>
<p>Example:</p>
<ul>
<li>hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2</li>
<li>hadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1</li>
</ul>
<p><strong>usage</strong></p>
<p>Usage: hadoop fs -usage command</p>
<p>Return the help for an individual command.</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop HDFS command/" class="archive-article-date">
  	<time datetime="2016-11-08T05:41:32.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop HDFS介绍及安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop HDFS介绍及安装/">Hadoop HDFS介绍及安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>The project includes these modules:</p>
<ul>
<li><strong>Hadoop Common</strong>: The common utilities that support the other Hadoop modules.</li>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
<li><strong>Hadoop YARN</strong>: A framework for job scheduling and cluster resource management.</li>
<li><strong>Hadoop MapReduce</strong>: A YARN-based system for parallel processing of large data sets.</li>
</ul>
<p>Other Hadoop-related projects at Apache include:</p>
<ul>
<li><strong>Ambari™</strong>: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.</li>
<li><strong>Avro™</strong>: A data serialization system.</li>
<li><strong>Cassandra™</strong>: A scalable multi-master database with no single points of failure.</li>
<li><strong>Chukwa™</strong>: A data collection system for managing large distributed systems.</li>
<li><strong>HBase™</strong>: A scalable, distributed database that supports structured data storage for large tables.</li>
<li><strong>Hive™</strong>: A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><strong>Mahout™:</strong> A Scalable machine learning and data mining library.</li>
<li><strong>Pig™</strong>: A high-level data-flow language and execution framework for parallel computation.</li>
<li><strong>Spark™</strong>: A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.</li>
<li><strong>Tez™</strong>: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.</li>
<li><strong>ZooKeeper™</strong>: A high-performance coordination service for distributed applications.</li>
</ul>
<h2 id="Hadoop-HDFS-1-x-vs-2-x"><a href="#Hadoop-HDFS-1-x-vs-2-x" class="headerlink" title="Hadoop HDFS 1.x vs 2.x"></a>Hadoop HDFS 1.x vs 2.x</h2><p><img src="/images/Hadoop_1_x_vs_2_x.png" alt=""><br>Hadoop HDFS有2个版本，1.x和2.x，两者之间还有比较大的差距：</p>
<ol>
<li>1.x只支持MapReduce运算. 2.x除了MapReduce之外，还支持Spark，MPI，Hama，Giraph等多种类型运算。</li>
<li>1.x中的MR组件既分发任务还需要进行资源调度。 2.x由另一个组件YARN来进行资源调度，MR至进行任务分发。</li>
<li>1.x无法实现HA，只有一个NameNode管理所有节点。 2.x支持HA，有SecondNameNode，但也只支持冷迁移。2.x还支持了Federation联邦，通过文件系统挂载点，将一个NameNode的负载分发下去。注意：HA和Federation可以结合使用。</li>
</ol>
<p>网上资料中包含了10点差异，但是从技术／架构角度上来说，这3点变化是核心的。</p>
<h2 id="Hadoop-HDFS-2-x-架构图"><a href="#Hadoop-HDFS-2-x-架构图" class="headerlink" title="Hadoop HDFS 2.x 架构图"></a>Hadoop HDFS 2.x 架构图</h2><p><img src="/images/Hadoop_HDFS_2_x.png" alt=""></p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>HDFS分布式文件系统中的管理者，负责管理文件系统的命名空间，集群配置信息，存储的复制。</p>
<h4 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h4><p>并非NameNode的热备份，辅助NameNode，定期合并FSimage和EditLog。在NameNode失效的情况下，可以依据Secondary NameNode本地存储的FSimage和EditLog，恢复自身作为NameNode，但可能会有部分文件丢失，原因在于Secondary NameNode上的FSimage和EditLog并不是实时更新的。</p>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode是文件存储的节点，用于存放文件的Block，并且周期性的将Block信息发送给NameNode。值得提一句的是：HDFS中的Block大小设置很有讲究，通常为64M／128M，过小的Block会给NameNode带来巨大的管理压力，过大的Block可能会导致磁盘空间的浪费。</p>
<h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><p>于NameNode交互，获取文件存放位置；再与DataNode交互，读取或写入数据；并且可以管理整个HDFS文件系统。</p>
<h4 id="文件写入过程"><a href="#文件写入过程" class="headerlink" title="文件写入过程"></a>文件写入过程</h4><ol>
<li>Client向NameNode发起文件写入请求。</li>
<li>NameNode根据文件大小和文件块配置情况，返回给Client它所管理的DataNode信息。</li>
<li>Client将文件划分为多个Block，更具DataNode的地址信息，按顺序写入到每一个DataNode块中。<h4 id="文件读取过程"><a href="#文件读取过程" class="headerlink" title="文件读取过程"></a>文件读取过程</h4></li>
<li>Client向NameNode发起文件读取请求。</li>
<li>NameNode返回文件存储的DataNode信息。</li>
<li>Client读取文件信息。</li>
</ol>
<h2 id="Hadoop-优缺点"><a href="#Hadoop-优缺点" class="headerlink" title="Hadoop 优缺点"></a>Hadoop 优缺点</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>处理超大文件</li>
<li>流式访问数据，一次写入，多次访问</li>
<li>运行于廉价的商用机器上</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>不适合低延迟的数据访问</li>
<li>无法高效存储大量的小文件</li>
<li>不支持多用户写入及任意修改文件</li>
</ol>
<h2 id="Hadoop-HDFS-2-x-安装"><a href="#Hadoop-HDFS-2-x-安装" class="headerlink" title="Hadoop HDFS 2.x 安装"></a>Hadoop HDFS 2.x 安装</h2><p>Hadoop HDFS 2.x 包含了3种安装模式：</p>
<ol>
<li>Standalone. 独立模式</li>
<li>Pseudo-Distributed Operation. 伪分布式</li>
<li>Cluster. 集群</li>
</ol>
<h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>默认情况下，Hadoop被配置成以非分布式模式运行的一个独立Java进程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/latest</div><div class="line"><span class="built_in">cd</span>  hadoop_home/</div><div class="line">mkdir input</div><div class="line">cp etc/hadoop/*.xml input</div><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div><div class="line">cat output/*</div></pre></td></tr></table></figure></p>
<h3 id="Pseudo-Distributed-Operation"><a href="#Pseudo-Distributed-Operation" class="headerlink" title="Pseudo-Distributed Operation"></a>Pseudo-Distributed Operation</h3><p>Hadoop可以在单节点上以所谓的伪分布式模式运行，此时每一个Hadoop守护进程都作为一个独立的Java进程运行。</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>不配置Yarn</strong><br><strong>etc/hadoop/core-site.xml:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/hdfs-site.xml:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">        &lt;value&gt;3&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>Setup passphraseless ssh:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ssh localhost</div><div class="line">ssh-keygen -t dsa -P <span class="string">''</span> <span class="_">-f</span> ~/.ssh/id_dsa</div><div class="line">cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line">chmod 0600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>
<hr>
<h4 id="HDFS使用命令"><a href="#HDFS使用命令" class="headerlink" title="HDFS使用命令"></a>HDFS使用命令</h4><p>The following instructions are to run a MapReduce job locally.</p>
<ol>
<li><p>Format the filesystem:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs namenode -format</div></pre></td></tr></table></figure>
</li>
<li><p>Start NameNode daemon and DataNode daemon:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<p>The hadoop daemon log output is written to the $(HADOOP_LOG_DIR) directory (defaults to $(HADOOP_HOME)/logs).</p>
<ol>
<li>Browse the web interface for the NameNode; by default it is available at:<br> NameNode - <a href="http://localhost:50070/" target="_blank" rel="external">http://localhost:50070/</a></li>
<li><p>Make the HDFS directories required to execute MapReduce jobs:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -mkdir /user</div><div class="line">bin/hdfs dfs -mkdir /user/&lt;username&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>Copy the input files into the distributed filesystem:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -put etc/hadoop input</div></pre></td></tr></table></figure>
</li>
<li><p>Run some of the examples provided:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div></pre></td></tr></table></figure>
</li>
<li><p>Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -get output output</div><div class="line">cat output/*</div></pre></td></tr></table></figure>
</li>
</ol>
<p>or<br>View the output files on the distributed filesystem:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -cat output/*</div></pre></td></tr></table></figure></p>
<ol>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="配置Yarn"><a href="#配置Yarn" class="headerlink" title="配置Yarn"></a>配置Yarn</h4><p><strong>配置Yarn on a Single Node</strong><br><strong>etc/hadoop/mapred-site.xml</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">        &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/yarn-site.xml:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<ol>
<li><p>Start ResourceManager daemon and NodeManager daemon:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-yarn.sh</div></pre></td></tr></table></figure>
</li>
<li><p>Browse the web interface for the ResourceManager; by default it is available at:<br>ResourceManager - <a href="http://localhost:8088/" target="_blank" rel="external">http://localhost:8088/</a></p>
</li>
<li>Run a MapReduce job.<br>与不配置Yarn的HDFS使用命令类似，可参考上面的例子。</li>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-yarn.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Cluster-Setup"><a href="#Cluster-Setup" class="headerlink" title="Cluster Setup"></a>Cluster Setup</h3><p>参考：<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</a><br>目前没有试过Hadoop Cluster的安装，网上资料很多，哪一天可以试试。<br>Hadoop’s Java configuration is driven by two types of important configuration files:</p>
<ul>
<li><p>Read-only default configuration - core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.</p>
</li>
<li><p>Site-specific configuration - etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml, etc/hadoop/yarn-site.xml and etc/hadoop/mapred-site.xml.</p>
</li>
</ul>
<p>Additionally, you can control the Hadoop scripts found in the bin/ directory of the distribution, by setting site-specific values via the etc/hadoop/hadoop-env.sh and etc/hadoop/yarn-env.sh.<br>HDFS daemons are NameNode, SecondaryNameNode, and DataNode. YARN damones are ResourceManager, NodeManager, and WebAppProxy.</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop HDFS介绍及安装/" class="archive-article-date">
  	<time datetime="2016-11-07T12:45:12.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/hello-world/">Hello World1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/hello-world/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/随笔/">随笔</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-personal-profile" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/personal-profile/">个人简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>欢迎浏览 <a href="https://sstar1314.github.io/" target="_blank" rel="external">SStar1314</a>! 我的个人博客. 以及 <a href="https://github.com/SStar1314/" target="_blank" rel="external">SStar1314</a>，我的github主页. 我的主页开通于2016年11月，一直想写点什么，两年多的时间，在Evernote上记录很多自己研究的东西，一直想开通一个博客，把Evernote上的东西搬上来，顺道梳理一下以前的知识点.</p>
<h2 id="介绍一下本人"><a href="#介绍一下本人" class="headerlink" title="介绍一下本人"></a>介绍一下本人</h2><h3 id="就业单位"><a href="#就业单位" class="headerlink" title="就业单位"></a>就业单位</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">现就业于 国际商业机器有限公司（IBM） 中国系统研发实验室（CSL） 西安分公司。</div><div class="line">职位： 软件开发工程师</div></pre></td></tr></table></figure>
<h3 id="毕业院校"><a href="#毕业院校" class="headerlink" title="毕业院校"></a>毕业院校</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">硕士： 西安交通大学</div><div class="line">学士： 西安交通大学</div></pre></td></tr></table></figure>
<h3 id="工作／研究生-相关方向"><a href="#工作／研究生-相关方向" class="headerlink" title="工作／研究生 相关方向"></a>工作／研究生 相关方向</h3><h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">集群管理。</div></pre></td></tr></table></figure>
<h4 id="研究生"><a href="#研究生" class="headerlink" title="研究生"></a>研究生</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">高新能计算，GPU加速，MPI＋OpenMP＋CUDA，解决非线性规划问题。</div></pre></td></tr></table></figure>
<h3 id="个人关心的方向或技术"><a href="#个人关心的方向或技术" class="headerlink" title="个人关心的方向或技术"></a>个人关心的方向或技术</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">OpenStack云计算框架: Keystone／Nova／Neutron／Glance／Cinder／Swift／Ironic</div><div class="line">	Keystone: RabbitMQ,Qpid,AMQP(Producer+Consumer+Exchange+Queue)</div><div class="line">	Nova: nova-scheduler,nova-compute,虚拟化</div><div class="line">	Neutron: OVS,Linux Bridge,vlan,vxlan,gre.</div><div class="line">		Provider network ---- Self-defined network</div><div class="line">虚拟化: KVM／Xen</div><div class="line">大数据: Headoop／Spark</div><div class="line">	Headoop: MapReduce,HDFS,Yarn,Hbase,Hive,Storm. Hadoop1.x vs 2.x</div><div class="line">	Spark: RDD,Spark-shell,Spark SQL,Spark Streaming,MLlib,GraphX</div><div class="line">NoSQL: HBase, Cassandra, MongoDB, Redis</div><div class="line">容器化: Docker, Kubernetes, rkt, CoreOS</div><div class="line">数据中心操作系统: DCOS, Mesos, Marathon, Chronos</div><div class="line">分布式协同: ZooKeeper, etcd</div><div class="line">其它: Kafka, Elastic Search, Logstash</div></pre></td></tr></table></figure>
<p>More info: <a href="https://github.com/SStar1314/" target="_blank" rel="external">我的github主页</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/personal-profile/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/简介/">简介</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/个人简介/">个人简介</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-title" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/title/">hello world</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/title/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-大数据" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/大数据/">我的大数据起步之旅</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>长久以来，大数据非常火，各大新闻app上都在谈这个名词，然而自己并不十分清楚，只是大约上知道有Hadoop和MapReduce这么2个东西，为什么会有大数据的名词？从何而来？为什么以前没有这个名词，却出现在几年前？为什么这几年突然蹦出个Spark，也是大数据领域的，这又是个啥东西。。。</p>
<p>现在想起来，自己第一次接触大数据还是为了参加一个公司内部的竞赛，被迫响应了本地公司领导的号召，代表了西安site去参加Spark编程竞赛。。。于是，一共6个人组了一个小队，基于Spark GraphX＋GeoJson做了个<a href="https://github.com/SStar1314/sp_roadmap" target="_blank" rel="external">地图</a>，期间被scala快折腾疯了，到现在我还认为scala是世上最屎的语言，没有之一！不过结果是好的，最后拿了个第一名回来，公司请吃了个大餐，还算值，哈哈</p>
<h2 id="言归正传"><a href="#言归正传" class="headerlink" title="言归正传"></a>言归正传</h2><p>要想知道大数据是什么？包括哪些技术？怎么应用？为什么大数据这年纪突然之间火起来？我认为我们可以问以下这些问题，当能正确回答这些问题，并理解为什么需要问这些问题的时候，那么我认为你已经走进了大数据这个圈子了。</p>
<ol>
<li>大数据，大数据，数据量有多大？这些数据怎么存储？</li>
<li>存了这么多数据，目的是什么？这些数据怎么利用？</li>
<li>数据从哪来？到哪去？</li>
</ol>
<h2 id="我的回答"><a href="#我的回答" class="headerlink" title="我的回答"></a>我的回答</h2><p>作为一个非大数据领域的外行程序员，自问自答，貌似有点班门弄斧，呵呵<br>那我只能说，我的博客我做主，就是这么任性～～哈哈</p>
<ol>
<li>大数据，大数据，数据量有多大？这些数据怎么存储？<br>数量级：数百TB至几十PB，并且赠长速度很快。这么多数据，一块磁盘才2TB，怎么存啊，我得买成百上千个磁盘才能存下来呀。这些数据存到磁盘上，是存到文件里，还是存到数据库里呢？<br>我可以告诉你的是，传统关系型数据库(MySQL,Oracle,SQL Server)无法存储这么大数据量的数据，就算可以存的下，查询／插入／更新操作会让用户抓狂的。那只能存到文件里面喽，放一个文件肯定是不可能的，想象以下一个2TB的文件，你要怎么打开？所以只能分开存成很多个文件，每个文件存储一部分信息。<br>试想一下，每台机器上都存很多文件，那么多台机器，无论是数据存储／备份／查询都很是问题，在这样一个情况之下，分布式文件系统应时而生，代表性的就是Hadoop HDFS。<br>HDFS解决了，大数据存储以及检索困难的问题，并且通过数据分布式备份解决了硬件设备错误的问题。</li>
<li>存了这么多数据，目的是什么？这些数据怎么利用？<br>为什么需要存储这么多数据呢，现在有一个普遍的认识，就是“我们没有更好的算法，我们只有更多的数据”。这意味着，基于大量数据，通过数据挖掘＋机器学习算法的结合，能够得到比纯算法推导更好的结果。所以，数据很重要。<br>针对于PB级大数据的使用／分析，又成了一个头疼的问题，通过传统的高新能计算的加速手段(比如: MPI)，无法取得良好的加速效果，表现的现象是：数据传输的时间远大于计算所需要的时间。原因是：实际的大数据应用中，对数据的分析算法往往很简单，痛点在于数据的读入及输出，而传统的高性能计算领域的加速效果侧重于算法加速，不适用于大数据领域。<br>于是，Hadoop MapReduce应时而生，将对大数据的分析任务划分成非常多的子任务，通过Map／Reduce两个过程，将计算任务分到集群中的每一台机器，借助于Hadoop Yarn资源调度器，达到“计算向数据靠近”的目标，从而可以利用上大数据。</li>
<li>数据从哪来？到哪去？<br>数据来源太广了，任何领域都能产生数据，我们应当分析出什么领域带动了大数据，他们的数据来自于哪。我个人任务大数据发源地是“互联网”，由人制作／产生的文档／音频／视频／直播将数据拉到第一个高度，让传统方法无法处理这么大规模的数据，随之来临的就是<a href="https://book.douban.com/subject/20429677/" target="_blank" rel="external">《大数据时代》</a>。“互联网”是一个不可避免的趋势及未来，传统企业在“互联网”的冲击和带动下，必然会逐渐重视数据的重要性，从而构建和自身企业核心业务相关的数据分析平台，甚至构建更大规模的云计算平台。众多传统企业的参与，会将数据拉到另一个高度，进入IOT物联网时代。这些是我个人对数据来源形势的理解。对于数据到哪去的问题，很简单，为了更好的服务于人。</li>
</ol>
<p>感谢你们花了大把时间看完，在这里我推荐一本书<a href="http://item.jd.com/11966465.html" target="_blank" rel="external">《云计算架构 技术与实践 第2版》</a>，华为 顾炯炯 编著，一定要是第2版哦，这本书很全面，作者没有藏私，很赞！不过阅读这本书，需要多多google。</p>
<h2 id="转到正题"><a href="#转到正题" class="headerlink" title="转到正题"></a>转到正题</h2><p>作为程序员，技术还是第一重要的，了解完高逼格的大趋势之后，搬砖的还是乖乖的回归原位，老老实实的谈技术吧。我会在其它文章中谈到以下内容：</p>
<ol>
<li>Hadoop<br>MapReduce, HDFS, Yarn, Hbase, Hive, Storm.<br>Hadoop1.x vs 2.x</li>
<li>Spark<br> RDD, Spark-shell, Spark SQL, Spark Streaming, MLlib, GraphX</li>
<li>NoSQL<br>HBase, Cassandra, MongoDB, Redis</li>
</ol>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/大数据/" class="archive-article-date">
  	<time datetime="2016-11-07T02:31:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-07</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Xia,MingXing
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: false
	}
</script>

<script src="/./main.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/大数据/" style="font-size: 20px;">大数据</a> <a href="/tags/简介/" style="font-size: 10px;">简介</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>