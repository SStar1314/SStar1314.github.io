<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>SStar1314</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SStar1314">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="SStar1314">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SStar1314">
  
    <link rel="alternative" href="/atom.xml" title="SStar1314" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/main.css">
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/favicon.ico" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Xia,MingXing</a></h1>
		</hgroup>

		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">所有文章</a></li>
	        
				<li><a href="/tags/随笔">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="#" title="rss">rss</a>
		        
					<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Xia,MingXing</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/favicon.ico" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Xia,MingXing</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/tags/随笔">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-HDFS Federation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS Federation/">HDFS Federation(联邦)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Federation-联邦"><a href="#HDFS-Federation-联邦" class="headerlink" title="HDFS Federation(联邦)"></a>HDFS Federation(联邦)</h2><p>在前面的文章介绍过，Hadoop的Federation是将整个文件系统划分为子集，每一个Federation中的NameNode负责管理其中一个子集，整个文件系统由这些子集通过挂载mount的方式构建。 Federation与HA结合使用。</p>
<p>官方doc: <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html</a></p>
<h3 id="HDFS-has-two-main-layers"><a href="#HDFS-has-two-main-layers" class="headerlink" title="HDFS has two main layers:"></a>HDFS has two main layers:</h3><p><strong>Namespace :</strong></p>
<pre><code>1. Consists of directories, files and blocks.
2. It supports all the namespace related file system operations such as create, delete, modify and list files and directories.            
</code></pre><p><strong>Block Storage Services ,  this has two parts :</strong></p>
<pre><code>1. Block Management (performed in the Namenode)
  Provides Datanode cluster membership by handling registrations, and periodic heart beats.
  Processes block reports and maintains location of blocks.
  Supports block related operations such as create, delete, modify and get block location.
  Manages replica placement, block replication for under replicated blocks, and deletes blocks that are over replicated.   
2. Storage   -   is provided by Datanodes by storing blocks on the local file system and allowing read/write access.
</code></pre><h3 id="Multiple-Namenodes-Namespaces"><a href="#Multiple-Namenodes-Namespaces" class="headerlink" title="Multiple Namenodes/Namespaces"></a>Multiple Namenodes/Namespaces</h3><p>Federation uses multiple independent Namenodes/namespaces to scale the name service horizontally. <strong>The Namenodes are federated; the Namenodes are independent and do not require coordination with each other.</strong> The Datanodes are used as common storage for blocks by all the Namespaces. Each Datanode registers with all the Namenodes in the cluster. Datanodes send periodic heartbeats and block reports.<br><img src="/images/HDFS_Fedaration_1.png" alt=""></p>
<h3 id="Federation-Configuration"><a href="#Federation-Configuration" class="headerlink" title="Federation Configuration"></a>Federation Configuration</h3><p>Federation configuration is <strong>backward compatible</strong> and allows existing single Namenode configuration to work without any change.<br><strong>Step 1:</strong> Add the dfs.nameservices parameter to your configuration and configure it with a list of comma separated NameServiceIDs. This will be used by the Datanodes to determine the Namenodes in the cluster.<br><strong>Step 2:</strong> For each Namenode and Secondary Namenode/BackupNode/Checkpointer add the following configuration parameters suffixed with the corresponding NameServiceID into the common configuration file:<br><strong>Namenode</strong></p>
<ul>
<li>dfs.namenode.rpc-address</li>
<li>dfs.namenode.servicerpc-address</li>
<li>dfs.namenode.http-address</li>
<li>dfs.namenode.https-address</li>
<li>dfs.namenode.keytab.file</li>
<li>dfs.namenode.name.dir</li>
<li>dfs.namenode.edits.dir</li>
<li>dfs.namenode.checkpoint.dir</li>
<li>dfs.namenode.checkpoint.edits.dir<br><strong>Secondary Namenode</strong></li>
<li>dfs.namenode.secondary.http-address</li>
<li>dfs.secondary.namenode.keytab.file<br><strong>BackupNode</strong></li>
<li>dfs.namenode.backup.address</li>
<li>dfs.secondary.namenode.keytab.file</li>
</ul>
<h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml:"></a>hdfs-site.xml:</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1,ns2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host1:rpc-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host1:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondaryhttp-address.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>snn-host1:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host2:rpc-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn-host2:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondaryhttp-address.ns2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>snn-host2:http-port<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="Formatting-Namenodes"><a href="#Formatting-Namenodes" class="headerlink" title="Formatting Namenodes"></a>Formatting Namenodes</h4><p>Step 1: Format a Namenode:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format [-clusterId &lt;cluster_id&gt;]</div></pre></td></tr></table></figure></p>
<p>Step 2: Format additional Namenodes<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format -clusterId &lt;cluster_id&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Upgrading-from-an-older-release-and-configuring-federation"><a href="#Upgrading-from-an-older-release-and-configuring-federation" class="headerlink" title="Upgrading from an older release and configuring federation"></a>Upgrading from an older release and configuring federation</h4><p>Older releases only support a single Namenode, after Upgrade the cluster to newer release in order to enable federation.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs start namenode --config <span class="variable">$HADOOP_CONF_DIR</span>  -upgrade -clusterId &lt;cluster_ID&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Adding-a-new-Namenode-to-an-existing-HDFS-cluster"><a href="#Adding-a-new-Namenode-to-an-existing-HDFS-cluster" class="headerlink" title="Adding a new Namenode to an existing HDFS cluster"></a>Adding a new Namenode to an existing HDFS cluster</h4><p>Perform the following steps:</p>
<ul>
<li>Add dfs.nameservices to the configuration.</li>
<li>Update the configuration with the NameServiceID suffix. Configuration key names changed post release 0.20. You must use the new configuration parameter names in order to use federation.</li>
<li>Add the new Namenode related config to the configuration file.</li>
<li>Propagate the configuration file to the all the nodes in the cluster.</li>
<li>Start the new Namenode and Secondary/Backup.</li>
<li>Refresh the Datanodes to pickup the newly added Namenode by running the following command against all the Datanodes in the cluster:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs dfsadmin -refreshNameNodes &lt;datanode_host_name&gt;:&lt;datanode_rpc_port&gt;</div></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS Federation/" class="archive-article-date">
  	<time datetime="2017-07-19T06:04:11.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS Snapshots" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS Snapshots/">HDFS Snapshots</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-Snapshots"><a href="#HDFS-Snapshots" class="headerlink" title="HDFS Snapshots"></a>HDFS Snapshots</h2><p>Snapshots功能非常重要，可以保证HDFS在出现异常情况时可以进行恢复。 Snapshots可以使用在整个HDFS系统上，也可以只对其中的部分文件目录。</p>
<p>HDFS Snapshots are read-only point-in-time copies of file system. Snapshots can be taken on a subtree of the file system or the entire file system.</p>
<p>The HDFS path should be Snapshottable.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Snapshots Paths “.snapshot” is used <span class="keyword">for</span> accessing its snapshots.                hadoop  fs -ls /foo/.snapshot</div><div class="line">Allow Snapshots <span class="built_in">command</span>:                   hdfs  dfsadmin -allowSnapshot &lt;path&gt;</div><div class="line">Disallow Snapshots <span class="built_in">command</span>:             hdfs  dfsadmin  -disallowSnapshot &lt;path&gt;   </div><div class="line">Create Snapshots <span class="built_in">command</span>:                  hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</div><div class="line">Delete Snapshots <span class="built_in">command</span>:                  hdfs dfs -deleteSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</div><div class="line">Rename Snapshots <span class="built_in">command</span>:                  hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;</div><div class="line">Get Snapshottable Directory Listing <span class="built_in">command</span>:           hdfs lsSnapshottableDir</div><div class="line">Get Snapshots Difference Report <span class="built_in">command</span>:                 hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS Snapshots/" class="archive-article-date">
  	<time datetime="2017-07-19T06:00:59.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HDFS安装(三种模式)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HDFS安装(三种模式)/">HDFS安装(三种模式)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS 分布式文件系统"></a>HDFS 分布式文件系统</h2><ul>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
</ul>
<h2 id="Hadoop-HDFS-2-x-安装"><a href="#Hadoop-HDFS-2-x-安装" class="headerlink" title="Hadoop HDFS 2.x 安装"></a>Hadoop HDFS 2.x 安装</h2><p>Hadoop HDFS 2.x 包含了3种安装模式：</p>
<ol>
<li>Standalone. 独立模式</li>
<li>Pseudo-Distributed Operation. 伪分布式</li>
<li>Cluster. 集群</li>
</ol>
<h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>默认情况下，Hadoop被配置成以非分布式模式运行的一个独立Java进程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/java/openjdk1.8/</div><div class="line"><span class="built_in">cd</span>  hadoop_home/</div><div class="line">mkdir input</div><div class="line">cp etc/hadoop/*.xml input</div><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div><div class="line">cat output/*</div></pre></td></tr></table></figure></p>
<h3 id="Pseudo-Distributed-Operation"><a href="#Pseudo-Distributed-Operation" class="headerlink" title="Pseudo-Distributed Operation"></a>Pseudo-Distributed Operation</h3><p>Hadoop可以在单节点上以所谓的伪分布式模式运行，此时每一个Hadoop守护进程都作为一个独立的Java进程运行。</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>不配置Yarn</strong><br><strong>etc/hadoop/core-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/hdfs-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>Setup passphraseless ssh login:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ssh localhost</div><div class="line">ssh-keygen -t dsa -P <span class="string">''</span> -f ~/.ssh/id_dsa</div><div class="line">cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line">chmod 0600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>
<hr>
<h4 id="伪分布式HDFS初始化及使用命令"><a href="#伪分布式HDFS初始化及使用命令" class="headerlink" title="伪分布式HDFS初始化及使用命令"></a>伪分布式HDFS初始化及使用命令</h4><p>The following instructions are to run a MapReduce job locally.</p>
<ol>
<li><p>Format the filesystem: 初始化!!!</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs namenode -format</div></pre></td></tr></table></figure>
</li>
<li><p>Start NameNode daemon and DataNode daemon:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<p>The hadoop daemon log output is written to the $(HADOOP_LOG_DIR) directory (defaults to $(HADOOP_HOME)/logs).</p>
<ol>
<li>Browse the web interface for the NameNode; by default it is available at:<br> NameNode - <a href="http://localhost:50070/" target="_blank" rel="external">http://localhost:50070/</a></li>
<li><p>Make the HDFS directories required to execute MapReduce jobs:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -mkdir /user</div><div class="line">bin/hdfs dfs -mkdir /user/&lt;username&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>Copy the input files into the distributed filesystem:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -put etc/hadoop input</div></pre></td></tr></table></figure>
</li>
<li><p>Run some of the examples provided:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></div></pre></td></tr></table></figure>
</li>
<li><p>Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -get output output</div><div class="line">cat output/*</div></pre></td></tr></table></figure>
</li>
</ol>
<p>or<br>View the output files on the distributed filesystem:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs dfs -cat output/*</div></pre></td></tr></table></figure></p>
<ol>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-dfs.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="配置Yarn"><a href="#配置Yarn" class="headerlink" title="配置Yarn"></a>配置Yarn</h4><p><strong>配置Yarn on a Single Node</strong><br><strong>etc/hadoop/mapred-site.xml</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>etc/hadoop/yarn-site.xml:</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<ol>
<li><p>Start ResourceManager daemon and NodeManager daemon:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-yarn.sh</div></pre></td></tr></table></figure>
</li>
<li><p>Browse the web interface for the ResourceManager; by default it is available at:<br>ResourceManager - <a href="http://localhost:8088/" target="_blank" rel="external">http://localhost:8088/</a></p>
</li>
<li>Run a MapReduce job.<br>与不配置Yarn的HDFS使用命令类似，可参考上面的例子。</li>
<li>When you’re done, stop the daemons with:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/stop-yarn.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Cluster-Setup"><a href="#Cluster-Setup" class="headerlink" title="Cluster Setup"></a>Cluster Setup</h3><p>参考：<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</a><br>下载 hadoop2.7.3 版本的压缩包，解压缩到master节点上， 解压路径为 ${Hadoop_Install} .<br>配置 hadoop cluster 中各个节点之间的passwordless 无密码访问。</p>
<h4 id="Configure-Hadoop-Cluster"><a href="#Configure-Hadoop-Cluster" class="headerlink" title="Configure Hadoop Cluster"></a>Configure Hadoop Cluster</h4><p>到 ${Hadoop_Install}/etc/hadoop/ 目录下 编辑配置文件： <strong>core-site.xml</strong>  <strong>hdfs-site.xml</strong>  <strong>mapred-site.xml</strong>  <strong>yarn-site.xml</strong> .<br><strong>core-site.xml :  configure important parameters</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-nn:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>hdfs-site.xml :  configure for NameNode + DataNode</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>10240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>mapred-site.xml :  Configure for MapReduce Applications + MapReduce JobHistory Server</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p><strong>yarn-site.xml :  Configure for ResourceManager + NodeManager + History Server</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8025<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8035<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-nn:8050<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h4 id="Hadoop-Cluster-Startup"><a href="#Hadoop-Cluster-Startup" class="headerlink" title="Hadoop Cluster Startup"></a>Hadoop Cluster Startup</h4><p><strong>Format a new distributed filesystem:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format &lt;cluster_name&gt;</div></pre></td></tr></table></figure></p>
<p>会生成一个name文件夹，里面存储fsimage和editlog文件，记录整个cluster中的文件系统。<br><strong>Start HDFS NameNode :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start namenode</div></pre></td></tr></table></figure></p>
<p><strong>Start HDFS DataNode :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> --script hdfs start datanode</div></pre></td></tr></table></figure></p>
<p><strong>Start all Hadoop slaves * :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hdfs]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-dfs.sh</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  ResourceManager :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start resourcemanager</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  NodeManager :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemons.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start nodemanager</div></pre></td></tr></table></figure></p>
<p><strong>Start  Yarn  WebAppProxy server if necessary:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_YARN_HOME</span>/sbin/yarn-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start proxyserver</div></pre></td></tr></table></figure></p>
<p><strong>Start all  Yarn  slaves *:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[yarn]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/start-yarn.sh</div></pre></td></tr></table></figure></p>
<p><strong>Start MapReduce JobHistory server :</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[mapred]$ <span class="variable">$HADOOP_PREFIX</span>/sbin/mr-jobhistory-daemon.sh --config <span class="variable">$HADOOP_CONF_DIR</span> start historyserver</div></pre></td></tr></table></figure></p>
<h4 id="Hadoop-Cluster-Web-Interfaces"><a href="#Hadoop-Cluster-Web-Interfaces" class="headerlink" title="Hadoop Cluster Web Interfaces"></a>Hadoop Cluster Web Interfaces</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NameNode     http://hadoop_namenode:50070/</div><div class="line">ResourceManager     http://hadoop_resourcemanager:8088/</div><div class="line">MapReduce JobHistory Server     http://jobhistory_serevr:19888/</div></pre></td></tr></table></figure>
<h4 id="Hadoop-Cluster-exclude-decommision-Datanodes"><a href="#Hadoop-Cluster-exclude-decommision-Datanodes" class="headerlink" title="Hadoop Cluster exclude/decommision Datanodes"></a>Hadoop Cluster exclude/decommision Datanodes</h4><p><strong>configure  hdfs-site.xml :</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hdfs_exclude.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>DFS exclude<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>Then write the decommission data node(slave2) to <strong>hdfs_exclude.txt</strong> file.<br>Last, force configure reload:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop dfsadmin  -refreshNodes</div><div class="line">hadoop dfsadmin  -report</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HDFS安装(三种模式)/" class="archive-article-date">
  	<time datetime="2017-07-19T05:40:33.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop介绍/">Hadoop介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop主要包含以下部分"><a href="#Hadoop主要包含以下部分" class="headerlink" title="Hadoop主要包含以下部分"></a>Hadoop主要包含以下部分</h2><ul>
<li><strong>Hadoop Common</strong>: The common utilities that support the other Hadoop modules.</li>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.</li>
<li><strong>Hadoop YARN</strong>: A framework for job scheduling and cluster resource management.</li>
<li><strong>Hadoop MapReduce</strong>: A YARN-based system for parallel processing of large data sets.</li>
</ul>
<h2 id="Hadoop生态圈"><a href="#Hadoop生态圈" class="headerlink" title="Hadoop生态圈"></a>Hadoop生态圈</h2><ul>
<li><strong>Ambari™</strong>: 部署服务. A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.</li>
<li><strong>Avro™</strong>: 数据序列化. A data serialization system.</li>
<li><strong>Cassandra™</strong>: 分布式NoSQL. A scalable multi-master database with no single points of failure.</li>
<li><strong>Chukwa™</strong>: 数据收集. A data collection system for managing large distributed systems.</li>
<li><strong>HBase™</strong>: 分布式NoSQL. A scalable, distributed database that supports structured data storage for large tables.</li>
<li><strong>Hive™</strong>: 数据仓库. A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><strong>Mahout™:</strong> 数据挖掘. A Scalable machine learning and data mining library.</li>
<li><strong>Pig™</strong>: 可转化为MapReduce的数据查询. A high-level data-flow language and execution framework for parallel computation.</li>
<li><strong>Spark™</strong>: 内存加速的运算框架. A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.</li>
<li><strong>Tez™</strong>: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.</li>
<li><strong>ZooKeeper™</strong>: 分布式协同框架. A high-performance coordination service for distributed applications.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop介绍/" class="archive-article-date">
  	<time datetime="2017-07-19T05:18:07.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-ZooKeeper RESTful" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/ZooKeeper RESTful/">ZooKeeper RESTful</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Enable-ZooKeeper-RESTful-service："><a href="#Enable-ZooKeeper-RESTful-service：" class="headerlink" title="Enable ZooKeeper RESTful service："></a>Enable ZooKeeper RESTful service：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">vim  rest.properties</div><div class="line"><span class="built_in">cd</span>   /root/zookeeper-3.4.8/src/contrib/rest</div><div class="line">nohup ant run &amp;    </div><div class="line">curl -H<span class="string">'Accept: application/json'</span> http://localhost:9998/znodes/v1/                    json</div><div class="line">curl -H<span class="string">'Accept: application/xml'</span> http://localhost:9998/znodes/v1/                    xml</div></pre></td></tr></table></figure>
<h3 id="RESTful-用法："><a href="#RESTful-用法：" class="headerlink" title="RESTful 用法："></a>RESTful 用法：</h3><p>#get children of the root node<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl http://localhost:9998/znodes/v1/?view=children</div></pre></td></tr></table></figure></p>
<p>#get “/cluster1/leader” as xml (default is json)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -H<span class="string">'Accept: application/xml'</span> http://localhost:9998/znodes/v1/cluster1/leader</div></pre></td></tr></table></figure></p>
<p>#get the data as text<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster1/leader?dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<p>#set a node (data.txt contains the ascii text you want to set on the node)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -T data.txt -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster1/leader?dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<p>#create a node<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">"data1"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/?op=create&amp;name=cluster2&amp;dataformat=utf8"</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">"data2"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/znodes/v1/cluster2?op=create&amp;name=leader&amp;dataformat=utf8"</span></div></pre></td></tr></table></figure>
<p>#create a new session<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d <span class="string">""</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/?op=create&amp;expire=10"</span></div></pre></td></tr></table></figure></p>
<p>#session heartbeat<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X <span class="string">"PUT"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/02dfdcc8-8667-4e53-a6f8-ca5c2b495a72"</span></div></pre></td></tr></table></figure></p>
<p>#delete a session<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X <span class="string">"DELETE"</span> -H<span class="string">'Content-Type: application/octet-stream'</span> -w <span class="string">"\n%&#123;http_code&#125;\n"</span> <span class="string">"http://localhost:9998/sessions/v1/02dfdcc8-8667-4e53-a6f8-ca5c2b495a72"</span></div></pre></td></tr></table></figure></p>
<h2 id="service-启动命令"><a href="#service-启动命令" class="headerlink" title="service 启动命令"></a>service 启动命令</h2><p><strong>RestAPI源码入门：</strong><br>RestAPI 入口main函数所在文件： <strong>org.apache.zookeeper.server.jersey.RestMain</strong></p>
<p><strong>ZooKeeper Source Code 解析：</strong><br>1.zkServer 脚本启动命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ZOOMAIN=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=<span class="variable">$JMXLOCALONLY</span> org.apache.zookeeper.server.quorum.QuorumPeerMain"</span></div><div class="line"></div><div class="line">nohup <span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</div><div class="line">        -agentlib:jdwp=transport=dt_socket,server=y,<span class="built_in">suspend</span>=y,address=7778 \                         </div><div class="line">        -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</div></pre></td></tr></table></figure></p>
<p>2.zkCli 脚本启动命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">"<span class="variable">$JAVA</span>"</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \</div><div class="line">     -cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$CLIENT_JVMFLAGS</span> <span class="variable">$JVMFLAGS</span> \</div><div class="line">     org.apache.zookeeper.ZooKeeperMain <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure></p>
<p>由上可知， ZooKeeper的server启动入口函数为 <strong>QuorumPeerMain</strong> ，而client的启动入口函数为 <strong>ZooKeeperMain</strong>。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/ZooKeeper RESTful/" class="archive-article-date">
  	<time datetime="2017-07-19T02:45:39.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ZooKeeper/">ZooKeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-分布式协同" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/分布式协同/">分布式协同技术</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="分布式协同技术"><a href="#分布式协同技术" class="headerlink" title="分布式协同技术"></a>分布式协同技术</h2><p>分布式协同技术诞生于分布式系统中，致力于解决各大分布式系统或分布式计算平台点到点的同步问题。 代表性的有 etcd, ZooKeeper, Consul, Doozerd。 其中：</p>
<h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><ul>
<li>golang 语言编写</li>
<li>coreos 公司研发</li>
<li>被mesos kubernetes等 热门分布式平台所应用</li>
<li>支持RESTful api</li>
<li>基于 Raft 算法<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3></li>
<li>java 语言编写</li>
<li>Apache 基金会maintain</li>
<li>被Hadoop Kafka等 热门分布式平台所应用</li>
<li>支持RESTful api</li>
<li>基于 Paxos 算法</li>
</ul>
<h2 id="分布式协同算法-Raft-和-Paxos"><a href="#分布式协同算法-Raft-和-Paxos" class="headerlink" title="分布式协同算法 Raft 和 Paxos"></a>分布式协同算法 Raft 和 Paxos</h2><p>推荐一个Raft算法动态描述的网站： <a href="https://raft.github.io/" target="_blank" rel="external">https://raft.github.io/</a></p>
<p>至于 Raft 和 Paxos 算法的区别，网上文章有一些，可以阅读一下，但是本人至今没仔细钻研过两个算法的区别，以后如果有时间再补上。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/分布式协同/" class="archive-article-date">
  	<time datetime="2017-07-19T01:36:10.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-07-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式协同/">分布式协同</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/分布式协同/">分布式协同</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kubernetes源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Kubectl-go"><a href="#Kubectl-go" class="headerlink" title="Kubectl.go"></a>Kubectl.go</h1><p>1.logs.InitLogs()<br>k8s.io/kubernetes/pkg/util/logs 日志管理代码<br>每30秒刷新一次日志。 使用了github.com/golang/glog</p>
<ol>
<li>NewDefaultClientConfigLoadingRules<br>currentMigrationRules</li>
</ol>
<p>type Factory struct {<br>    clients <em>ClientCache<br>    flags   </em>pflag.FlagSet</p>
<pre><code>// Returns interfaces for dealing with arbitrary runtime.Objects.
Object func() (meta.RESTMapper, runtime.ObjectTyper)
// Returns interfaces for dealing with arbitrary
// runtime.Unstructured. This performs API calls to discover types.
UnstructuredObject func() (meta.RESTMapper, runtime.ObjectTyper, error)
// Returns interfaces for decoding objects - if toInternal is set, decoded objects will be converted
// into their internal form (if possible). Eventually the internal form will be removed as an option,
// and only versioned objects will be returned.
Decoder func(toInternal bool) runtime.Decoder
// Returns an encoder capable of encoding a provided object into JSON in the default desired version.
JSONEncoder func() runtime.Encoder
// ClientSet gives you back an internal, generated clientset
ClientSet func() (*internalclientset.Clientset, error)
// Returns a RESTClient for accessing Kubernetes resources or an error.
RESTClient func() (*restclient.RESTClient, error)
// Returns a client.Config for accessing the Kubernetes server.
ClientConfig func() (*restclient.Config, error)
// Returns a RESTClient for working with the specified RESTMapping or an error. This is intended
// for working with arbitrary resources and is not guaranteed to point to a Kubernetes APIServer.
ClientForMapping func(mapping *meta.RESTMapping) (resource.RESTClient, error)
// Returns a RESTClient for working with Unstructured objects.
UnstructuredClientForMapping func(mapping *meta.RESTMapping) (resource.RESTClient, error)
// Returns a Describer for displaying the specified RESTMapping type or an error.
Describer func(mapping *meta.RESTMapping) (kubectl.Describer, error)
// Returns a Printer for formatting objects of the given type or an error.
Printer func(mapping *meta.RESTMapping, options kubectl.PrintOptions) (kubectl.ResourcePrinter, error)
// Returns a Scaler for changing the size of the specified RESTMapping type or an error
Scaler func(mapping *meta.RESTMapping) (kubectl.Scaler, error)
// Returns a Reaper for gracefully shutting down resources.
Reaper func(mapping *meta.RESTMapping) (kubectl.Reaper, error)
// Returns a HistoryViewer for viewing change history
HistoryViewer func(mapping *meta.RESTMapping) (kubectl.HistoryViewer, error)
// Returns a Rollbacker for changing the rollback version of the specified RESTMapping type or an error
Rollbacker func(mapping *meta.RESTMapping) (kubectl.Rollbacker, error)
// Returns a StatusViewer for printing rollout status.
StatusViewer func(mapping *meta.RESTMapping) (kubectl.StatusViewer, error)
// MapBasedSelectorForObject returns the map-based selector associated with the provided object. If a
// new set-based selector is provided, an error is returned if the selector cannot be converted to a
// map-based selector
MapBasedSelectorForObject func(object runtime.Object) (string, error)
// PortsForObject returns the ports associated with the provided object
PortsForObject func(object runtime.Object) ([]string, error)
// ProtocolsForObject returns the &lt;port, protocol&gt; mapping associated with the provided object
ProtocolsForObject func(object runtime.Object) (map[string]string, error)
// LabelsForObject returns the labels associated with the provided object
LabelsForObject func(object runtime.Object) (map[string]string, error)
// LogsForObject returns a request for the logs associated with the provided object
LogsForObject func(object, options runtime.Object) (*restclient.Request, error)
// PauseObject marks the provided object as paused ie. it will not be reconciled by its controller.
PauseObject func(object runtime.Object) (bool, error)
// ResumeObject resumes a paused object ie. it will be reconciled by its controller.
ResumeObject func(object runtime.Object) (bool, error)
// Returns a schema that can validate objects stored on disk.
Validator func(validate bool, cacheDir string) (validation.Schema, error)
// SwaggerSchema returns the schema declaration for the provided group version kind.
SwaggerSchema func(unversioned.GroupVersionKind) (*swagger.ApiDeclaration, error)
// Returns the default namespace to use in cases where no
// other namespace is specified and whether the namespace was
// overridden.
DefaultNamespace func() (string, bool, error)
// Generators returns the generators for the provided command
Generators func(cmdName string) map[string]kubectl.Generator
// Check whether the kind of resources could be exposed
CanBeExposed func(kind unversioned.GroupKind) error
// Check whether the kind of resources could be autoscaled
CanBeAutoscaled func(kind unversioned.GroupKind) error
// AttachablePodForObject returns the pod to which to attach given an object.
AttachablePodForObject func(object runtime.Object) (*api.Pod, error)
// UpdatePodSpecForObject will call the provided function on the pod spec this object supports,
// return false if no pod spec is supported, or return an error.
UpdatePodSpecForObject func(obj runtime.Object, fn func(*api.PodSpec) error) (bool, error)
// EditorEnvs returns a group of environment variables that the edit command
// can range over in order to determine if the user has specified an editor
// of their choice.
EditorEnvs func() []string
// PrintObjectSpecificMessage prints object-specific messages on the provided writer
PrintObjectSpecificMessage func(obj runtime.Object, out io.Writer)
</code></pre><p>}</p>
<p>const (<br>    RunV1GeneratorName                          = “run/v1”<br>    RunPodV1GeneratorName                       = “run-pod/v1”<br>    ServiceV1GeneratorName                      = “service/v1”<br>    ServiceV2GeneratorName                      = “service/v2”<br>    ServiceNodePortGeneratorV1Name              = “service-nodeport/v1”<br>    ServiceClusterIPGeneratorV1Name             = “service-clusterip/v1”<br>    ServiceLoadBalancerGeneratorV1Name          = “service-loadbalancer/v1”<br>    ServiceAccountV1GeneratorName               = “serviceaccount/v1”<br>    HorizontalPodAutoscalerV1Beta1GeneratorName = “horizontalpodautoscaler/v1beta1”<br>    HorizontalPodAutoscalerV1GeneratorName      = “horizontalpodautoscaler/v1”<br>    DeploymentV1Beta1GeneratorName              = “deployment/v1beta1”<br>    DeploymentBasicV1Beta1GeneratorName         = “deployment-basic/v1beta1”<br>    JobV1Beta1GeneratorName                     = “job/v1beta1”<br>    JobV1GeneratorName                          = “job/v1”<br>    ScheduledJobV2Alpha1GeneratorName           = “scheduledjob/v2alpha1”<br>    NamespaceV1GeneratorName                    = “namespace/v1”<br>    ResourceQuotaV1GeneratorName                = “resourcequotas/v1”<br>    SecretV1GeneratorName                       = “secret/v1”<br>    SecretForDockerRegistryV1GeneratorName      = “secret-for-docker-registry/v1”<br>    SecretForTLSV1GeneratorName                 = “secret-for-tls/v1”<br>    ConfigMapV1GeneratorName                    = “configmap/v1”<br>)</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kubernetes源码/" class="archive-article-date">
  	<time datetime="2016-12-15T12:28:08.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-12-15</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka 源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka 源码/">Kafka 启动＋源代码import进Eclipse</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafka-源代码-import进Eclipse"><a href="#Kafka-源代码-import进Eclipse" class="headerlink" title="Kafka 源代码 import进Eclipse"></a>Kafka 源代码 import进Eclipse</h3><p><strong>Eclipse 安装 scala IDE 插件：</strong><br><a href="http://download.scala-ide.org/sdk/lithium/e44/scala211/stable/site" target="_blank" rel="external">http://download.scala-ide.org/sdk/lithium/e44/scala211/stable/site</a></p>
<p><strong>源代码 import进Eclipse</strong></p>
<ul>
<li>1.brew install gradle</li>
<li>2.gradle</li>
<li>3./gradlew eclipse      对应  ./gradlew idea</li>
</ul>
<h3 id="Kafka-启动"><a href="#Kafka-启动" class="headerlink" title="Kafka 启动"></a>Kafka 启动</h3><p>Step 1: Download the code</p>
<p>Download the 0.10.0.0 release and un-tar it.</p>
<blockquote>
<p>tar -xzf kafka_2.11-0.10.0.0.tgz</p>
<p>cd kafka_2.11-0.10.0.0</p>
</blockquote>
<p>Step 2: Start the server</p>
<p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p>
<blockquote>
<p>bin/zookeeper-server-start.sh config/zookeeper.properties<br>[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)<br>…</p>
</blockquote>
<p>Now start the Kafka server:</p>
<blockquote>
<p>bin/kafka-server-start.sh config/server.properties<br>[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)<br>[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)<br>…</p>
</blockquote>
<p>Step 3: Create a topic</p>
<p>Let’s create a topic named “test” with a single partition and only one replica:</p>
<blockquote>
<p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test</p>
</blockquote>
<p>We can now see that topic if we run the list topic command:</p>
<blockquote>
<p>bin/kafka-topics.sh –list –zookeeper localhost:2181</p>
</blockquote>
<p>test</p>
<p>Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.</p>
<p>Step 4: Send some messages</p>
<p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.</p>
<p>Run the producer and then type a few messages into the console to send to the server.</p>
<blockquote>
<p>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test<br>This is a message</p>
</blockquote>
<p>This is another message</p>
<p>Step 5: Start a consumer</p>
<p>Kafka also has a command line consumer that will dump out messages to standard output.</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –topic test –from-beginning<br>This is a message<br>This is another message</p>
</blockquote>
<p>If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.</p>
<p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p>
<p>Step 6: Setting up a multi-broker cluster</p>
<p>So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).</p>
<p>First we make a config file for each of the brokers:</p>
<blockquote>
<p>cp config/server.properties config/server-1.properties</p>
<p>cp config/server.properties config/server-2.properties</p>
</blockquote>
<p>Now edit these new files and set the following properties:</p>
<p>config/server-1.properties:<br>    broker.id=1<br>    listeners=PLAINTEXT://:9093<br>    log.dir=/tmp/kafka-logs-1</p>
<p>config/server-2.properties:<br>    broker.id=2<br>    listeners=PLAINTEXT://:9094<br>    log.dir=/tmp/kafka-logs-2</p>
<p>The broker.id property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.</p>
<p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p>
<blockquote>
<p>bin/kafka-server-start.sh config/server-1.properties &amp;<br>…<br>bin/kafka-server-start.sh config/server-2.properties &amp;<br>…</p>
</blockquote>
<p>Now create a new topic with a replication factor of three:</p>
<blockquote>
<p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 3 –partitions 1 –topic my-replicated-topic</p>
</blockquote>
<p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the “describe topics” command:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topic<br>Topic:my-replicated-topic      PartitionCount:1        ReplicationFactor:3    Configs:<br>        Topic: my-replicated-topic      Partition: 0    Leader: 1      Replicas: 1,2,0 Isr: 1,2,0</p>
</blockquote>
<p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p>
<ul>
<li>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li>
<li>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li>
<li>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li>
</ul>
<p>Note that in my example node 1 is the leader for the only partition of the topic.</p>
<p>We can run the same command on the original topic we created to see where it is:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test<br>Topic:test      PartitionCount:1        ReplicationFactor:1    Configs:<br>        Topic: test    Partition: 0    Leader: 0      Replicas: 0    Isr: 0</p>
</blockquote>
<p>So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p>
<p>Let’s publish a few messages to our new topic:</p>
<blockquote>
<p>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Now let’s consume these messages:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –from-beginning –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:</p>
<blockquote>
<p>ps | grep server-1.properties<br>7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java…<br>kill -9 7564</p>
</blockquote>
<p>Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:</p>
<blockquote>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topic<br>Topic:my-replicated-topic      PartitionCount:1        ReplicationFactor:3    Configs:<br>        Topic: my-replicated-topic      Partition: 0    Leader: 2      Replicas: 1,2,0 Isr: 2,0</p>
</blockquote>
<p>But the messages are still be available for consumption even though the leader that took the writes originally is down:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –from-beginning –topic my-replicated-topic<br>…<br>my test message 1<br>my test message 2<br>^C</p>
</blockquote>
<p>Step 7: Use Kafka Connect to import/export data</p>
<p>Writing data from the console and writing it back to the console is a convenient place to start, but you’ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data. Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs connectors, which implement the custom logic for interacting with an external system. In this quickstart we’ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file. First, we’ll start by creating some seed data to test with:</p>
<blockquote>
<p>echo -e “foo\nbar” &gt; test.txt</p>
</blockquote>
<p>Next, we’ll start two connectors running in standalone mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p>
<blockquote>
<p>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</p>
</blockquote>
<p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file. During startup you’ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from</p>
<p>test.txt</p>
<p>and producing them to the topic</p>
<p>connect-test</p>
<p>, and the sink connector should start reading messages from the topic</p>
<p>connect-test</p>
<p>and write them to the file</p>
<p>test.sink.txt</p>
<p>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p>
<blockquote>
<p>cat test.sink.txt<br>foo<br>bar</p>
</blockquote>
<p>Note that the data is being stored in the Kafka topic</p>
<p>connect-test</p>
<p>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 –topic connect-test –from-beginning<br>{“schema”:{“type”:”string”,”optional”:false},”payload”:”foo”}<br>{“schema”:{“type”:”string”,”optional”:false},”payload”:”bar”}<br>…</p>
</blockquote>
<p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p>
<blockquote>
<p>echo “Another line” &gt;&gt; test.txt</p>
</blockquote>
<p>You should see the line appear in the console consumer output and in the sink file.</p>
<p>Step 8: Use Kafka Streams to process data</p>
<p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).</p>
<p>KTable wordCounts = textLines<br>    // Split each text line, by whitespace, into words.<br>    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(“\W+”)))</p>
<pre><code>// Ensure the words are available as record keys for the next aggregate operation.
.map((key, value) -&gt; new KeyValue&lt;&gt;(value, value))

// Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;.
.countByKey(&quot;Counts&quot;)
</code></pre><p>It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on aninfinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data.</p>
<p>We will now prepare input data to a Kafka topic, which will subsequently processed by a Kafka Streams application.</p>
<blockquote>
<p>echo -e “all streams lead to kafka\nhello kafka streams\njoin kafka summit” &gt; file-input.txt</p>
</blockquote>
<p>Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):</p>
<blockquote>
<p>bin/kafka-topics.sh –create \<br>            –zookeeper localhost:2181 \<br>            –replication-factor 1 \<br>            –partitions 1 \<br>            –topic streams-file-input</p>
<p>cat file-input.txt | bin/kafka-console-producer.sh –broker-list localhost:9092 –topic streams-file-input</p>
</blockquote>
<p>We can now run the WordCount demo application to process the input data:</p>
<blockquote>
<p>bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo</p>
</blockquote>
<p>There won’t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p>
<p>We can now inspect the output of the WordCount demo application by reading from its output topic:</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –zookeeper localhost:2181 \<br>            –topic streams-wordcount-output \<br>            –from-beginning \<br>            –formatter kafka.tools.DefaultMessageFormatter \<br>            –property print.key=true \<br>            –property print.value=true \<br>            –property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \<br>            –property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</p>
</blockquote>
<p>with the following output data being printed to the console:</p>
<p>all    1<br>streams 1<br>lead    1<br>to      1<br>kafka  1<br>hello  1<br>kafka  2<br>streams 2<br>join    1<br>kafka  3<br>summit  1</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka 源码/" class="archive-article-date">
  	<time datetime="2016-11-10T06:08:52.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka Rest API" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka Rest API/">Kafka Rest API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafak-Connect-REST-API"><a href="#Kafak-Connect-REST-API" class="headerlink" title="Kafak Connect  REST API"></a>Kafak Connect  REST API</h3><p>Since Kafka Connect is intended to be run as a service, it also provides a REST API for managing connectors. By default this service runs on port 8083. The following are the currently supported endpoints:</p>
<ul>
<li><code>GET /connectors</code> - return a list of active connectors</li>
<li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string name field and a object config field with the connector configuration parameters</li>
<li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
<li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
<li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
<li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
<li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
<li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
<li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
<li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
<li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
<li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
<li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
</ul>
<p>Kafka Connect also provides a REST API for getting information about connector plugins:</p>
<ul>
<li><code>GET /connector-plugins-</code> return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
<li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka Rest API/" class="archive-article-date">
  	<time datetime="2016-11-10T05:59:23.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Kafka" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Kafka/">Kafka</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafka-具有3大功能"><a href="#Kafka-具有3大功能" class="headerlink" title="Kafka 具有3大功能:"></a>Kafka 具有3大功能:</h3><ul>
<li>1.Publish &amp; Subscribe: to streams of data like a messaging system</li>
<li>2.Process: streams of data efficiently</li>
<li>3.Store: streams of data safely in a distributed replicated cluster</li>
</ul>
<p><img src="/images/Kafka_1.png" alt=""></p>
<p><img src="/images/Kafka_2.png" alt=""></p>
<h3 id="Producer-amp-Consumer-导图"><a href="#Producer-amp-Consumer-导图" class="headerlink" title="Producer &amp; Consumer 导图"></a>Producer &amp; Consumer 导图</h3><p><img src="/images/Kafka_3.png" alt=""></p>
<p><img src="/images/Kafka_4.png" alt=""></p>
<p><img src="/images/Kafka_5.png" alt=""></p>
<h3 id="Kafka-four-core-APIs"><a href="#Kafka-four-core-APIs" class="headerlink" title="Kafka four core APIs"></a>Kafka four core APIs</h3><p>Kafka has four core APIs:</p>
<ul>
<li>The Producer API allows an application to publish a stream records to one or more Kafka topics.</li>
<li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
<li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
<li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems.</li>
</ul>
<h3 id="Producer-APIs"><a href="#Producer-APIs" class="headerlink" title="Producer APIs"></a>Producer APIs</h3><p><em>kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.</em><br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/* Sends the data, partitioned by key to the topic using either the */</span></div><div class="line">  <span class="comment">/* synchronous or the asynchronous producer */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(kafka.javaapi.producer.ProducerData&lt;K,V&gt; producerData)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Sends a list of data, partitioned by key to the topic using either */</span></div><div class="line">  <span class="comment">/* the synchronous or the asynchronous producer */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(java.util.List&lt;kafka.javaapi.producer.ProducerData&lt;K,V&gt;&gt; producerData)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Closes the producer and cleans up */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li><p><strong>can handle queueing/buffering of multiple producer requests and asynchronous dispatch of the batched data</strong><br><code>kafka.producer.Producer</code> provides the ability to batch multiple produce requests (producer.type=async), before serializing and dispatching them to the appropriate kafka broker partition. The size of the batch can be controlled by a few config parameters. As events enter a queue, they are buffered in a queue, until either queue.time or batch.size is reached. A background thread (<code>kafka.producer.async.ProducerSendThread</code>) dequeues the batch of data and lets the <code>kafka.producer.EventHandler</code> serialize and send the data to the appropriate kafka broker partition. A custom event handler can be plugged in through the event.handler config parameter. At various stages of this producer queue pipeline, it is helpful to be able to inject callbacks, either for plugging in custom logging/tracing code or custom monitoring logic. This is possible by implementing the <code>kafka.producer.async.CallbackHandler</code> interface and setting <code>callback.handlerconfig</code> parameter to that class.</p>
</li>
<li><p><strong>handles the serialization of data through a user-specified Encoder:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">interface Encoder&lt;T&gt; &#123;</div><div class="line">  public Message toMessage(T data);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>The default is the no-op <code>kafka.serializer.DefaultEncoder</code></p>
<ul>
<li><strong>provides software load balancing through an optionally user-specified Partitioner:</strong></li>
</ul>
<p>The routing decision is influenced by the kafka.producer.Partitioner.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">interface Partitioner&lt;T&gt; &#123;</div><div class="line">   int partition(T key, int numPartitions);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Consumer-APIs"><a href="#Consumer-APIs" class="headerlink" title="Consumer APIs"></a>Consumer APIs</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleConsumer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/* Send fetch request to a broker and get back a set of messages. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> ByteBufferMessageSet <span class="title">fetch</span><span class="params">(FetchRequest request)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/* Send a list of fetch requests to a broker and get back a response set. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> MultiFetchResponse <span class="title">multifetch</span><span class="params">(List&lt;FetchRequest&gt; fetches)</span></span>;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Get a list of valid offsets (up to maxSize) before the given time.</div><div class="line">   * The result is a list of offsets, in descending order.</div><div class="line">   * <span class="doctag">@param</span> time: time in millisecs,</div><div class="line">   *              if set to OffsetRequest$.MODULE$.LATEST_TIME(), get from the latest offset available.</div><div class="line">   *              if set to OffsetRequest$.MODULE$.EARLIEST_TIME(), get from the earliest offset available.</div><div class="line">   */</div><div class="line">  <span class="keyword">public</span> <span class="keyword">long</span>[] getOffsetsBefore(String topic, <span class="keyword">int</span> partition, <span class="keyword">long</span> time, <span class="keyword">int</span> maxNumOffsets);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* create a connection to the cluster */</span></div><div class="line">ConsumerConnector connector = Consumer.create(consumerConfig);</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">interface</span> <span class="title">ConsumerConnector</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * This method is used to get a list of KafkaStreams, which are iterators over</div><div class="line">   * MessageAndMetadata objects from which you can obtain messages and their</div><div class="line">   * associated metadata (currently only topic).</div><div class="line">   *  Input: a map of &lt;topic, #streams&gt;</div><div class="line">   *  Output: a map of &lt;topic, list of message streams&gt;</div><div class="line">   */</div><div class="line">  <span class="keyword">public</span> Map&lt;String,List&lt;KafkaStream&gt;&gt; createMessageStreams(Map&lt;String,Int&gt; topicCountMap);</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * You can also obtain a list of KafkaStreams, that iterate over messages</div><div class="line">   * from topics that match a TopicFilter. (A TopicFilter encapsulates a</div><div class="line">   * whitelist or a blacklist which is a standard Java regex.)</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">public</span> List&lt;KafkaStream&gt; <span class="title">createMessageStreamsByFilter</span><span class="params">(</span></span></div><div class="line">      TopicFilter topicFilter, <span class="keyword">int</span> numStreams);</div><div class="line"></div><div class="line">  <span class="comment">/* Commit the offsets of all messages consumed so far. */</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">commitOffsets</span><span class="params">()</span></span></div><div class="line"></div><div class="line">  <span class="comment">/* Shut down the connector */</span></div><div class="line">  <span class="keyword">public</span> <span class="title">shutdown</span><span class="params">()</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Kafka-ZooKeeper文件目录"><a href="#Kafka-ZooKeeper文件目录" class="headerlink" title="Kafka ZooKeeper文件目录"></a>Kafka ZooKeeper文件目录</h3><p><strong>Broker Node Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/brokers/ids/[0...N] --&gt; &#123;&quot;jmx_port&quot;:...,&quot;timestamp&quot;:...,&quot;endpoints&quot;:[...],&quot;host&quot;:...,&quot;version&quot;:...,&quot;port&quot;:...&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Broker Topic Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/brokers/topics/[topic]/partitions/[0...N]/state --&gt; &#123;&quot;controller_epoch&quot;:...,&quot;leader&quot;:...,&quot;version&quot;:...,&quot;leader_epoch&quot;:...,&quot;isr&quot;:[...]&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Consumer Id Registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/ids/[consumer_id] --&gt; &#123;&quot;version&quot;:...,&quot;subscription&quot;:&#123;...:...&#125;,&quot;pattern&quot;:...,&quot;timestamp&quot;:...&#125; (ephemeral node)</div></pre></td></tr></table></figure></p>
<p><strong>Consumer Offsets:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/offsets/[topic]/[partition_id] --&gt; offset_counter_value ((persistent node)</div></pre></td></tr></table></figure></p>
<p><strong>Partition Owner registry:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/consumers/[group_id]/owners/[topic]/[partition_id] --&gt; consumer_node_id (ephemeral node)</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Kafka/" class="archive-article-date">
  	<time datetime="2016-11-10T05:43:33.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Xia,MingXing
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: false
	}
</script>

<script src="/./main.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cassandra/" style="font-size: 10px;">Cassandra</a> <a href="/tags/Hadoop/" style="font-size: 20px;">Hadoop</a> <a href="/tags/Kafka/" style="font-size: 12.5px;">Kafka</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/ZooKeeper/" style="font-size: 12.5px;">ZooKeeper</a> <a href="/tags/etcd/" style="font-size: 10px;">etcd</a> <a href="/tags/介绍/" style="font-size: 10px;">介绍</a> <a href="/tags/分布式协同/" style="font-size: 10px;">分布式协同</a> <a href="/tags/大数据/" style="font-size: 17.5px;">大数据</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">平均每天保证几小时的学习时间，用一万小时定律激励自己，每年多13/14两个月就好</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>