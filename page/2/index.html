<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>SStar1314</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SStar1314">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="SStar1314">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SStar1314">
  
    <link rel="alternative" href="/atom.xml" title="SStar1314" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/main.css">
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/favicon.ico" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Xia,MingXing</a></h1>
		</hgroup>

		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">所有文章</a></li>
	        
				<li><a href="/tags/随笔">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="#" title="rss">rss</a>
		        
					<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Xia,MingXing</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/favicon.ico" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Xia,MingXing</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/tags/随笔">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/SStar1314/" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="mail" target="_blank" href="/xaxiamx@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-Spark Cluster三种模式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark Cluster三种模式/">Spark Cluster 三种模式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Spark-Cluster-三种模式"><a href="#Spark-Cluster-三种模式" class="headerlink" title="Spark Cluster 三种模式"></a>Spark Cluster 三种模式</h2><ul>
<li><strong>Standalone</strong> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><strong>Apache Mesos</strong> – a general cluster manager that can also run Hadoop MapReduce and service applications.</li>
<li><strong>Hadoop YARN</strong> – the resource manager in Hadoop 2.</li>
</ul>
<h3 id="Standalone-模式："><a href="#Standalone-模式：" class="headerlink" title="Standalone 模式："></a>Standalone 模式：</h3><p>start Master:<br><code>./sbin/start-master.sh</code><br>start Slave:<br><code>./sbin/start-slave.sh &lt;master-spark-URL&gt;</code></p>
<ul>
<li>sbin/start-master.sh - Starts a master instance on the machine the script is executed on.</li>
<li>sbin/start-slaves.sh - Starts a slave instance on each machine specified in the conf/slaves file.</li>
<li>sbin/start-slave.sh - Starts a slave instance on the machine the script is executed on.</li>
<li>sbin/start-all.sh - Starts both a master and a number of slaves as described above.</li>
<li>sbin/stop-master.sh - Stops the master that was started via the bin/start-master.sh script.</li>
<li>sbin/stop-slaves.sh - Stops all slave instances on the machines specified in the conf/slaves file.</li>
<li>sbin/stop-all.sh - Stops both the master and the slaves as described above.</li>
</ul>
<p>Connecting an Application to the Cluster:<br><code>./bin/spark-shell --master spark://IP:PORT</code><br>Launching Spark Applications:<br><code>./bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt;</code><br>Resource Scheduling:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf()</div><div class="line">             .setMaster(...)</div><div class="line">             .setAppName(...)</div><div class="line">             .set(&quot;spark.cores.max&quot;, &quot;10&quot;)</div><div class="line">val sc = new SparkContext(conf)</div></pre></td></tr></table></figure></p>
<h3 id="Running-Spark-on-Mesos-模式："><a href="#Running-Spark-on-Mesos-模式：" class="headerlink" title="Running Spark on Mesos 模式："></a>Running Spark on Mesos 模式：</h3><p>参考： <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/running-on-mesos.html</a></p>
<p><strong>Installing Mesos:</strong></p>
<ul>
<li>Spark 2.0.1 is designed for use with Mesos 0.21.0. <a href="http://mesos.apache.org/gettingstarted/" target="_blank" rel="external">http://mesos.apache.org/gettingstarted/</a><br><strong>Connecting  Spark  to  Mesos:</strong></li>
<li>To use Mesos from Spark, you need a Spark binary package available in a place accessible by Mesos, and a Spark driver program configured to connect to Mesos.<br><strong>Uploading  Spark Package:</strong></li>
<li>Download a Spark binary package from the Spark download page</li>
<li>Upload to hdfs/http/s3<br><strong>To host on HDFS, use the Hadoop fs put command:</strong><br><code>hadoop fs -put spark-2.0.1.tar.gz /path/to/spark-2.0.1.tar.gz</code><br><strong>Using a  Mesos Master URL:</strong></li>
<li>The Master URLs for Mesos are in the form <code>mesos://host:5050</code> for a single-master Mesos cluster, or <code>mesos://zk://host1:2181,host2:2181,host3:2181/mesos</code> for a multi-master Mesos cluster using ZooKeeper.<br><strong>Client Mode:</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf()</div><div class="line">       .setMaster(&quot;mesos://HOST:5050&quot;)</div><div class="line">       .setAppName(&quot;My app&quot;)</div><div class="line">       .set(&quot;spark.executor.uri&quot;, &quot;&lt;path to spark-2.0.1.tar.gz uploaded above&gt;&quot;)</div><div class="line">val sc = new SparkContext(conf)</div><div class="line">     ./bin/spark-shell --master mesos://host:5050</div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Cluster Mode:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">       --class org.apache.spark.examples.SparkPi \</div><div class="line">       --master mesos://207.184.161.138:7077 \</div><div class="line">       --deploy-mode cluster \</div><div class="line">       --supervise \</div><div class="line">       --executor-memory 20G \</div><div class="line">       --total-executor-cores 100 \</div><div class="line">       http://path/to/examples.jar \</div><div class="line">       1000 \</div></pre></td></tr></table></figure></p>
<h3 id="Running-Spark-on-YARN-模式："><a href="#Running-Spark-on-YARN-模式：" class="headerlink" title="Running Spark on YARN 模式："></a>Running Spark on YARN 模式：</h3><p><strong>Cluster mode:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</div><div class="line"></div><div class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">    --master yarn \</div><div class="line">    --deploy-mode cluster \</div><div class="line">    --driver-memory 4g \</div><div class="line">    --executor-memory 2g \</div><div class="line">    --executor-cores 1 \</div><div class="line">    --queue thequeue \</div><div class="line">    lib/spark-examples*.jar \</div><div class="line">    10</div></pre></td></tr></table></figure></p>
<p><strong>Client mode:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-shell --master yarn --deploy-mode client</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark Cluster三种模式/" class="archive-article-date">
  	<time datetime="2016-11-09T02:11:16.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Spark介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Spark介绍/">Spark介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Spark-运行架构"><a href="#Spark-运行架构" class="headerlink" title="Spark 运行架构"></a>Spark 运行架构</h3><p><img src="/images/Spark_Arch.png" alt=""></p>
<p><img src="/images/Spark_Arch_1.png" alt=""></p>
<p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行。</p>
<p>Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。<strong>Driver 程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。</strong></p>
<h3 id="Spark-组件"><a href="#Spark-组件" class="headerlink" title="Spark 组件"></a>Spark 组件</h3><ul>
<li>ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。在YARN模式中为资源管理器。</li>
<li>Worker：从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制。</li>
<li>Driver：运行Application的main()函数并创建SparkContext。</li>
<li>Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。</li>
<li>SparkContext：整个应用的上下文，控制应用的生命周期。</li>
<li>RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。</li>
<li>DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler。</li>
<li>TaskScheduler：将任务（Task）分发给Executor执行。</li>
<li>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。</li>
<li>SparkEnv内创建并包含如下一些重要组件的引用。</li>
<li>MapOutPutTracker：负责Shuffle元信息的存储。</li>
<li>BroadcastManager：负责广播变量的控制与元信息的存储。</li>
<li>BlockManager：负责存储管理、创建和查找块。</li>
<li>MetricsSystem：监控运行时性能指标信息。</li>
<li>SparkConf：负责存储配置信息。</li>
</ul>
<h3 id="Spark-提交Job"><a href="#Spark-提交Job" class="headerlink" title="Spark 提交Job"></a>Spark 提交Job</h3><p>参考： <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html</a><br>可向 本地 或 集群 提交。</p>
<h3 id="Spark-源代码-import-进-Eclipse"><a href="#Spark-源代码-import-进-Eclipse" class="headerlink" title="Spark 源代码 import 进 Eclipse"></a>Spark 源代码 import 进 Eclipse</h3><p>参考： <a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></p>
<p>到Spark源代码目录下，运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mvn -DskipTests clean package</div><div class="line">mvn eclipse:eclipse</div></pre></td></tr></table></figure></p>
<h3 id="推荐一本书"><a href="#推荐一本书" class="headerlink" title="推荐一本书"></a>推荐一本书</h3><p>《Advanced Analytics with Spark. 2015.4》</p>
<h3 id="生态"><a href="#生态" class="headerlink" title="生态"></a>生态</h3><p><img src="/images/Spark_Eco.jpg" alt=""></p>
<p>Spark can integaration with Hadoop ecosystem.</p>
<ul>
<li>1.Avro and Parquet can store data on Hadoop.</li>
<li>2.Can read and write to NoSQL databases like HBase and Cassandra.</li>
<li>3.Spark Streaming can ingest data from Flume and Kafka.</li>
<li>4.SparkSQL can interact with Hive Metastore.</li>
<li>5.It can run inside YARN, Hadoop’s scheduler and resource manager.</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Spark介绍/" class="archive-article-date">
  	<time datetime="2016-11-09T01:48:14.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Cassandra" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Cassandra/">Cassandra 数据库</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Cassandra-数据库"><a href="#Cassandra-数据库" class="headerlink" title="Cassandra 数据库"></a>Cassandra 数据库</h3><p>Apache Cassandra是一套开源分布式NoSQL数据库系统。它最初由Facebook开发，用于储存收件箱等简单格式数据，集Google BigTable的数据模型与AmazonDynamo的完全分布式架构于一身。Facebook于2008将 Cassandra 开源，此后，由于Cassandra良好的可扩展性和性能，被Apple, Comcast,Instagram, Spotify, eBay, Rackspace, Netflix等知名网站所采用，成为了一种流行的分布式结构化数据存储方案。</p>
<p>在数据库排行榜“DB-Engines Ranking”中，Cassandra排在第七位，是非关系型数据库中排名第二高的（仅次于MongoDB）。</p>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>Cassandra使用了Google 设计的 BigTable的数据模型，Cassandra使用的是宽列存储模型(Wide Column Stores)，每行数据由row key唯一标识之后，可以有最多20亿个列，每个列由一个column key标识，每个column key下对应若干value。这种模型可以理解为是一个二维的key-value存储，即整个数据模型被定义成一个类似 map&lt; key1, map&lt; key2,value&gt;&gt;的类型。</p>
<h3 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h3><p>与BigTable和其模仿者HBase不同，Cassandra的数据并不存储在分布式文件系统如GFS或HDFS中，而是直接存于本地。与BigTable一样，Cassandra也是日志型数据库，即把新写入的数据存储在内存的Memtable中并通过磁盘中的CommitLog来做持久化，内存填满后将数据按照key的顺序写进一个只读文件SSTable中，每次读取数据时将所有SSTable和内存中的数据进行查找和合并。这种系统的特点是写入比读取更快，因为写入一条数据是顺序计入commit log中，不需要随机读取磁盘以及搜索。</p>
<h3 id="与类似开源系统的比较"><a href="#与类似开源系统的比较" class="headerlink" title="与类似开源系统的比较"></a>与类似开源系统的比较</h3><p>HBase是Apache Hadoop项目的一个子项目，是Google BigTable的一个克隆，与Cassandra一样，它们都使用了BigTable的列族式的数据模型，但是：</p>
<ul>
<li>Cassandra只有一种节点，而HBase有多种不同角色，除了处理读写请求的region server之外，其架构在一套完整的HDFS分布式文件系统之上，并需要ZooKeeper来同步集群状态，部署上Cassandra更简单。</li>
<li>Cassandra的数据一致性策略是可配置的，可选择是强一致性还是性能更高的最终一致性；而HBase总是强一致性的。</li>
<li>Cassandra通过一致性哈希来决定一行数据存储在哪些节点，靠概率上的平均来实现负载均衡；而HBase每段数据(region)只有一个节点负责处理，由master来动态分配一个region是否大到需要拆分成两个，同时会将过热的节点上的一些region动态的分配给负载较低的节点，因此实现动态的负载均衡。</li>
<li>因为每个region同时只能有一个节点处理，一旦这个节点无响应，在系统将这个节点的所有region转移到其他节点之前这些数据便无法读写，加上master也只有一个节点，备用master的恢复也需要时间，因此HBase在一定程度上有单点问题；而Cassandra无单点问题。</li>
<li>Cassandra的读写性能优于HBase。</li>
</ul>
<h3 id="Cassandra-服务启动"><a href="#Cassandra-服务启动" class="headerlink" title="Cassandra 服务启动"></a>Cassandra 服务启动</h3><ul>
<li><p>1.启动服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./cassandra   </div><div class="line">org.apache.cassandra.service.CassandraDaemon</div></pre></td></tr></table></figure>
</li>
<li><p>2.启动用户交互              实际启动cqlsh.py来进行交互</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./cqlsh</div></pre></td></tr></table></figure>
</li>
</ul>
<p>Cassandra Shell 命令：  <a href="http://www.w3ii.com/cassandra/cassandra_shell_commands.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_shell_commands.html</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cassandra -f           前台启动 cassandra</div></pre></td></tr></table></figure></p>
<ul>
<li>3.停止服务<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kill  pid</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Cql-使用"><a href="#Cql-使用" class="headerlink" title="Cql 使用"></a>Cql 使用</h3><p><strong>创建使用Cqlsh一个密钥空间</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_create_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_create_keyspace.html</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">CREATE KEYSPACE tutorialspoint</div><div class="line">WITH replication = &#123;&apos;class&apos;:&apos;SimpleStrategy&apos;, &apos;replication_factor&apos; : 3&#125;;</div><div class="line">DESCRIBE keyspaces;</div></pre></td></tr></table></figure></p>
<p><strong>使用Java创建API密钥空间一</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_create_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_create_keyspace.html</a></p>
<p><strong>改变使用Cqlsh KEYSPACE</strong><br><a href="http://www.w3ii.com/cassandra/cassandra_alter_keyspace.html" target="_blank" rel="external">http://www.w3ii.com/cassandra/cassandra_alter_keyspace.html</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ALTER KEYSPACE “KeySpace Name”</div><div class="line">WITH replication = &#123;&apos;class&apos;: ‘Strategy name’, &apos;replication_factor&apos; : ‘No.Of  replicas’&#125;;</div><div class="line"></div><div class="line">ALTER KEYSPACE tutorialspoint</div><div class="line">WITH replication = &#123;&apos;class’:&apos;SimpleStrategy&apos;, &apos;replication_factor&apos; : 2&#125;;</div></pre></td></tr></table></figure></p>
<p><strong>测试密钥空间的durable_writes属性</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cqlsh&gt; SELECT * FROM system_schema.keyspaces;</div><div class="line">ALTER KEYSPACE test</div><div class="line">WITH REPLICATION = &#123;&apos;class&apos; : &apos;NetworkTopologyStrategy&apos;, &apos;datacenter1&apos; : 3&#125;</div><div class="line"></div><div class="line">AND DURABLE_WRITES = true;</div></pre></td></tr></table></figure></p>
<p><strong>删除使用Cqlsh一个密钥空间</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DROP KEYSPACE tutorialspoint;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">USE tutorialspoint;</div><div class="line">cqlsh:tutorialspoint&gt;; CREATE TABLE emp(</div><div class="line">   emp_id int PRIMARY KEY,</div><div class="line">   emp_name text,</div><div class="line">   emp_city text,</div><div class="line">   emp_sal varint,</div><div class="line">   emp_phone varint</div><div class="line">   );</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; select * from emp;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra修改表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; ALTER TABLE emp</div><div class="line">   ... ADD emp_email text;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra删除表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; DROP TABLE emp;</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; DESCRIBE COLUMNFAMILIES;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra截断表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tp&gt; TRUNCATE student;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建索引</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; CREATE INDEX name ON emp1 (emp_name);</div><div class="line"></div><div class="line">Cassandra DROP INDEX</div><div class="line">cqlsh:tp&gt; drop index name;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra批量</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; BEGIN BATCH</div><div class="line">     INSERT INTO emp (emp_id, emp_city, emp_name, emp_phone, emp_sal) values(  4,&apos;Pune&apos;,&apos;rajeev&apos;,9848022331, 30000);</div><div class="line">     UPDATE emp SET emp_sal = 50000 WHERE emp_id =3;</div><div class="line">     DELETE emp_city FROM emp WHERE emp_id = 2;</div><div class="line">     APPLY BATCH;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra创建数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(1,&apos;ram&apos;, &apos;Hyderabad&apos;, 9848022338, 50000);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(2,&apos;robin&apos;, &apos;Hyderabad&apos;, 9848022339, 40000);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO emp (emp_id, emp_name, emp_city,</div><div class="line">   emp_phone, emp_sal) VALUES(3,&apos;rahman&apos;, &apos;Chennai&apos;, 9848022330, 45000);</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra更新数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; UPDATE emp SET emp_city=&apos;Delhi&apos;,emp_sal=50000</div><div class="line">   WHERE emp_id=2;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra删除数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; DELETE emp_sal FROM emp WHERE emp_id=3;</div></pre></td></tr></table></figure></p>
<p><strong>Cassandra CQL集合</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data(name text PRIMARY KEY, email list&lt;text&gt;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data(name, email) VALUES (&apos;ramu&apos;,</div><div class="line">[&apos;abc@gmail.com&apos;,&apos;cba@yahoo.com&apos;])</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data</div><div class="line">... SET email = email +[&apos;xyz@w3ii.com&apos;]</div><div class="line">... where name = &apos;ramu&apos;;</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data2 (name text PRIMARY KEY, phone set&lt;varint&gt;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data2(name, phone)VALUES (&apos;rahman&apos;,    &#123;9848022338,9848022339&#125;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data2</div><div class="line">  ... SET phone = phone + &#123;9848022330&#125;</div><div class="line">  ... where name = &apos;rahman&apos;;</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; CREATE TABLE data3 (name text PRIMARY KEY, address map&lt;timestamp, text&gt;);</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; INSERT INTO data3 (name, address)</div><div class="line">   VALUES (&apos;robin&apos;, &#123;&apos;home&apos; : &apos;hyderabad&apos; , &apos;office&apos; : &apos;Delhi&apos; &#125; );</div><div class="line"></div><div class="line">cqlsh:tutorialspoint&gt; UPDATE data3</div><div class="line">   ... SET address = address+&#123;&apos;office&apos;:&apos;mumbai&apos;&#125;</div><div class="line">   ... WHERE name = &apos;robin&apos;;</div></pre></td></tr></table></figure></p>
<h3 id="Cassandra-安装-及-源代码分析"><a href="#Cassandra-安装-及-源代码分析" class="headerlink" title="Cassandra 安装 及 源代码分析"></a>Cassandra 安装 及 源代码分析</h3><p><strong>service 启动入口函数：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">org.apache.cassandra.service.CassandraDaemon</div></pre></td></tr></table></figure></p>
<p><strong>installation:</strong><br><a href="http://cassandra.apache.org/doc/latest/getting_started/installing.html" target="_blank" rel="external">http://cassandra.apache.org/doc/latest/getting_started/installing.html</a></p>
<p><strong>import源代码进Eclipse：</strong></p>
<ul>
<li>1.ant build</li>
<li>2.ant generate-eclipse-files</li>
</ul>
<p>还有其它操作：</p>
<ul>
<li>执行ant avro-generate</li>
<li>执行ant gen-thrift-java</li>
<li>执行ant generate-eclipse-files</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Cassandra/" class="archive-article-date">
  	<time datetime="2016-11-08T13:09:51.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/">Cassandra</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/NoSQL/">NoSQL</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-NoSQL介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/NoSQL介绍/">NoSQL参考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="NoSQL-参考"><a href="#NoSQL-参考" class="headerlink" title="NoSQL 参考"></a>NoSQL 参考</h3><p><img src="/images/NoSQL.png" alt=""></p>
<h3 id="NoSQL-对比"><a href="#NoSQL-对比" class="headerlink" title="NoSQL 对比"></a>NoSQL 对比</h3><p><img src="/images/NoSQL_Compare.png" alt=""></p>
<h3 id="CAP-原理"><a href="#CAP-原理" class="headerlink" title="CAP 原理"></a>CAP 原理</h3><p><img src="/images/CAP.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/NoSQL介绍/" class="archive-article-date">
  	<time datetime="2016-11-08T12:49:48.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NoSQL/">NoSQL</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/NoSQL/">NoSQL</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Storm" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Storm/">Storm(流计算)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Storm-简介"><a href="#Storm-简介" class="headerlink" title="Storm 简介"></a>Storm 简介</h2><p>参考： <a href="http://storm.apache.org/releases/0.10.2/index.html" target="_blank" rel="external">http://storm.apache.org/releases/0.10.2/index.html</a><br>Storm是一个分布式计算框架，主要由Clojure编程语言编写。最初是由Nathan Marz[1]及其团队创建于BackType，[2]该项目在被Twitter取得后开源。[3]它使用用户创建的“管（spouts）”和“螺栓（bolts）”来定义信息源和操作来允许批量、分布式处理流式数据。</p>
<h3 id="Storm-源代码导入-Eclipse"><a href="#Storm-源代码导入-Eclipse" class="headerlink" title="Storm 源代码导入 Eclipse"></a>Storm 源代码导入 Eclipse</h3><p>下载源代码并导入Eclipse： （可参考网页：<a href="http://ylzhj02.iteye.com/blog/2162197）" target="_blank" rel="external">http://ylzhj02.iteye.com/blog/2162197）</a></p>
<ul>
<li>1.git clone git://github.com/apache/storm.git</li>
<li>2.mvn clean package install -DskipTests=true</li>
<li>3.mvn eclipse:eclipse</li>
</ul>
<h3 id="Storm-Topology-架构"><a href="#Storm-Topology-架构" class="headerlink" title="Storm Topology 架构"></a>Storm Topology 架构</h3><p><img src="/images/Storm_Topology.png" alt=""></p>
<ul>
<li>Topology：storm中运行的一个实时应用程序，因为各个组件间的消息流动形成逻辑上的一个拓扑结构。</li>
<li>Spout：在一个topology中产生源数据流的组件。通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。Spout是一个主动的角色，其接口中有个nextTuple()函数，storm框架会不停地调用此函数，用户只要在其中生成源数据即可。</li>
<li>Bolt：在一个topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。</li>
<li>Tuple：一次消息传递的基本单元。本来应该是一个key-value的map，但是由于各个组件间传递的tuple的字段名称已经事先定义好，所以tuple中只要按序填入各个value就行了，所以就是一个value list.</li>
<li>Stream：源源不断传递的tuple就组成了stream。</li>
</ul>
<h3 id="Storm-Detail"><a href="#Storm-Detail" class="headerlink" title="Storm Detail"></a>Storm Detail</h3><p>在Storm的集群里面有两种节点： 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个叫Nimbus后台程序，它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分发代码，分配计算任务给机器，并且监控状态。<br>每一个工作节点上面运行一个叫做Supervisor的节点。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭工作进程。每一个工作进程执行一个topology的一个子集；一个运行的topology由运行在很多机器上的很多工作进程组成。<br><img src="/images/Storm1.png" alt="">  <img src="/images/Storm2.jpg" alt="">  <img src="/images/Storm3.png" alt=""></p>
<ul>
<li>Storm提供的最基本的处理stream的原语是spout和bolt。你可以实现spout和bolt提供的接口来处理你的业务逻辑。</li>
<li>消息源spout是Storm里面一个topology里面的消息生产者。一般来说消息源会从一个外部源读取数据并且向topology里面发出消息：tuple。Spout可以是可靠的也可以是不可靠的。如果这个tuple没有被storm成功处理，可靠的消息源spouts可以重新发射一个tuple， 但是不可靠的消息源spouts一旦发出一个tuple就不能重发了。</li>
<li>消息源可以发射多条消息流stream。使用OutputFieldsDeclarer.declareStream来定义多个stream，然后使用SpoutOutputCollector来发射指定的stream。</li>
<li>Spout类里面最重要的方法是nextTuple。要么发射一个新的tuple到topology里面或者简单的返回如果已经没有新的tuple。要注意的是nextTuple方法不能阻塞，因为storm在同一个线程上面调用所有消息源spout的方法。</li>
<li>另外两个比较重要的spout方法是ack和fail。storm在检测到一个tuple被整个topology成功处理的时候调用ack，否则调用fail。storm只对可靠的spout调用ack和fail。</li>
<li>所有的消息处理逻辑被封装在bolts里面。Bolts可以做很多事情：过滤，聚合，查询数据库等等。</li>
<li>Bolts可以简单的做消息流的传递。复杂的消息流处理往往需要很多步骤，从而也就需要经过很多bolts。比如算出一堆图片里面被转发最多的图片就至少需要两步：第一步算出每个图片的转发数量。第二步找出转发最多的前10个图片。(如果要把这个过程做得更具有扩展性那么可能需要更多的步骤)。</li>
<li>Bolts可以发射多条消息流， 使用OutputFieldsDeclarer.declareStream定义stream，使用OutputCollector.emit来选择要发射的stream。</li>
<li>Bolts的主要方法是execute, 它以一个tuple作为输入，bolts使用OutputCollector来发射tuple，bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知Storm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。 一般的流程是： bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。</li>
<li>定义一个topology的其中一步是定义每个bolt接收什么样的流作为输入。stream grouping就是用来定义一个stream应该如果分配数据给bolts上面的多个tasks。</li>
</ul>
<h3 id="Storm里面有7种类型的stream-grouping"><a href="#Storm里面有7种类型的stream-grouping" class="headerlink" title="Storm里面有7种类型的stream grouping"></a>Storm里面有7种类型的stream grouping</h3><ul>
<li>　　Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。</li>
<li>　　Fields Grouping：按字段分组， 比如按userid来分组， 具有同样userid的tuple会被分到相同的Bolts里的一个task， 而不同的userid则会被分配到不同的bolts里的task。</li>
<li>　　All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。</li>
<li>　　Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。</li>
<li>　　Non Grouping：不分组，这个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。</li>
<li>　　Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。 只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id (OutputCollector.emit方法也会返回task的id)。</li>
<li>　　Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。</li>
</ul>
<h3 id="Storm-集群安装和配置"><a href="#Storm-集群安装和配置" class="headerlink" title="Storm 集群安装和配置"></a>Storm 集群安装和配置</h3><p><strong>Summary of the steps for setting up a Storm cluster:</strong></p>
<ul>
<li>Set up a Zookeeper cluster</li>
<li>Install dependencies on Nimbus and worker machines</li>
<li>Download and extract a Storm release to Nimbus and worker machines</li>
<li>Fill in mandatory configurations into storm.yaml</li>
<li>Launch daemons under supervision using “storm” script and a supervisor of your choice</li>
</ul>
<p><strong>配置：</strong><br><img src="/images/Storm_Config.png" alt=""></p>
<p><strong>源代码入口：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/storm nimbus                                   org.apache.storm.daemon.nimbus</div><div class="line">bin/storm supervisor                              org.apache.storm.daemon.supervisor</div><div class="line">bin/storm ui                                             org.apache.storm.ui.core</div></pre></td></tr></table></figure></p>
<p><strong>运行测试案例：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/storm   jar   examples/storm-starter/storm-starter-topologies-1.0.2.jar    org.apache.storm.starter.WordCountTopology</div></pre></td></tr></table></figure></p>
<p><strong>nimbus 与 supervisor之间通过zookeeper 进行通信</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[zk: localhost:2181(CONNECTED) 9] ls /storm</div><div class="line">[backpressure, workerbeats, nimbuses, supervisors, errors, logconfigs, storms, assignments, leader-lock, blobstore]</div></pre></td></tr></table></figure></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Storm/" class="archive-article-date">
  	<time datetime="2016-11-08T12:27:38.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hive" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hive/">Hive(数据仓库)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h2><p>参考： <a href="https://cwiki.apache.org/confluence/display/Hive/Home" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Home</a><br>Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。Apache Hive起初由Facebook开发。</p>
<p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
<ul>
<li>1.hive是一个数据仓库</li>
<li>2.hive基于hadoop。</li>
</ul>
<h3 id="Hive-安装"><a href="#Hive-安装" class="headerlink" title="Hive 安装"></a>Hive 安装</h3><p>参考： <a href="http://doctuts.readthedocs.io/en/latest/hive.html" target="_blank" rel="external">http://doctuts.readthedocs.io/en/latest/hive.html</a></p>
<ul>
<li><p>1.安装hadoop</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=/root/hadoop-2.7.3</div><div class="line">export HIVE_HOME=/root/apache-hive-2.1.0-bin</div><div class="line">export PATH=$PATH:$HIVE_HOME/bin</div></pre></td></tr></table></figure>
</li>
<li><p>2.启动hadoop hdfs文件系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hdfs   namenode   -format</div><div class="line">sbin/start-all.sh    或者   start-dfs.sh and start-yarn.sh</div></pre></td></tr></table></figure>
</li>
<li><p>3.创建文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/hadoop fs -mkdir /usr/hive/warehouse</div><div class="line">bin/hadoop fs -chmod g+w /usr/hive/warehouse</div></pre></td></tr></table></figure>
</li>
<li><p>4.第一次运行hive之前，需要设置schema</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">schematool -initSchema -dbType derby</div></pre></td></tr></table></figure>
</li>
</ul>
<p>如果已经尝试运行hive出错之后，再去设置schema也会出错，需要做：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv  metastore_db  metastore_db.tmp</div></pre></td></tr></table></figure></p>
<ul>
<li>5.运行hive<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hive</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Hive-启动源代码入口："><a href="#Hive-启动源代码入口：" class="headerlink" title="Hive 启动源代码入口："></a>Hive 启动源代码入口：</h3><p>bin/hive  —&gt;  hive script中会执行  bin/ext/*.sh, 以及 bin/ext/util/*.sh 命令<br>SERVICE_LIST 变量 在bin/ext/*.sh 中增加value；<br>SERVICE 变量在启动命令时 赋值； 默认   SERVICE=”cli”</p>
<p><img src="/images/Hive_Code_1.png" alt=""> <img src="/images/Hive_Code_2.png" alt=""></p>
<p>—&gt;   execHiveCmd 启动 java 源代码的入口               org.apache.hive.beeline.cli.HiveCli   或者   org.apache.hadoop.hive.cli.CliDriver</p>
<h3 id="Hive-vs-HBase"><a href="#Hive-vs-HBase" class="headerlink" title="Hive vs HBase"></a>Hive vs HBase</h3><p>共同点：</p>
<ul>
<li>1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储</li>
</ul>
<p>区别：</p>
<ul>
<li>1.Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。</li>
<li>2.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。</li>
<li>3.Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。</li>
<li>4.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。</li>
<li>5.hive借用hadoop的MapReduce来完成一些hive中的命令的执行</li>
<li>6.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。</li>
<li>7.hbase是列存储。</li>
<li>8.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。</li>
<li>9.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。</li>
</ul>
<h3 id="improt-Hive源代码到Eclipse里面："><a href="#improt-Hive源代码到Eclipse里面：" class="headerlink" title="improt Hive源代码到Eclipse里面："></a>improt Hive源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h3 id="Hive-知识范围"><a href="#Hive-知识范围" class="headerlink" title="Hive 知识范围"></a>Hive 知识范围</h3><p><img src="/images/Hive_Related.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hive/" class="archive-article-date">
  	<time datetime="2016-11-08T09:51:43.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-HBase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/HBase/">HBase(非关系型分布式数据库)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HBase-简介"><a href="#HBase-简介" class="headerlink" title="HBase 简介"></a>HBase 简介</h2><p>HBase 是一个开源的非关系型分布式数据库。 <a href="https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hbase-tutorial-get-started-linux/" target="_blank" rel="external">https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hbase-tutorial-get-started-linux/</a></p>
<ul>
<li>HBase架构指南： <a href="http://www.guru99.com/hbase-architecture-data-flow-usecases.html" target="_blank" rel="external">http://www.guru99.com/hbase-architecture-data-flow-usecases.html</a></li>
<li>安装指南： <a href="http://www.guru99.com/hbase-installation-guide.html" target="_blank" rel="external">http://www.guru99.com/hbase-installation-guide.html</a></li>
<li>HBase Shell 命令指南： <a href="http://www.guru99.com/hbase-shell-general-commands.html" target="_blank" rel="external">http://www.guru99.com/hbase-shell-general-commands.html</a></li>
</ul>
<h2 id="HBase-NoSQL-数据库优缺点"><a href="#HBase-NoSQL-数据库优缺点" class="headerlink" title="HBase NoSQL 数据库优缺点"></a>HBase NoSQL 数据库优缺点</h2><p>Hbase,Casandra,Bigtable都属于面向 <strong>列存储</strong> 的分布式存储系统。</p>
<p>HBase 基本单元：</p>
<ul>
<li>Table: Collection of rows present.</li>
<li>Row: Collection of column families.</li>
<li>Column Family: Collection of columns.</li>
<li>Column: Collection of key-value pairs.</li>
<li>Namespace: Logical grouping of tables.</li>
<li>Cell: A {row, column, version} tuple exactly specifies a cell definition in HBase.</li>
</ul>
<p>列存储 vs 行存储：<br><img src="/images/Column_vs_Row.png" alt=""></p>
<p>Hbase的优点：<br>1 列的可以动态增加，并且列为空就不存储数据,节省存储空间.<br>2 Hbase自动切分数据，使得数据存储自动具有水平scalability.<br>3 Hbase可以提供高并发读写操作的支持</p>
<p>Hbase的缺点：<br>1 不能支持条件查询，只支持按照Row key来查询.<br>2 暂时不能支持Master server的故障切换,当Master宕机后,整个存储系统就会挂掉.</p>
<h2 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h2><p><img src="/images/HBase_Arch.png" alt=""></p>
<p><img src="/images/HBase_Arch_1.png" alt=""></p>
<h2 id="Hbase-Data-Flow"><a href="#Hbase-Data-Flow" class="headerlink" title="Hbase Data Flow"></a>Hbase Data Flow</h2><p><img src="/images/HBase_Data_Flow.png" alt=""></p>
<p>The Read and Write operations from Client into Hfile can be shown in below diagram.</p>
<ul>
<li>Step 1) Client wants to write data and in turn first communicates with Regions server and then regions</li>
<li>Step 2) Regions contacting memstore for storing associated with the column family</li>
<li>Step 3) First data stores into Memstore, where the data is sorted and after that it flushes into HFile. The main reason for using Memstore is to store data in Distributed file system based on Row Key. Memstore will be placed in Region server main memory while HFiles are written into HDFS.</li>
<li>Step 4) Client wants to read data from Regions</li>
<li>Step 5) In turn Client can have direct access to Mem store, and it can request for data.</li>
<li>Step 6) Client approaches HFiles to get the data. The data are fetched and retrieved by the Client.</li>
</ul>
<h2 id="HBase-vs-HDFS"><a href="#HBase-vs-HDFS" class="headerlink" title="HBase vs HDFS"></a>HBase vs HDFS</h2><p><img src="/images/HBase_vs_HDFS.png" alt=""></p>
<h3 id="improt-HBase源代码到Eclipse里面："><a href="#improt-HBase源代码到Eclipse里面：" class="headerlink" title="improt HBase源代码到Eclipse里面："></a>improt HBase源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h2 id="HBase-源码入口"><a href="#HBase-源码入口" class="headerlink" title="HBase 源码入口"></a>HBase 源码入口</h2><p><img src="/images/HBase_Code.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/HBase/" class="archive-article-date">
  	<time datetime="2016-11-08T09:16:42.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop 源码入口" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop 源码入口/">Hadoop 源码入口</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="improt-Hadoop源代码到Eclipse里面："><a href="#improt-Hadoop源代码到Eclipse里面：" class="headerlink" title="improt Hadoop源代码到Eclipse里面："></a>improt Hadoop源代码到Eclipse里面：</h3><ul>
<li><ol>
<li>mvn clean install -DskipTests</li>
</ol>
</li>
<li><ol>
<li>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs</li>
</ol>
</li>
</ul>
<h2 id="Hadoop-源码入口"><a href="#Hadoop-源码入口" class="headerlink" title="Hadoop 源码入口"></a>Hadoop 源码入口</h2><p>这些是Hadoop 源码阅读的入口位置，可以通过这些main函数看进源码实现。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Hadoop <span class="built_in">command</span>:</div><div class="line">    <span class="comment"># the core commands</span></div><div class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fs"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.fs.FsShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"version"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"jar"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.RunJar</div><div class="line">      <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$&#123;YARN_OPTS&#125;</span>"</span> ]] || [[ -n <span class="string">"<span class="variable">$&#123;YARN_CLIENT_OPTS&#125;</span>"</span> ]]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"WARNING: Use \"yarn jar\" to launch YARN applications."</span> 1&gt;&amp;2</div><div class="line">      <span class="keyword">fi</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"key"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.crypto.key.KeyShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"checknative"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.util.NativeLibraryChecker</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"distcp"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tools.DistCp</div><div class="line">      CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"daemonlog"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.log.LogLevel</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"archive"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tools.HadoopArchives</div><div class="line">      CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"credential"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.security.alias.CredentialShell</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"trace"</span> ] ; <span class="keyword">then</span></div><div class="line">      CLASS=org.apache.hadoop.tracing.TraceAdmin</div><div class="line">    <span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"classpath"</span> ] ; <span class="keyword">then</span></div><div class="line">      <span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> <span class="_">-gt</span> 1 ]; <span class="keyword">then</span></div><div class="line">        CLASS=org.apache.hadoop.util.Classpath</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">Hdfs <span class="built_in">command</span>:</div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"namenode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.namenode.NameNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_NAMENODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"zkfc"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.tools.DFSZKFailoverController'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_ZKFC_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"secondarynamenode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_SECONDARYNAMENODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"datanode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.server.datanode.DataNode'</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$starting_secure_dn</span>"</span> = <span class="string">"true"</span> ]; <span class="keyword">then</span></div><div class="line">    HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -jvm server <span class="variable">$HADOOP_DATANODE_OPTS</span>"</span></div><div class="line">  <span class="keyword">else</span></div><div class="line">    HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -server <span class="variable">$HADOOP_DATANODE_OPTS</span>"</span></div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"journalnode"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.hdfs.qjournal.server.JournalNode'</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_JOURNALNODE_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"dfs"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.fs.FsShell</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"dfsadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"haadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSHAAdmin</div><div class="line">  CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;TOOL_PATH&#125;</span></div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fsck"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DFSck</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"balancer"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_BALANCER_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"mover"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.server.mover.Mover</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$&#123;HADOOP_OPTS&#125;</span> <span class="variable">$&#123;HADOOP_MOVER_OPTS&#125;</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"storagepolicies"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.StoragePolicyAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"jmxget"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.JMXGet</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oiv"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oiv_legacy"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"oev"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"fetchdt"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DelegationTokenFetcher</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"getconf"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.GetConf</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"groups"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.GetGroups</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"snapshotDiff"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"lsSnapshottableDir"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"portmap"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.portmap.Portmap</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_PORTMAP_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"nfs3"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.nfs.nfs3.Nfs3</div><div class="line">  HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> <span class="variable">$HADOOP_NFS3_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"cacheadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.CacheAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"crypto"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.CryptoAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"version"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"debug"</span> ]; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.hdfs.tools.DebugAdmin</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"classpath"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> <span class="_">-gt</span> 0 ]; <span class="keyword">then</span></div><div class="line">    CLASS=org.apache.hadoop.util.Classpath</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter</div><div class="line">org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">Yarn:</div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"rmadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.client.cli.RMAdminCLI'</span></div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"scmadmin"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.client.SCMAdmin'</span></div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"application"</span> ] ||</div><div class="line">     [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"applicationattempt"</span> ] ||</div><div class="line">     [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"container"</span> ]; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.ApplicationCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line">  <span class="built_in">set</span> -- <span class="variable">$COMMAND</span> <span class="variable">$@</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"node"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.NodeCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"queue"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.QueueCLI</div><div class="line">  YARN_OPTS=<span class="string">"<span class="variable">$YARN_OPTS</span> <span class="variable">$YARN_CLIENT_OPTS</span>"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="string">"<span class="variable">$COMMAND</span>"</span> = <span class="string">"resourcemanager"</span> ] ; <span class="keyword">then</span></div><div class="line">  CLASSPATH=<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$YARN_CONF_DIR</span>/rm-config/<span class="built_in">log</span>4j.properties</div><div class="line">  CLASS=<span class="string">'org.apache.hadoop.yarn.server.resourcemanager.ResourceManager’               main     —&gt; serviceInit —&gt; serviceStart</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_RESOURCEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_RESOURCEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_RESOURCEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "historyserver" ] ; then</div><div class="line">  echo "DEPRECATED: Use of this command to start the timeline server is deprecated." 1&gt;&amp;2</div><div class="line">  echo "Instead use the timelineserver command for it." 1&gt;&amp;2</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/ahs-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_HISTORYSERVER_OPTS"</div><div class="line">  if [ "$YARN_HISTORYSERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_HISTORYSERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "timelineserver" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/timelineserver-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_TIMELINESERVER_OPTS"</div><div class="line">  if [ "$YARN_TIMELINESERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_TIMELINESERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "sharedcachemanager" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/scm-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_SHAREDCACHEMANAGER_OPTS"</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_SHAREDCACHEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_SHAREDCACHEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_SHAREDCACHEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "nodemanager" ] ; then</div><div class="line">  CLASSPATH=$&#123;CLASSPATH&#125;:$YARN_CONF_DIR/nm-config/log4j.properties</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.nodemanager.NodeManager<span class="string">'                                    main     —&gt; serviceInit —&gt; serviceStart</span></div><div class="line">  YARN_OPTS="$YARN_OPTS -server $YARN_NODEMANAGER_OPTS"</div><div class="line">  if [ "$YARN_NODEMANAGER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_NODEMANAGER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "proxyserver" ] ; then</div><div class="line">  CLASS='org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer<span class="string">'</span></div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_PROXYSERVER_OPTS"</div><div class="line">  if [ "$YARN_PROXYSERVER_HEAPSIZE" != "" ]; then</div><div class="line">    JAVA_HEAP_MAX="-Xmx""$YARN_PROXYSERVER_HEAPSIZE""m"</div><div class="line">  fi</div><div class="line">elif [ "$COMMAND" = "version" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.util.VersionInfo</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "jar" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.util.RunJar</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "logs" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.LogsCLI</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "daemonlog" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.log.LogLevel</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">elif [ "$COMMAND" = "cluster" ] ; then</div><div class="line">  CLASS=org.apache.hadoop.yarn.client.cli.ClusterCLI</div><div class="line">  YARN_OPTS="$YARN_OPTS $YARN_CLIENT_OPTS"</div><div class="line">else</div><div class="line">  CLASS=$COMMAND</div><div class="line">fi</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop 源码入口/" class="archive-article-date">
  	<time datetime="2016-11-08T08:54:09.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop Yarn(待补充)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop Yarn(待补充)/">Hadoop Yarn(待补充)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-Yarn-流程图"><a href="#Hadoop-Yarn-流程图" class="headerlink" title="Hadoop Yarn 流程图"></a>Hadoop Yarn 流程图</h2><p>Hadoop v1 资源调度：<br><img src="/images/Hadoop_Yarn_1.png" alt=""></p>
<p>Hadoop v2 Yarn 资源调度：<br><img src="/images/Hadoop_Yarn_2.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop Yarn(待补充)/" class="archive-article-date">
  	<time datetime="2016-11-08T08:50:52.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-Hadoop MapReduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/Hadoop MapReduce/">Hadoop MapReduce</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hadoop-MapReduce-流程图"><a href="#Hadoop-MapReduce-流程图" class="headerlink" title="Hadoop MapReduce 流程图"></a>Hadoop MapReduce 流程图</h2><p><img src="/images/Hadoop_MapReduce.png" alt=""><br>图中1：表示待处理数据，比如日志，比如单词计数.<br>图中2：表示map阶段，对他们split，然后送到不同分区.<br>图中3：表示reduce阶段，对这些数据整合处理.<br>图中4：表示二次mapreduce,这个是mapreduce的链式.</p>
<p><img src="/images/Hadoop_MapReduce_1.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce.jpg" alt=""></p>
<p><strong>在hadoop中，map-&gt;combine-&gt;partition-&gt;shuffle-&gt;reduce，五个步骤的作用分别是什么？</strong></p>
<ul>
<li>combine和partition都是函数，中间的步骤应该只有shuffle！</li>
<li>combine分为map端和reduce端，作用是把同一个key的键值对合并在一起，可以自定义的。</li>
<li>combine函数把一个map函数产生的<key,value>对（多个key,value）合并成一个新的<key2,value2>.将新的<key2,value2>作为输入到reduce函数中</key2,value2></key2,value2></key,value></li>
<li>这个value2亦可称之为values，因为有多个。这个合并的目的是为了减少网络传输。</li>
<li>partition是分割map每个节点的结果，按照key分别映射给不同的reduce，也是可以自定义的。这里其实可以理解归类。</li>
<li>partition的作用就是把这些数据归类。只不过在写程序的时候，mapreduce使用哈希HashPartitioner帮我们归类了。这个我们也可以自定义。</li>
<li>shuffle就是map和reduce之间的过程，包含了两端的combine和partition。</li>
<li>Map的结果，会通过partition分发到Reducer上，Reducer做完Reduce操作后，通过OutputFormat，进行输出</li>
<li>shuffle阶段的主要函数是fetchOutputs(),这个函数的功能就是将map阶段的输出，copy到reduce 节点本地。</li>
</ul>
<p><em>摘自aboutyun社区，一些值得思考的问题</em><br><strong>1.Shuffle的定义是什么？</strong><br><strong>2.map task与reduce task的执行是否在不同的节点上？</strong><br><strong>3.Shuffle产生的意义是什么？</strong><br><strong>4.每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据该如何处理？</strong><br><strong>5.在map task执行时，它是如何读取HDFS的？</strong><br><strong>6.读取的Split与block的对应关系可能是什么？</strong><br><strong>7.MapReduce提供Partitioner接口，它的作用是什么？</strong><br><strong>8.溢写是在什么情况下发生？</strong><br><strong>9.溢写是为什么不影响往缓冲区写map结果的线程？</strong><br><strong>10.当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对谁的排序？</strong><br><strong>11.哪些场景才能使用Combiner呢？</strong><br><strong>12.Merge的作用是什么？</strong><br><strong>13.reduce中Copy过程采用是什么协议？</strong><br><strong>14.reduce中merge过程有几种方式，与map有什么相似之处？</strong><br><strong>15.溢写过程中如果有很多个key/value对需要发送到某个reduce端去，那么如何处理这些key/value值</strong></p>
<p><strong>Shuffle产生的意义是什么？</strong><br>在Hadoop这样的集群环境中，大部分map task与reduce task的执行是在不同的节点上。当然很多情况下Reduce执行时需要跨节点去拉取其它节点上的map task结果。如果集群正在运行的job有很多，那么task的正常执行对集群内部的网络资源消耗会很严重。这种网络消耗是正常的，我们不能限制，能做的就是最大化地减少不必要的消耗。还有在节点内，相比于内存，磁盘IO对job完成时间的影响也是可观的。从最基本的要求来说，Shuffle过程的期望可以有：<br>完整地从map task端拉取数据到reduce 端。<br>在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗。<br>减少磁盘IO对task执行的影响。</p>
<p><strong>每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据该如何处理？</strong><br>每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p>
<p><strong>MapReduce提供Partitioner接口，它的作用是什么？</strong><br>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。</p>
<p><strong>什么是溢写？</strong><br>在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写。</p>
<p><strong>溢写是为什么不影响往缓冲区写map结果的线程？</strong><br>溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</p>
<p><strong>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对谁的排序？</strong><br>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。</p>
<p><strong>溢写过程中如果有很多个key/value对需要发送到某个reduce端去，那么如何处理这些key/value值？</strong><br>如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p>
<p><strong>哪些场景才能使用Combiner呢？</strong><br>Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。</p>
<p><strong>Merge的作用是什么？</strong><br>最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge</p>
<p><strong>每个reduce task不断的通过什么协议从JobTracker那里获取map task是否完成的信息？</strong><br>每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息</p>
<p><strong>reduce中Copy过程采用是什么协议？</strong><br>Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。</p>
<p><strong>reduce中merge过程有几种方式？</strong><br>merge有三种形式：1)内存到内存  2)内存到磁盘  3)磁盘到磁盘。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。</p>
<h3 id="Map-过程"><a href="#Map-过程" class="headerlink" title="Map 过程"></a>Map 过程</h3><p><img src="/images/Hadoop_Map.jpg" alt=""><br>整个流程分了四步。简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p>
<p>当然这里的每一步都可能包含着多个步骤与细节，下面对细节来说明：</p>
<ul>
<li><p>1.在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。在WordCount例子里，假设map的输入数据都是像“aaa”这样的字符串。</p>
</li>
<li><p>2.在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。这个job有多个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。</p>
</li>
</ul>
<p>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。</p>
<p>在WordCount例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。</p>
<ul>
<li>3.这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写，字面意思很直观。这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</li>
</ul>
<p>当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。</p>
<p>在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节是，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p>
<p>在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。</p>
<p>如果client设置过Combiner，那么现在就是使用Combiner的时候了。将有相同key的key/value对的value加起来，减少溢写到磁盘的数据量。Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用。那哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。</p>
<ul>
<li>4.每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个溢写文件存在。当map task真正完成时，内存缓冲区中的数据也全部溢写到磁盘中形成一个溢写文件。最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge。Merge是怎样的？如前面的例子，“aaa”从某个map task读取过来时值是5，从另外一个map 读取时值是8，因为它们有相同的key，所以得merge成group。什么是group。对于“aaa”就是像这样的：{“aaa”, [5, 8, 2, …]}，数组中的值就是从不同溢写文件中读取出来的，然后再把这些值加起来。请注意，因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在，在这个过程中如果client设置过Combiner，也会使用Combiner来合并相同的key。</li>
</ul>
<p>至此，map端的所有工作都已结束，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。</p>
<h3 id="Reduce-过程"><a href="#Reduce-过程" class="headerlink" title="Reduce 过程"></a>Reduce 过程</h3><p><img src="/images/Hadoop_Reduce.jpg" alt=""></p>
<p>如map端的细节图，Shuffle在reduce端的过程也能用图上标明的三点来概括。当前reduce copy数据的前提是它要从JobTracker获得有哪些map task已执行结束。Reducer真正运行之前，所有的时间都是在拉取数据，做merge，且不断重复地在做。如前面的方式一样，下面也分段地描述reduce 端的Shuffle细节：</p>
<ul>
<li><p>1.Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</p>
</li>
<li><p>2.Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：1)内存到内存  2)内存到磁盘  3)磁盘到磁盘。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。</p>
</li>
<li><p>3.Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。</p>
</li>
</ul>
<h3 id="Map-Reduce-过程图片"><a href="#Map-Reduce-过程图片" class="headerlink" title="Map-Reduce 过程图片"></a>Map-Reduce 过程图片</h3><p><img src="/images/Hadoop_Map_Reduce.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce_1.png" alt=""></p>
<p><img src="/images/Hadoop_Map_Reduce_2.png" alt=""></p>
<h3 id="Map-Reduce-过程例子"><a href="#Map-Reduce-过程例子" class="headerlink" title="Map-Reduce 过程例子"></a>Map-Reduce 过程例子</h3><p><img src="/images/Hadoop_Map_Reduce_Example.png" alt=""></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/Hadoop MapReduce/" class="archive-article-date">
  	<time datetime="2016-11-08T08:05:53.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-08</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Xia,MingXing
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: false
	}
</script>

<script src="/./main.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cassandra/" style="font-size: 10px;">Cassandra</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/Spark/" style="font-size: 17.5px;">Spark</a> <a href="/tags/ZooKeeper/" style="font-size: 12.5px;">ZooKeeper</a> <a href="/tags/etcd/" style="font-size: 10px;">etcd</a> <a href="/tags/分布式协同/" style="font-size: 10px;">分布式协同</a> <a href="/tags/大数据/" style="font-size: 20px;">大数据</a> <a href="/tags/简介/" style="font-size: 10px;">简介</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">平均每天保证几小时的学习时间，用一万小时定律激励自己，每年多13/14两个月就好</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>